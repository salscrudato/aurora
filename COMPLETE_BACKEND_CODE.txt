================================================================================
AURORANOTES BACKEND - COMPLETE CODE EXPORT
Generated: 2025-12-16T08:55:18.766Z
Total Files: 21
Total Lines: 6,761
Total Size: 207.7 KB
================================================================================

TABLE OF CONTENTS
--------------------------------------------------------------------------------
 1. src/chat.ts
 2. src/chunking.ts
 3. src/citationValidator.ts
 4. src/config.ts
 5. src/embeddings.ts
 6. src/firestore.ts
 7. src/genaiClient.ts
 8. src/index.ts
 9. src/internalAuth.ts
10. src/notes.ts
11. src/query.ts
12. src/queryExpansion.ts
13. src/queue.ts
14. src/rateLimit.ts
15. src/reranker.ts
16. src/retrieval.ts
17. src/retrievalLogger.ts
18. src/types.ts
19. src/utils.ts
20. src/vectorIndex.ts
21. src/vectorRetriever.ts

================================================================================
FILE: src/chat.ts
LINES: 703
PATH: /Users/salscrudato/Projects/auroranotes-api/src/chat.ts
================================================================================

/**
 * AuroraNotes API - Chat Service
 *
 * RAG-powered chat with inline citations, retry logic, and enhanced error handling.
 * Includes structured retrieval logging for observability.
 */

import {
  CHAT_MODEL,
  CHAT_TIMEOUT_MS,
  CHAT_MAX_QUERY_LENGTH,
  CHAT_TEMPERATURE,
  RETRIEVAL_TOP_K,
  RETRIEVAL_RERANK_TO,
  DEFAULT_TENANT_ID,
  MAX_CHUNKS_IN_CONTEXT,
  CITATION_RETRY_ENABLED,
  CITATION_VERIFICATION_ENABLED,
  CITATION_MIN_OVERLAP_SCORE,
  CITATION_ENTAILMENT_ENABLED,
} from "./config";
import { ChatRequest, ChatResponse, Citation, ScoredChunk, QueryIntent, SourcesPack } from "./types";
import { retrieveRelevantChunks, analyzeQuery } from "./retrieval";
import { logInfo, logError, logWarn, sanitizeText, isValidTenantId } from "./utils";
import { validateCitationsWithChunks } from "./citationValidator";
import {
  createRetrievalLog,
  logRetrieval,
  RetrievalLogEntry,
  RetrievalTimings,
  QualityFlags,
  CitationLogEntry,
  computeScoreDistribution,
  candidateCountsToStageDetails,
} from "./retrievalLogger";
import { getGenAIClient, isGenAIAvailable } from "./genaiClient";

// Retry configuration
const MAX_LLM_RETRIES = 2;
const LLM_RETRY_DELAY_MS = 1000;

/**
 * Create a timeout promise that rejects after specified milliseconds
 */
function createTimeout<T>(ms: number, context: string): Promise<T> {
  return new Promise((_, reject) => {
    setTimeout(() => {
      reject(new Error(`Timeout after ${ms}ms: ${context}`));
    }, ms);
  });
}

// Citation accuracy thresholds (tuned for better recall while maintaining precision)
const MIN_CITATION_COVERAGE = 0.5;        // Trigger repair if < 50% of sources cited
const MIN_CITATION_COVERAGE_STRICT = 0.6; // Warn if < 60% coverage after repair
const MIN_KEYWORD_OVERLAP = 2;            // Min keyword matches for citation validity

// NOTE: MIN_CITATION_SCORE filtering is now done in retrieval (MIN_COMBINED_SCORE)
// to ensure prompt source count == citationsMap.size EXACTLY.
// All chunks returned from retrieval are "source-worthy" and included in citations.

/**
 * Custom error for server configuration issues (not client errors)
 */
export class ConfigurationError extends Error {
  constructor(message: string) {
    super(message);
    this.name = 'ConfigurationError';
  }
}

/**
 * Custom error for rate limiting
 */
export class RateLimitError extends Error {
  constructor(message: string) {
    super(message);
    this.name = 'RateLimitError';
  }
}

/**
 * Retry LLM call with exponential backoff and hard timeout
 */
async function withLLMRetry<T>(
  fn: () => Promise<T>,
  context: string,
  timeoutMs: number = CHAT_TIMEOUT_MS
): Promise<T> {
  let lastError: Error | unknown;

  for (let attempt = 0; attempt <= MAX_LLM_RETRIES; attempt++) {
    try {
      // Race between LLM call and timeout
      const result = await Promise.race([
        fn(),
        createTimeout<T>(timeoutMs, context),
      ]);
      return result;
    } catch (err) {
      lastError = err;
      const errMessage = err instanceof Error ? err.message : String(err);

      // Don't retry on certain errors
      if (errMessage.includes('INVALID_ARGUMENT') ||
          errMessage.includes('PERMISSION_DENIED') ||
          errMessage.includes('API key')) {
        throw err;
      }

      // Check for rate limiting
      if (errMessage.includes('429') || errMessage.includes('RESOURCE_EXHAUSTED')) {
        throw new RateLimitError('API rate limit exceeded');
      }

      // Log timeout errors with context for debugging
      if (errMessage.includes('Timeout')) {
        logWarn(`${context} timeout`, { attempt: attempt + 1, timeoutMs });
      }

      if (attempt < MAX_LLM_RETRIES) {
        const delay = LLM_RETRY_DELAY_MS * Math.pow(2, attempt);
        logWarn(`${context} retry`, { attempt: attempt + 1, delayMs: delay });
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
  }

  throw lastError;
}

/**
 * Extract the most informative snippet from a chunk
 * Prioritizes sentence-complete excerpts and key phrases
 */
function extractBestSnippet(text: string, maxLength: number = 200): string {
  if (text.length <= maxLength) return text;

  // Try to find a complete sentence that fits
  const sentences = text.split(/(?<=[.!?])\s+/);
  if (sentences[0] && sentences[0].length <= maxLength) {
    let snippet = sentences[0];
    // Add more sentences if they fit
    for (let i = 1; i < sentences.length; i++) {
      if (snippet.length + sentences[i].length + 1 <= maxLength) {
        snippet += ' ' + sentences[i];
      } else {
        break;
      }
    }
    return snippet;
  }

  // Fallback: truncate at word boundary
  const truncated = text.slice(0, maxLength);
  const lastSpace = truncated.lastIndexOf(' ');
  if (lastSpace > maxLength * 0.7) {
    return truncated.slice(0, lastSpace) + '…';
  }
  return truncated + '…';
}

/**
 * Build a SourcesPack from scored chunks - the single source of truth for sources/citations.
 *
 * IMPORTANT: No filtering here! All chunks passed in are "source-worthy"
 * (already filtered by MIN_COMBINED_SCORE in retrieval).
 * This ensures prompt source count == citationsMap.size EXACTLY.
 *
 * @param chunks - The exact chunks to use as sources (already filtered/reranked)
 * @returns SourcesPack with 1:1 mapping between sources and citations
 */
function buildSourcesPack(chunks: ScoredChunk[]): SourcesPack {
  const citationsMap = new Map<string, Citation>();

  // Create 1:1 mapping - every chunk becomes a citation
  chunks.forEach((chunk, index) => {
    const cid = `N${index + 1}`;
    citationsMap.set(cid, {
      cid,
      noteId: chunk.noteId,
      chunkId: chunk.chunkId,
      createdAt: chunk.createdAt.toISOString(),
      snippet: extractBestSnippet(chunk.text, 250),
      score: Math.round(chunk.score * 100) / 100,
    });
  });

  return {
    sources: chunks,
    citationsMap,
    sourceCount: chunks.length, // Equals citationsMap.size
  };
}

/**
 * Extract key topics from chunks for context hints
 */
function extractTopicsFromChunks(chunks: ScoredChunk[]): string[] {
  const topicPatterns = [
    /\b(meeting|sprint|planning|decision|architecture|design)\b/gi,
    /\b(RAG|pipeline|chunking|embedding|retrieval|vector)\b/gi,
    /\b(Cloud Run|Firestore|API|backend|frontend)\b/gi,
    /\b(pagination|scaling|performance|optimization)\b/gi,
  ];

  const topics = new Set<string>();
  const allText = chunks.map(c => c.text).join(' ').toLowerCase();

  for (const pattern of topicPatterns) {
    const matches = allText.match(pattern);
    if (matches) {
      matches.slice(0, 3).forEach(m => topics.add(m.toLowerCase()));
    }
  }

  return Array.from(topics).slice(0, 5);
}

/**
 * Get intent-specific formatting instructions
 */
function getIntentInstructions(intent: QueryIntent): string {
  switch (intent) {
    case 'summarize':
      return `FORMAT: Summary request - structure as:
• Brief overview (1-2 sentences) with citation
• Key points as bullet list, each citing its source [N#]
• Keep it concise, focus on most important information`;

    case 'list':
      return `FORMAT: List request - structure as:
• Numbered or bulleted list
• Each item concise and cited [N#]
• Group related items if applicable`;

    case 'decision':
      return `FORMAT: Decision inquiry - structure as:
• State what was decided with citation [N#]
• Key reasons/rationale with citations
• Any trade-offs or alternatives mentioned`;

    case 'action_item':
      return `FORMAT: Action item request - structure as:
• Clear list of action items/todos
• Include any deadlines, owners, or priorities mentioned
• Cite the source for each item [N#]`;

    case 'question':
      return `FORMAT: Direct question - structure as:
• Answer directly in the first sentence with citation [N#]
• Add supporting details if relevant
• Be concise and precise`;

    default:
      return '';
  }
}

/**
 * Build the RAG prompt with sources and intent-aware instructions.
 * Uses SourcesPack to ensure prompt source count == citationsMap.size EXACTLY.
 */
function buildPrompt(
  query: string,
  sourcesPack: SourcesPack,
  intent: QueryIntent = 'search'
): string {
  const { sources, citationsMap, sourceCount } = sourcesPack;

  // Format sources with clear structure and metadata
  // Uses citationsMap to ensure 1:1 correspondence with prompt source count
  const sourcesText = Array.from(citationsMap.entries())
    .map(([cid, citation]) => {
      const chunk = sources.find(c => c.chunkId === citation.chunkId);
      const text = chunk?.text || citation.snippet;
      const date = new Date(citation.createdAt).toLocaleDateString('en-US', {
        month: 'short', day: 'numeric', year: 'numeric'
      });
      return `[${cid}] (${date}):\n${text}`;
    })
    .join('\n\n---\n\n');

  // Extract topics from notes for context
  const noteTopics = extractTopicsFromChunks(sources);
  const topicsHint = noteTopics.length > 0
    ? `The notes contain information about: ${noteTopics.join(', ')}.`
    : '';

  // Get intent-specific instructions
  const intentInstructions = getIntentInstructions(intent);
  const intentSection = intentInstructions ? `\n${intentInstructions}` : '';

  // CRITICAL: Use sourceCount (== citationsMap.size) for all source count references
  // This ensures the LLM prompt source count matches the valid citation range exactly
  return `You are an intelligent assistant helping the user with their personal notes. Answer questions using ONLY the provided note excerpts.

CITATION FORMAT (CRITICAL):
• Format: [N1], [N2], [N3], etc.
• Place IMMEDIATELY after the fact, BEFORE punctuation: "The budget is $50,000 [N1]."
• Multiple sources for same fact: "decided on AWS [N1][N3]"
• Different facts from different sources: "uses React [N1] and PostgreSQL [N2]"

CITATION REQUIREMENTS:
1. EVERY factual statement MUST have at least one citation
2. You have exactly ${sourceCount} sources - aim to cite most of them if they're relevant
3. NEVER make up citation IDs - only use N1 through N${sourceCount}
4. If a source is relevant to your answer, you MUST cite it somewhere
5. Check each source and ask: "Did I cite this?" If relevant and not cited, add it.

EXAMPLE (with 3 sources):
Bad: "The project uses React and costs $50,000." (missing citations)
Good: "The project uses React [N1] and costs $50,000 [N2]. The timeline is 6 months [N3]."

RESPONSE GUIDELINES:
• Answer the question directly first, then elaborate
• Only state facts from the sources - never infer or assume
• If sources don't fully answer, acknowledge what's missing
${intentSection}

${topicsHint}

=== USER'S NOTE EXCERPTS (${sourceCount} sources) ===
${sourcesText}
=== END OF NOTES ===

Question: ${query}

Answer using the notes above. EVERY fact must have a citation [N#]:`;
}

// NOTE: Citation validation functions (validateCitations, extractVerificationKeywords,
// calculateOverlapScore, verifyCitationRelevance) have been consolidated into
// src/citationValidator.ts as the single canonical validation module.
// Use validateCitationsWithChunks() for all citation validation.

/**
 * Build a repair prompt to fix missing citations
 */
function buildCitationRepairPrompt(
  originalAnswer: string,
  citations: Map<string, Citation>
): string {
  const citationList = Array.from(citations.entries())
    .map(([cid, c]) => `${cid}: "${c.snippet}"`)
    .join('\n');

  return `Your previous answer was missing citations. Rewrite it with COMPREHENSIVE citation references.

AVAILABLE SOURCES (${citations.size} total):
${citationList}

ORIGINAL ANSWER (needs citations added):
${originalAnswer}

CITATION REQUIREMENTS:
1. EVERY factual claim MUST have at least one citation [N#] immediately after it
2. Place citations BEFORE punctuation: "fact [N1]." not "fact. [N1]"
3. When combining info from multiple sources, cite ALL: "X uses React [N1] and Node [N2]"
4. If the same fact appears in multiple sources, cite all of them: "decided on X [N1][N3]"
5. ONLY use citation IDs from the list above (N1, N2, etc.)
6. Do NOT change the meaning or add new information
7. Review EVERY source and find where it should be cited

EXAMPLE:
Original: "The project uses React for frontend and PostgreSQL for database."
With citations: "The project uses React for frontend [N1] and PostgreSQL for database [N2][N4]."

Rewrite the answer with comprehensive citations (aim to cite most or all sources):`;
}

/**
 * Generate chat response with RAG
 */
export async function generateChatResponse(request: ChatRequest): Promise<ChatResponse> {
  const startTime = Date.now();

  // Sanitize and validate input
  const query = sanitizeText(request.message, CHAT_MAX_QUERY_LENGTH + 100).trim();
  const tenantId = request.tenantId || DEFAULT_TENANT_ID;

  // Initialize structured retrieval log (generate requestId if not provided)
  const retrievalLog = createRetrievalLog(tenantId, query) as RetrievalLogEntry;

  // Timing metrics for observability
  const timing: RetrievalTimings = {
    totalMs: 0,
  };

  // Quality flags for logging
  const qualityFlags: QualityFlags = {
    citationCoveragePct: 0,
    invalidCitationsRemoved: 0,
    fallbackUsed: false,
    insufficientEvidence: false,
    regenerationAttempted: false,
  };

  if (!query) {
    throw new Error('message is required');
  }

  if (query.length > CHAT_MAX_QUERY_LENGTH) {
    throw new Error(`message too long (max ${CHAT_MAX_QUERY_LENGTH} chars)`);
  }

  if (!isValidTenantId(tenantId)) {
    throw new Error('invalid tenantId format');
  }

  // Analyze query for intent and keywords
  const queryAnalysis = analyzeQuery(query);

  // Retrieve relevant chunks
  const retrievalStart = Date.now();
  let { chunks, strategy, candidateCount, candidateCounts } = await retrieveRelevantChunks(query, {
    tenantId,
    topK: RETRIEVAL_TOP_K,
    rerankTo: Math.min(RETRIEVAL_RERANK_TO, MAX_CHUNKS_IN_CONTEXT),
  });
  timing.retrievalMs = Date.now() - retrievalStart;

  // Handle no results
  if (chunks.length === 0) {
    return {
      answer: "I don't have any notes to search through. Try creating some notes first!",
      citations: [],
      meta: {
        model: CHAT_MODEL,
        retrieval: { k: 0, strategy: 'no_results', intent: queryAnalysis.intent },
      },
    };
  }

  // Build SourcesPack - single source of truth for sources/citations
  // This ensures prompt source count == citationsMap.size EXACTLY
  const sourcesPack = buildSourcesPack(chunks);
  const { citationsMap, sourceCount } = sourcesPack;

  // Build prompt with intent-aware instructions using SourcesPack
  const prompt = buildPrompt(query, sourcesPack, queryAnalysis.intent);

  // Call LLM with retry logic
  const client = getGenAIClient();
  let answer: string;

  const generationStart = Date.now();
  try {
    const result = await withLLMRetry(async () => {
      return await client.models.generateContent({
        model: CHAT_MODEL,
        contents: prompt,
        config: {
          temperature: CHAT_TEMPERATURE,
          maxOutputTokens: 1024,
        },
      });
    }, 'LLM generation');

    answer = result.text || '';
    timing.generationMs = Date.now() - generationStart;

    if (!answer) {
      throw new Error('Empty response from model');
    }
  } catch (err) {
    timing.generationMs = Date.now() - generationStart;
    if (err instanceof RateLimitError) {
      logError('LLM rate limit hit', err);
      throw err; // Let the handler return 429
    }
    logError('LLM generation failed', err);
    throw new Error('Failed to generate response');
  }

  // Unified citation validation pipeline using citationValidator
  // This consolidates: invalid citation removal, formatting cleanup, overlap verification
  const citationsList = Array.from(citationsMap.values());
  const validationResult = validateCitationsWithChunks(
    answer,
    citationsList,
    chunks,
    {
      strictMode: true,
      minOverlapScore: CITATION_MIN_OVERLAP_SCORE,
      verifyRelevance: CITATION_VERIFICATION_ENABLED,
      requestId: retrievalLog.requestId,
    }
  );

  let cleanedAnswer = validationResult.validatedAnswer;
  let usedCitations = validationResult.validatedCitations;
  // Use a function to check hasCitations dynamically (usedCitations may be updated by repair)
  const checkHasCitations = () => usedCitations.length > 0;

  // Track citation quality metrics
  const totalRemovedCount = validationResult.invalidCitationsRemoved.length + validationResult.droppedCitations.length;
  if (totalRemovedCount > 0) {
    qualityFlags.invalidCitationsRemoved = totalRemovedCount;
  }

  // Detect if response looks like uncertainty about the question
  const looksLikeUncertainty =
    cleanedAnswer.toLowerCase().includes("don't have") ||
    cleanedAnswer.toLowerCase().includes("don't see") ||
    cleanedAnswer.toLowerCase().includes("cannot find") ||
    cleanedAnswer.toLowerCase().includes("no notes about") ||
    cleanedAnswer.toLowerCase().includes("no information");

  // Calculate citation coverage using sourceCount (== citationsMap.size)
  // This ensures we compute coverage against the EXACT number of sources in the prompt
  const citationCoverage = sourceCount > 0 ? usedCitations.length / sourceCount : 1;
  const hasLowCoverage = sourceCount >= 3 && citationCoverage < MIN_CITATION_COVERAGE && !looksLikeUncertainty;

  // Retry if no citations found OR low citation coverage (citation repair)
  if ((!checkHasCitations() || hasLowCoverage) && !looksLikeUncertainty && CITATION_RETRY_ENABLED && sourceCount > 0) {
    const repairStart = Date.now();
    const repairReason = !checkHasCitations() ? 'no citations' : `low coverage (${Math.round(citationCoverage * 100)}%)`;
    qualityFlags.regenerationAttempted = true;
    logInfo('Attempting citation repair', { reason: repairReason, citationCount: usedCitations.length, sourceCount });

    try {
      const repairPrompt = buildCitationRepairPrompt(answer, citationsMap);
      // Use shorter timeout for repair since it's a secondary operation
      const repairResult = await withLLMRetry(async () => {
        return await client.models.generateContent({
          model: CHAT_MODEL,
          contents: repairPrompt,
          config: {
            temperature: 0.1, // Low temp for repair
            maxOutputTokens: 1024,
          },
        });
      }, 'Citation repair', CHAT_TIMEOUT_MS / 2);

      const repairedAnswer = repairResult.text || '';
      if (repairedAnswer) {
        // Use unified validation for repaired answer
        const repairedValidation = validateCitationsWithChunks(
          repairedAnswer,
          citationsList,
          chunks,
          {
            strictMode: true,
            minOverlapScore: CITATION_MIN_OVERLAP_SCORE,
            verifyRelevance: CITATION_VERIFICATION_ENABLED,
            requestId: retrievalLog.requestId,
          }
        );
        const repairedHasCitations = repairedValidation.validatedCitations.length > 0;
        // Accept repair if it improved citation coverage (using sourceCount)
        const repairedCoverage = repairedValidation.validatedCitations.length / sourceCount;
        if (repairedHasCitations && repairedCoverage > citationCoverage) {
          cleanedAnswer = repairedValidation.validatedAnswer;
          usedCitations = repairedValidation.validatedCitations;
          strategy += '_repaired';
          logInfo('Citation repair successful', {
            citationCount: usedCitations.length,
            coverageBefore: Math.round(citationCoverage * 100),
            coverageAfter: Math.round(repairedCoverage * 100),
          });
        } else {
          logWarn('Citation repair did not improve coverage, using original');
        }
      }
      timing.repairMs = Date.now() - repairStart;
    } catch (repairErr) {
      timing.repairMs = Date.now() - repairStart;
      logError('Citation repair error', repairErr);
      // Continue with original answer
    }
  }

  // If still no valid citations and answer doesn't acknowledge uncertainty, provide helpful fallback
  if (!checkHasCitations() && !looksLikeUncertainty) {
    qualityFlags.insufficientEvidence = true;
    qualityFlags.fallbackUsed = true;
    logInfo('No citations found, using fallback response');

    // Build a helpful response mentioning what topics ARE in the notes
    const noteTopics = extractTopicsFromChunks(chunks);
    let fallbackAnswer: string;

    if (noteTopics.length > 0) {
      fallbackAnswer = `I couldn't find notes specifically about that. Your notes currently cover topics like ${noteTopics.join(', ')}. Try creating a note about what you're looking for!`;
    } else {
      fallbackAnswer = "I couldn't find notes about that topic. Try rephrasing your question, or create a note about this topic so I can help you next time!";
    }

    return {
      answer: fallbackAnswer,
      citations: [],
      meta: {
        model: CHAT_MODEL,
        retrieval: {
          k: chunks.length,
          strategy,
          candidateCount,
          rerankCount: chunks.length,
          timeMs: Date.now() - startTime,
        },
      },
    };
  }

  timing.totalMs = Date.now() - startTime;
  // Use sourceCount (== citationsMap.size) for consistent coverage calculation
  const finalCoverage = sourceCount > 0 ? Math.round((usedCitations.length / sourceCount) * 100) : 100;

  // Build citation log entries
  const citationLogEntries: CitationLogEntry[] = usedCitations.map(c => ({
    cid: c.cid,
    noteId: c.noteId,
    chunkId: c.chunkId,
    score: c.score,
    snippetLength: c.snippet.length,
  }));

  // Update quality flags
  qualityFlags.citationCoveragePct = finalCoverage;

  // Determine retrieval mode
  const retrievalMode = strategy.includes('hybrid') ? 'hybrid' :
    strategy.includes('vector') ? 'vector' :
    strategy.includes('fallback') ? 'fallback' : 'keyword_only';

  // Complete the retrieval log entry with comprehensive observability
  const finalLog: RetrievalLogEntry = {
    ...retrievalLog,
    intent: queryAnalysis.intent,
    retrievalMode: retrievalMode as 'vector' | 'hybrid' | 'keyword_only' | 'fallback',
    candidateCounts: {
      vectorK: candidateCounts?.vectorK || 0,
      keywordK: candidateCounts?.lexicalK || 0,
      mergedK: candidateCounts?.mergedK || candidateCount,
      afterRerank: candidateCounts?.rerankedK || chunks.length,
      finalChunks: candidateCounts?.finalK || chunks.length,
    },
    // Add detailed stage counts for debugging retrieval issues
    stageDetails: candidateCounts ? candidateCountsToStageDetails(candidateCounts) : undefined,
    // Score distribution helps identify single-source dominance or sparse results
    scoreDistribution: computeScoreDistribution(chunks),
    rerankMethod: strategy,
    citations: citationLogEntries,
    timings: timing,
    quality: qualityFlags,
    answerLength: cleanedAnswer.length,
  };

  // Log the structured retrieval trace
  logRetrieval(finalLog);

  // Log comprehensive metrics for observability (existing log)
  logInfo('Chat response generated', {
    requestId: retrievalLog.requestId,
    queryLength: query.length,
    intent: queryAnalysis.intent,
    // Counts
    candidatesFetched: candidateCount,
    chunksUsed: chunks.length,
    citationsUsed: usedCitations.length,
    citationCoverage: `${finalCoverage}%`,
    strategy,
    // Latency breakdown
    timing: {
      retrievalMs: timing.retrievalMs || 0,
      generationMs: timing.generationMs || 0,
      repairMs: timing.repairMs || 0,
      totalMs: timing.totalMs,
    },
  });

  // Warn if coverage is below strict threshold (helps identify issues in production)
  if (finalCoverage < MIN_CITATION_COVERAGE_STRICT * 100 && sourceCount >= 3 && !looksLikeUncertainty) {
    logWarn('Low citation coverage detected', {
      requestId: retrievalLog.requestId,
      coverage: `${finalCoverage}%`,
      threshold: `${MIN_CITATION_COVERAGE_STRICT * 100}%`,
      citationCount: usedCitations.length,
      sourceCount,
      query: query.slice(0, 100),
    });
  }

  return {
    answer: cleanedAnswer,
    citations: usedCitations,
    meta: {
      model: CHAT_MODEL,
      retrieval: {
        k: chunks.length,
        strategy,
        candidateCount,
        rerankCount: chunks.length,
        intent: queryAnalysis.intent,
        timeMs: timing.totalMs,
        // Include timing breakdown for debugging (without exposing in API)
      },
    },
  };
}



================================================================================
FILE: src/chunking.ts
LINES: 463
PATH: /Users/salscrudato/Projects/auroranotes-api/src/chunking.ts
================================================================================

/**
 * AuroraNotes API - Chunking Pipeline
 *
 * Splits notes into semantic chunks for embedding and retrieval.
 * Uses improved semantic boundary detection and context preservation.
 */

import { FieldValue, Timestamp, WriteBatch } from "firebase-admin/firestore";
import { getDb } from "./firestore";
import {
  CHUNKS_COLLECTION,
  CHUNK_TARGET_SIZE,
  CHUNK_MIN_SIZE,
  CHUNK_MAX_SIZE,
  CHUNK_OVERLAP,
  EMBEDDINGS_ENABLED,
  VERTEX_VECTOR_SEARCH_ENABLED,
} from "./config";
import { NoteDoc, ChunkDoc } from "./types";
import { hashText, estimateTokens, logInfo, logError, logWarn, extractTermsForIndexing, TERMS_VERSION } from "./utils";
import { generateEmbeddings, EmbeddingError } from "./embeddings";
import { getVertexIndex, VertexDatapoint } from "./vectorIndex";

// Semantic boundary patterns (ordered by preference)
const PARAGRAPH_BOUNDARY = /\n\n+/;
const SENTENCE_BOUNDARY = /(?<=[.!?])\s+(?=[A-Z])/;
const LIST_BOUNDARY = /\n(?=[-*•]|\d+\.)/;
const COLON_BOUNDARY = /:\s*\n/;

/**
 * Split text into semantic units (paragraphs, then sentences)
 */
function splitIntoSemanticUnits(text: string): string[] {
  // First split by paragraphs
  const paragraphs = text.split(PARAGRAPH_BOUNDARY).filter(p => p.trim());

  const units: string[] = [];

  for (const para of paragraphs) {
    // If paragraph is small enough, keep it as one unit
    if (para.length <= CHUNK_TARGET_SIZE) {
      units.push(para.trim());
    } else {
      // Split long paragraphs into sentences
      const sentences = para.split(/(?<=[.!?])\s+/).filter(s => s.trim());
      units.push(...sentences.map(s => s.trim()));
    }
  }

  return units;
}

/**
 * Split text into chunks using improved semantic boundary detection
 */
export function splitIntoChunks(text: string): string[] {
  const normalizedText = text.replace(/\r\n/g, '\n').trim();

  // Short text: return as single chunk if it meets minimum size
  if (normalizedText.length <= CHUNK_MAX_SIZE) {
    return normalizedText.length >= CHUNK_MIN_SIZE ? [normalizedText] : [];
  }

  const units = splitIntoSemanticUnits(normalizedText);
  const chunks: string[] = [];
  let currentChunk = '';
  let previousContext = ''; // Store context for overlap

  for (const unit of units) {
    const trimmedUnit = unit.trim();
    if (!trimmedUnit) continue;

    const potentialLength = currentChunk.length + (currentChunk ? 1 : 0) + trimmedUnit.length;

    // If adding this unit exceeds max, finalize current chunk
    if (potentialLength > CHUNK_MAX_SIZE && currentChunk.length >= CHUNK_MIN_SIZE) {
      chunks.push(currentChunk);

      // Create overlap context from the end of previous chunk
      previousContext = extractOverlapContext(currentChunk, CHUNK_OVERLAP);
      currentChunk = previousContext ? previousContext + ' ' + trimmedUnit : trimmedUnit;
    } else if (potentialLength > CHUNK_MAX_SIZE && currentChunk.length < CHUNK_MIN_SIZE) {
      // Current chunk too small but adding unit exceeds max - force add
      currentChunk = currentChunk ? currentChunk + ' ' + trimmedUnit : trimmedUnit;

      // If now exceeds max, force split
      if (currentChunk.length > CHUNK_MAX_SIZE) {
        const splitPoint = findBestSplitPoint(currentChunk, CHUNK_TARGET_SIZE);
        chunks.push(currentChunk.slice(0, splitPoint).trim());
        previousContext = extractOverlapContext(currentChunk.slice(0, splitPoint), CHUNK_OVERLAP);
        currentChunk = previousContext + ' ' + currentChunk.slice(splitPoint).trim();
      }
    } else {
      // Add unit to current chunk
      currentChunk = currentChunk ? currentChunk + ' ' + trimmedUnit : trimmedUnit;
    }

    // Check if we're at a good size to finalize
    if (currentChunk.length >= CHUNK_TARGET_SIZE && currentChunk.length <= CHUNK_MAX_SIZE) {
      // Look for a natural break point
      const breakPoint = findNaturalBreak(currentChunk, CHUNK_TARGET_SIZE);
      if (breakPoint > CHUNK_MIN_SIZE && breakPoint < currentChunk.length - 50) {
        chunks.push(currentChunk.slice(0, breakPoint).trim());
        previousContext = extractOverlapContext(currentChunk.slice(0, breakPoint), CHUNK_OVERLAP);
        currentChunk = previousContext + ' ' + currentChunk.slice(breakPoint).trim();
      }
    }
  }

  // Handle remaining text
  if (currentChunk.trim()) {
    if (currentChunk.length >= CHUNK_MIN_SIZE) {
      chunks.push(currentChunk.trim());
    } else if (chunks.length > 0) {
      // Merge small remainder with last chunk if possible
      const lastChunk = chunks[chunks.length - 1];
      if (lastChunk.length + currentChunk.length + 1 <= CHUNK_MAX_SIZE) {
        chunks[chunks.length - 1] = lastChunk + ' ' + currentChunk.trim();
      } else {
        // Keep as separate chunk even if small
        chunks.push(currentChunk.trim());
      }
    } else {
      // Only chunk and it's small - keep it anyway
      chunks.push(currentChunk.trim());
    }
  }

  return chunks.filter(c => c.length > 0);
}

/**
 * Extract context for overlap, preferring sentence boundaries
 */
function extractOverlapContext(text: string, maxLength: number): string {
  if (text.length <= maxLength) return text;

  const suffix = text.slice(-maxLength);

  // Try to start at a sentence boundary
  const sentenceStart = suffix.search(/(?<=[.!?])\s+/);
  if (sentenceStart > 10) {
    return suffix.slice(sentenceStart).trim();
  }

  // Fall back to word boundary
  const wordStart = suffix.indexOf(' ');
  if (wordStart > 0) {
    return suffix.slice(wordStart).trim();
  }

  return suffix.trim();
}

/**
 * Find the best split point near target, preferring sentence boundaries
 */
function findBestSplitPoint(text: string, target: number): number {
  const searchStart = Math.max(0, target - 100);
  const searchEnd = Math.min(text.length, target + 100);
  const window = text.slice(searchStart, searchEnd);

  // Prefer sentence endings
  const sentenceEnd = window.search(/[.!?]\s+/);
  if (sentenceEnd > 0) {
    return searchStart + sentenceEnd + 2;
  }

  // Fall back to comma or semicolon
  const clauseEnd = window.search(/[,;]\s+/);
  if (clauseEnd > 0) {
    return searchStart + clauseEnd + 2;
  }

  // Fall back to space
  const lastSpace = window.lastIndexOf(' ');
  if (lastSpace > 0) {
    return searchStart + lastSpace;
  }

  return target;
}

/**
 * Find a natural break point in text
 */
function findNaturalBreak(text: string, target: number): number {
  return findBestSplitPoint(text, target);
}

/**
 * Compute a hash of the full note text for idempotency checking
 */
function computeNoteTextHash(text: string): string {
  return hashText(text);
}

/**
 * Process a note into chunks and store them
 *
 * IDEMPOTENT: Skips processing if note text hasn't changed (based on hash).
 * Only regenerates embeddings for chunks that are missing them.
 */
export async function processNoteChunks(note: NoteDoc): Promise<void> {
  const db = getDb();
  const startTime = Date.now();
  const noteTextHash = computeNoteTextHash(note.text);

  try {
    // Fetch existing chunks for this note (with fallback if index missing)
    let existingChunks: ChunkDoc[] = [];
    try {
      const existingChunksSnap = await db
        .collection(CHUNKS_COLLECTION)
        .where('noteId', '==', note.id)
        .orderBy('position', 'asc')
        .get();
      existingChunks = existingChunksSnap.docs.map(d => d.data() as ChunkDoc);
    } catch (indexErr: unknown) {
      const errMsg = indexErr instanceof Error ? indexErr.message : String(indexErr);
      if (errMsg.includes('FAILED_PRECONDITION') || errMsg.includes('requires an index')) {
        // Fallback: query without orderBy, sort in memory
        const fallbackSnap = await db
          .collection(CHUNKS_COLLECTION)
          .where('noteId', '==', note.id)
          .get();
        existingChunks = fallbackSnap.docs
          .map(d => d.data() as ChunkDoc)
          .sort((a, b) => a.position - b.position);
      } else {
        throw indexErr;
      }
    }

    // Check if note text has changed by comparing content hashes
    // If chunks exist and their combined hashes match, skip reprocessing
    if (existingChunks.length > 0) {
      const existingHashes = existingChunks.map(c => c.textHash).join('|');
      const newTextChunks = splitIntoChunks(note.text);
      const newHashes = newTextChunks.map(t => hashText(t)).join('|');

      if (existingHashes === newHashes) {
        // Note hasn't changed - check if any chunks need embeddings
        const chunksMissingEmbeddings = existingChunks.filter(c => !c.embedding);

        if (chunksMissingEmbeddings.length === 0) {
          logInfo('Note unchanged and all embeddings present, skipping', {
            noteId: note.id,
            chunkCount: existingChunks.length,
          });
          return;
        }

        // Only regenerate missing embeddings
        if (EMBEDDINGS_ENABLED && chunksMissingEmbeddings.length > 0) {
          logInfo('Regenerating missing embeddings', {
            noteId: note.id,
            missingCount: chunksMissingEmbeddings.length,
            totalChunks: existingChunks.length,
          });

          try {
            const textsToEmbed = chunksMissingEmbeddings.map(c => c.text);
            const embeddings = await generateEmbeddings(textsToEmbed);

            // generateEmbeddings now guarantees embeddings.length === textsToEmbed.length
            // or throws EmbeddingError. Safe to iterate 1:1.
            const batch = db.batch();
            for (let i = 0; i < chunksMissingEmbeddings.length; i++) {
              const chunkRef = db.collection(CHUNKS_COLLECTION).doc(chunksMissingEmbeddings[i].chunkId);
              batch.update(chunkRef, {
                embedding: embeddings[i],
                embeddingModel: 'text-embedding-004',
              });
            }
            await batch.commit();

            logInfo('Missing embeddings regenerated', {
              noteId: note.id,
              embeddingsAdded: chunksMissingEmbeddings.length,
              elapsedMs: Date.now() - startTime,
            });
          } catch (err) {
            // Log embedding errors with details but continue without embeddings
            if (err instanceof EmbeddingError) {
              logError('Embedding regeneration failed with misalignment', err, {
                noteId: note.id,
                missingIndices: err.missingIndices,
              });
            } else {
              logError('Embedding regeneration failed', err, { noteId: note.id });
            }
          }
        }
        return;
      }
    }

    // Note has changed - full reprocessing required
    logInfo('Note changed, reprocessing chunks', {
      noteId: note.id,
      hadExistingChunks: existingChunks.length > 0,
    });

    // Delete existing chunks by fetching fresh references
    if (existingChunks.length > 0) {
      const deleteBatch = db.batch();
      for (const chunk of existingChunks) {
        const docRef = db.collection(CHUNKS_COLLECTION).doc(chunk.chunkId);
        deleteBatch.delete(docRef);
      }
      await deleteBatch.commit();
    }

    // Split note into chunks
    const textChunks = splitIntoChunks(note.text);

    if (textChunks.length === 0) {
      logInfo('Note too short for chunking', { noteId: note.id });
      return;
    }

    // Create chunk documents with terms for lexical indexing
    const chunks: ChunkDoc[] = textChunks.map((text, position) => ({
      chunkId: `${note.id}_${String(position).padStart(3, '0')}`,
      noteId: note.id,
      tenantId: note.tenantId,
      text,
      textHash: hashText(text),
      position,
      tokenEstimate: estimateTokens(text),
      createdAt: note.createdAt,
      // Lexical indexing fields
      terms: extractTermsForIndexing(text),
      termsVersion: TERMS_VERSION,
    }));

    // Generate embeddings if enabled
    if (EMBEDDINGS_ENABLED) {
      try {
        const embeddings = await generateEmbeddings(textChunks);
        // generateEmbeddings now guarantees embeddings.length === textChunks.length
        // or throws EmbeddingError. Safe to iterate 1:1.
        for (let i = 0; i < chunks.length; i++) {
          chunks[i].embedding = embeddings[i];
          chunks[i].embeddingModel = 'text-embedding-004';
        }
      } catch (err) {
        // Log embedding errors with details but continue without embeddings
        if (err instanceof EmbeddingError) {
          logError('Embedding generation failed with misalignment', err, {
            noteId: note.id,
            missingIndices: err.missingIndices,
          });
        } else {
          logError('Embedding generation failed', err, { noteId: note.id });
        }
        // Continue without embeddings - retrieval will fall back to keyword search
      }
    }

    // Store chunks in batches (Firestore limit: 500 per batch)
    const BATCH_SIZE = 400;
    for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
      const batch = db.batch();
      const batchChunks = chunks.slice(i, i + BATCH_SIZE);

      for (const chunk of batchChunks) {
        const ref = db.collection(CHUNKS_COLLECTION).doc(chunk.chunkId);
        batch.set(ref, chunk);
      }

      await batch.commit();
    }

    // Sync to Vertex AI Vector Search if enabled
    await syncChunksToVertexIndex(chunks);

    const elapsedMs = Date.now() - startTime;
    logInfo('Chunks processed', {
      noteId: note.id,
      chunkCount: chunks.length,
      hasEmbeddings: chunks[0]?.embedding !== undefined,
      elapsedMs,
    });
  } catch (err) {
    logError('Chunk processing failed', err, { noteId: note.id });
    throw err;
  }
}

/**
 * Get all chunks for a note
 */
export async function getChunksForNote(noteId: string): Promise<ChunkDoc[]> {
  const db = getDb();
  const snap = await db
    .collection(CHUNKS_COLLECTION)
    .where('noteId', '==', noteId)
    .orderBy('position', 'asc')
    .get();

  return snap.docs.map(d => d.data() as ChunkDoc);
}

/**
 * Sync chunks to Vertex AI Vector Search index
 *
 * This is called after chunks are saved to Firestore.
 * Only syncs chunks that have embeddings.
 * Fails silently to avoid blocking note creation.
 */
async function syncChunksToVertexIndex(chunks: ChunkDoc[]): Promise<void> {
  if (!VERTEX_VECTOR_SEARCH_ENABLED) {
    return;
  }

  const vertexIndex = getVertexIndex();
  if (!vertexIndex) {
    return;
  }

  // Filter to chunks with embeddings
  const chunksWithEmbeddings = chunks.filter(c => c.embedding && c.embedding.length > 0);
  if (chunksWithEmbeddings.length === 0) {
    return;
  }

  // Convert to Vertex datapoints
  const datapoints: VertexDatapoint[] = chunksWithEmbeddings.map(chunk => ({
    datapointId: `${chunk.chunkId}:${chunk.noteId}`,
    featureVector: chunk.embedding!,
    restricts: [
      {
        namespace: 'tenantId',
        allowList: [chunk.tenantId],
      },
    ],
  }));

  try {
    const success = await vertexIndex.upsert(datapoints);
    if (success) {
      logInfo('Synced chunks to Vertex index', {
        chunkCount: datapoints.length,
        noteId: chunks[0]?.noteId,
      });
    } else {
      logWarn('Failed to sync chunks to Vertex index', {
        chunkCount: datapoints.length,
        noteId: chunks[0]?.noteId,
      });
    }
  } catch (err) {
    // Log but don't throw - Vertex sync is best-effort
    logError('Vertex index sync error', err, {
      chunkCount: datapoints.length,
      noteId: chunks[0]?.noteId,
    });
  }
}



================================================================================
FILE: src/citationValidator.ts
LINES: 387
PATH: /Users/salscrudato/Projects/auroranotes-api/src/citationValidator.ts
================================================================================

/**
 * AuroraNotes API - Citation Validator
 *
 * Unified citation validation pipeline for RAG answers:
 * - Parse citation tokens from text
 * - Remove invalid citations (not in source list)
 * - Reorder citations by first appearance
 * - Compute citation coverage (sentence-level)
 * - Verify citation relevance using keyword overlap
 * - Clean formatting (duplicate citations, spacing)
 *
 * This is the SINGLE canonical validation module for all citation operations.
 */

import { logWarn, logInfo } from './utils';
import { Citation, ScoredChunk } from './types';

// Re-export Citation type for convenience
export type { Citation } from './types';

// Configuration for overlap verification
const DEFAULT_MIN_OVERLAP_SCORE = 0.15;  // Min keyword overlap for validity

/**
 * Result from citation validation pipeline
 */
export interface ValidationResult {
  validatedAnswer: string;
  validatedCitations: Citation[];
  invalidCitationsRemoved: string[];
  droppedCitations: string[];        // Citations dropped due to low overlap
  suspiciousCitations: string[];     // Citations with low but non-zero overlap
  citationCoveragePct: number;
  allCitationsValid: boolean;
  orderedByFirstAppearance: boolean;
  overlapScores: Map<string, number>;  // Overlap scores for each citation
}

/**
 * Options for citation validation
 */
export interface ValidationOptions {
  strictMode?: boolean;           // Drop citations below overlap threshold
  minOverlapScore?: number;       // Min overlap score (default: 0.15)
  verifyRelevance?: boolean;      // Whether to verify overlap (default: true)
  requestId?: string;             // For logging
}

/**
 * Parse citation tokens from answer text
 * Returns array of cid strings (e.g., "N1", "N2")
 */
export function parseCitationTokens(answer: string): string[] {
  const pattern = /\[N(\d+)\]/g;
  const tokens: string[] = [];
  let match;
  while ((match = pattern.exec(answer)) !== null) {
    tokens.push(`N${match[1]}`);
  }
  return tokens;
}

/**
 * Get unique citation IDs in order of first appearance
 */
export function getOrderedUniqueCitations(answer: string): string[] {
  const tokens = parseCitationTokens(answer);
  const seen = new Set<string>();
  const ordered: string[] = [];
  for (const token of tokens) {
    if (!seen.has(token)) {
      seen.add(token);
      ordered.push(token);
    }
  }
  return ordered;
}

/**
 * Remove invalid citation tokens from answer text
 */
export function removeInvalidCitations(
  answer: string,
  validCids: Set<string>
): { cleaned: string; removed: string[] } {
  const removed: string[] = [];
  
  const cleaned = answer.replace(/\[N(\d+)\]/g, (match, num) => {
    const cid = `N${num}`;
    if (validCids.has(cid)) {
      return match; // Keep valid citation
    } else {
      removed.push(cid);
      return ''; // Remove invalid citation
    }
  });
  
  return { cleaned: cleaned.replace(/\s+/g, ' ').trim(), removed };
}

/**
 * Calculate citation coverage: % of factual sentences with citations
 */
export function calculateCitationCoverage(answer: string): number {
  // Split into sentences
  const sentences = answer
    .split(/(?<=[.!?])\s+/)
    .map(s => s.trim())
    .filter(s => s.length > 15); // Substantial sentences only

  if (sentences.length === 0) return 100;

  // Count sentences with at least one citation
  const citedCount = sentences.filter(s => /\[N\d+\]/.test(s)).length;

  return Math.round((citedCount / sentences.length) * 100);
}

/**
 * Clean up citation formatting issues in answer
 */
export function cleanCitationFormatting(answer: string): string {
  return answer
    // Remove duplicate adjacent citations [N1][N1] -> [N1]
    .replace(/(\[N\d+\])(\s*\1)+/g, '$1')
    // Clean up spaces around citations: "word [N1] ." -> "word [N1]."
    .replace(/\s+([.!?,;:])/g, '$1')
    // Fix multiple spaces
    .replace(/\s+/g, ' ')
    // Remove any leftover empty brackets
    .replace(/\[\s*\]/g, '')
    .trim();
}

// Stop words for keyword extraction
const STOP_WORDS = new Set([
  'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
  'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
  'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as',
  'and', 'or', 'but', 'if', 'this', 'that', 'these', 'those', 'it',
  'based', 'notes', 'according', 'mentioned', 'stated', 'using', 'used'
]);

/**
 * Extract keywords from text for overlap verification
 */
export function extractVerificationKeywords(text: string): Set<string> {
  return new Set(
    text.toLowerCase()
      .replace(/\[N\d+\]/g, '') // Remove citation markers
      .replace(/[^\w\s]/g, ' ')
      .split(/\s+/)
      .filter(word => word.length > 2 && !STOP_WORDS.has(word))
  );
}

/**
 * Calculate overlap score between two keyword sets
 * Uses Szymkiewicz–Simpson coefficient (min-based overlap)
 */
export function calculateOverlapScore(set1: Set<string>, set2: Set<string>): number {
  if (set1.size === 0 || set2.size === 0) return 0;

  let intersection = 0;
  for (const word of set1) {
    if (set2.has(word)) intersection++;
  }

  const minSize = Math.min(set1.size, set2.size);
  return intersection / minSize;
}

/**
 * Verify citation relevance using keyword overlap
 * Returns citations that have sufficient keyword overlap with the answer
 */
export function verifyCitationRelevance(
  answer: string,
  citations: Citation[],
  chunks: ScoredChunk[],
  options: { strictMode?: boolean; minOverlapScore?: number } = {}
): {
  validCitations: Citation[];
  droppedCitations: string[];
  suspiciousCitations: string[];
  overlapScores: Map<string, number>;
} {
  const { strictMode = true, minOverlapScore = DEFAULT_MIN_OVERLAP_SCORE } = options;

  const answerKeywords = extractVerificationKeywords(answer);
  const validCitations: Citation[] = [];
  const droppedCitations: string[] = [];
  const suspiciousCitations: string[] = [];
  const overlapScores = new Map<string, number>();

  for (const citation of citations) {
    // Find the full chunk text for this citation
    const chunk = chunks.find(c => c.chunkId === citation.chunkId);
    const sourceText = chunk?.text || citation.snippet;
    const sourceKeywords = extractVerificationKeywords(sourceText);

    // Calculate overlap score (0 to 1)
    const overlapScore = calculateOverlapScore(answerKeywords, sourceKeywords);
    overlapScores.set(citation.cid, overlapScore);

    if (overlapScore >= minOverlapScore) {
      validCitations.push(citation);
    } else if (overlapScore === 0 && strictMode) {
      droppedCitations.push(citation.cid);
    } else if (overlapScore < minOverlapScore && strictMode) {
      droppedCitations.push(citation.cid);
    } else {
      validCitations.push(citation);
      if (overlapScore < minOverlapScore * 0.5) {
        suspiciousCitations.push(citation.cid);
      }
    }
  }

  return { validCitations, droppedCitations, suspiciousCitations, overlapScores };
}

/**
 * Full citation validation pipeline
 *
 * Performs all citation validation steps:
 * 1. Remove invalid citations (not in source list)
 * 2. Clean formatting (duplicate citations, spacing)
 * 3. Reorder by first appearance
 * 4. Optionally verify overlap relevance
 * 5. Calculate coverage metrics
 *
 * @param answer - The LLM answer text
 * @param citations - Available citations from sources
 * @param chunks - Full chunk data for overlap verification
 * @param options - Validation options
 */
export function validateCitationsWithChunks(
  answer: string,
  citations: Citation[],
  chunks: ScoredChunk[],
  options: ValidationOptions = {}
): ValidationResult {
  const {
    strictMode = true,
    minOverlapScore = DEFAULT_MIN_OVERLAP_SCORE,
    verifyRelevance = true,
    requestId = 'unknown'
  } = options;

  // Build map of valid cids
  const validCids = new Set(citations.map(c => c.cid));

  // Step 1: Remove invalid citations
  const { cleaned: cleanedInvalid, removed } = removeInvalidCitations(answer, validCids);

  if (removed.length > 0) {
    logWarn('Removed invalid citations from answer', {
      requestId,
      removedCount: removed.length,
      removedCids: removed,
    });
  }

  // Step 2: Clean formatting
  const cleanedAnswer = cleanCitationFormatting(cleanedInvalid);

  // Step 3: Get citations actually used in the answer (in order)
  const usedCids = getOrderedUniqueCitations(cleanedAnswer);
  const usedCidSet = new Set(usedCids);

  // Filter citations to only those actually cited
  let usedCitations = citations.filter(c => usedCidSet.has(c.cid));

  // Reorder by first appearance
  usedCitations = usedCids
    .map(cid => usedCitations.find(c => c.cid === cid))
    .filter((c): c is Citation => c !== undefined);

  // Step 4: Verify overlap relevance
  let droppedCitations: string[] = [];
  let suspiciousCitations: string[] = [];
  let overlapScores = new Map<string, number>();

  if (verifyRelevance && usedCitations.length > 0) {
    const verifyResult = verifyCitationRelevance(cleanedAnswer, usedCitations, chunks, {
      strictMode,
      minOverlapScore,
    });

    usedCitations = verifyResult.validCitations;
    droppedCitations = verifyResult.droppedCitations;
    suspiciousCitations = verifyResult.suspiciousCitations;
    overlapScores = verifyResult.overlapScores;

    if (droppedCitations.length > 0) {
      logWarn('Dropped unsupported citations (low keyword overlap)', {
        requestId,
        droppedCitations,
        threshold: minOverlapScore,
      });
    }
  }

  // Step 5: Calculate coverage
  const coveragePct = calculateCitationCoverage(cleanedAnswer);

  return {
    validatedAnswer: cleanedAnswer,
    validatedCitations: usedCitations,
    invalidCitationsRemoved: removed,
    droppedCitations,
    suspiciousCitations,
    citationCoveragePct: coveragePct,
    allCitationsValid: removed.length === 0 && droppedCitations.length === 0,
    orderedByFirstAppearance: true,
    overlapScores,
  };
}

/**
 * Simple citation validation (backwards compatible)
 * Use validateCitationsWithChunks for full pipeline with overlap verification
 */
export function validateCitations(
  answer: string,
  citations: Citation[],
  requestId: string
): ValidationResult {
  // Use simplified validation without chunk data (no overlap verification)
  const validCids = new Set(citations.map(c => c.cid));
  const { cleaned, removed } = removeInvalidCitations(answer, validCids);

  if (removed.length > 0) {
    logWarn('Removed invalid citations from answer', {
      requestId,
      removedCount: removed.length,
      removedCids: removed,
    });
  }

  const cleanedAnswer = cleanCitationFormatting(cleaned);
  const usedCids = getOrderedUniqueCitations(cleanedAnswer);
  const usedCidSet = new Set(usedCids);
  const usedCitations = citations.filter(c => usedCidSet.has(c.cid));

  const orderedCitations = usedCids
    .map(cid => usedCitations.find(c => c.cid === cid))
    .filter((c): c is Citation => c !== undefined);

  const coveragePct = calculateCitationCoverage(cleanedAnswer);

  return {
    validatedAnswer: cleanedAnswer,
    validatedCitations: orderedCitations,
    invalidCitationsRemoved: removed,
    droppedCitations: [],
    suspiciousCitations: [],
    citationCoveragePct: coveragePct,
    allCitationsValid: removed.length === 0,
    orderedByFirstAppearance: true,
    overlapScores: new Map(),
  };
}

/**
 * Check if answer needs regeneration due to low citation coverage
 */
export function needsRegeneration(
  coveragePct: number,
  threshold: number = 50
): boolean {
  return coveragePct < threshold;
}

/**
 * Calculate source utilization: % of available sources that were cited
 */
export function calculateSourceUtilization(
  usedCitationCount: number,
  totalSourceCount: number
): number {
  if (totalSourceCount === 0) return 100;
  return Math.round((usedCitationCount / totalSourceCount) * 100);
}



================================================================================
FILE: src/config.ts
LINES: 184
PATH: /Users/salscrudato/Projects/auroranotes-api/src/config.ts
================================================================================

/**
 * AuroraNotes API - Configuration
 * 
 * Centralized configuration loaded from environment variables
 * with sensible defaults and validation.
 */

function envInt(key: string, defaultValue: number): number {
  const val = process.env[key];
  if (!val) return defaultValue;
  const parsed = parseInt(val, 10);
  return isNaN(parsed) ? defaultValue : parsed;
}

function envBool(key: string, defaultValue: boolean): boolean {
  const val = process.env[key]?.toLowerCase();
  if (!val) return defaultValue;
  return val === 'true' || val === '1';
}

// ============================================
// Server Config
// ============================================
export const PORT = envInt('PORT', 8080);
export const PROJECT_ID = process.env.GOOGLE_CLOUD_PROJECT || process.env.GCLOUD_PROJECT || 'local';

// ============================================
// Firestore Collections
// ============================================
export const NOTES_COLLECTION = process.env.NOTES_COLLECTION || 'notes';
export const CHUNKS_COLLECTION = process.env.CHUNKS_COLLECTION || 'noteChunks';

// ============================================
// Notes Config
// ============================================
export const MAX_NOTE_LENGTH = envInt('MAX_NOTE_LENGTH', 5000);
export const DEFAULT_TENANT_ID = process.env.DEFAULT_TENANT_ID || 'public';
export const NOTES_PAGE_LIMIT = envInt('NOTES_PAGE_LIMIT', 50);
export const MAX_NOTES_PAGE_LIMIT = 100;

// ============================================
// Chunking Config (tuned for citation accuracy)
// ============================================
export const CHUNK_TARGET_SIZE = envInt('CHUNK_TARGET_SIZE', 450);      // Slightly smaller for precision (was 500)
export const CHUNK_MIN_SIZE = envInt('CHUNK_MIN_SIZE', 80);             // Allow smaller chunks (was 100)
export const CHUNK_MAX_SIZE = envInt('CHUNK_MAX_SIZE', 700);            // Smaller max for focused content (was 800)
export const CHUNK_OVERLAP = envInt('CHUNK_OVERLAP', 75);               // More overlap for context (was 50)

// ============================================
// Embeddings Config
// ============================================
export const EMBEDDING_MODEL = process.env.EMBEDDING_MODEL || 'text-embedding-004';
export const EMBEDDING_DIMENSIONS = envInt('EMBEDDING_DIMENSIONS', 768);
export const EMBEDDINGS_ENABLED = envBool('EMBEDDINGS_ENABLED', true);
export const EMBEDDING_TIMEOUT_MS = envInt('EMBEDDING_TIMEOUT_MS', 15000); // 15 seconds per embedding call

// ============================================
// Retrieval Config
// ============================================
export const RETRIEVAL_TOP_K = envInt('RETRIEVAL_TOP_K', 30);           // Initial candidates
export const RETRIEVAL_RERANK_TO = envInt('RETRIEVAL_RERANK_TO', 8);    // After reranking
export const RETRIEVAL_DEFAULT_DAYS = envInt('RETRIEVAL_DEFAULT_DAYS', 90);
export const RETRIEVAL_MAX_CONTEXT_CHARS = envInt('RETRIEVAL_MAX_CONTEXT_CHARS', 12000);
export const RETRIEVAL_MAX_CONTEXT_TOKENS = envInt('RETRIEVAL_MAX_CONTEXT_TOKENS', 3000);

// ============================================
// Chat / LLM Config
// ============================================
export const CHAT_MODEL = process.env.CHAT_MODEL || 'gemini-2.0-flash';
export const CHAT_TIMEOUT_MS = envInt('CHAT_TIMEOUT_MS', 30000);
export const CHAT_MAX_QUERY_LENGTH = envInt('CHAT_MAX_QUERY_LENGTH', 2000);
export const CHAT_TEMPERATURE = parseFloat(process.env.CHAT_TEMPERATURE || '0.3');

// ============================================
// Cost Controls
// ============================================
export const MAX_CHUNKS_IN_CONTEXT = envInt('MAX_CHUNKS_IN_CONTEXT', 12);
export const MAX_EMBEDDING_BATCH_SIZE = envInt('MAX_EMBEDDING_BATCH_SIZE', 10);
export const CLOUD_RUN_MAX_INSTANCES = envInt('CLOUD_RUN_MAX_INSTANCES', 10);
export const CLOUD_RUN_CONCURRENCY = envInt('CLOUD_RUN_CONCURRENCY', 80);

// ============================================
// Feature Flags
// ============================================
export const VECTOR_SEARCH_ENABLED = envBool('VECTOR_SEARCH_ENABLED', true);
export const RERANKING_ENABLED = envBool('RERANKING_ENABLED', true);
export const LLM_RERANK_ENABLED = envBool('LLM_RERANK_ENABLED', false);  // Optional LLM-based reranking
export const CITATION_RETRY_ENABLED = envBool('CITATION_RETRY_ENABLED', true);  // Retry on invalid citations
export const ASYNC_EMBEDDINGS = envBool('ASYNC_EMBEDDINGS', false); // Future: queue for large notes

// ============================================
// Vertex AI Vector Search (for 100k+ scale)
// ============================================
export const VERTEX_VECTOR_SEARCH_ENABLED = envBool('VERTEX_VECTOR_SEARCH_ENABLED', false);
export const VERTEX_VECTOR_SEARCH_REGION = process.env.VERTEX_VECTOR_SEARCH_REGION || 'us-central1';
// Endpoint config: prefer VERTEX_INDEX_ENDPOINT_RESOURCE (full resource name)
// Fallback to VERTEX_INDEX_ENDPOINT_ID + project/region construction
export const VERTEX_INDEX_ENDPOINT_RESOURCE = process.env.VERTEX_INDEX_ENDPOINT_RESOURCE || '';
export const VERTEX_INDEX_ENDPOINT_ID = process.env.VERTEX_INDEX_ENDPOINT_ID || '';
// Legacy: VERTEX_VECTOR_SEARCH_ENDPOINT (public endpoint domain) - deprecated
export const VERTEX_VECTOR_SEARCH_ENDPOINT = process.env.VERTEX_VECTOR_SEARCH_ENDPOINT || '';
// Index ID for upsert/remove operations
export const VERTEX_VECTOR_SEARCH_INDEX_ID = process.env.VERTEX_VECTOR_SEARCH_INDEX_ID || '';
// Deployed index ID within the endpoint
export const VERTEX_DEPLOYED_INDEX_ID = process.env.VERTEX_DEPLOYED_INDEX_ID || '';
// Distance metric for score conversion
export const VERTEX_DISTANCE_METRIC = (process.env.VERTEX_DISTANCE_METRIC as 'COSINE' | 'DOT_PRODUCT' | 'SQUARED_L2') || 'COSINE';

// ============================================
// Multi-Stage Retrieval Config
// ============================================
// Vector candidate generation
export const RETRIEVAL_VECTOR_TOP_K = envInt('RETRIEVAL_VECTOR_TOP_K', 300);  // Primary vector candidates
// Lexical candidate generation
export const RETRIEVAL_LEXICAL_TOP_K = envInt('RETRIEVAL_LEXICAL_TOP_K', 100);  // Lexical (exact match) candidates
export const RETRIEVAL_LEXICAL_MAX_TERMS = envInt('RETRIEVAL_LEXICAL_MAX_TERMS', 10);  // Max query terms for array-contains-any
// Recency candidates (soft support)
export const RETRIEVAL_RECENCY_TOP_K = envInt('RETRIEVAL_RECENCY_TOP_K', 50);  // Recent chunk candidates
// Merging and reranking
export const RETRIEVAL_MERGED_TOP_K = envInt('RETRIEVAL_MERGED_TOP_K', 150);  // After merging all stages
export const RETRIEVAL_MMR_ENABLED = envBool('RETRIEVAL_MMR_ENABLED', true);  // Maximal Marginal Relevance diversity
export const RETRIEVAL_MMR_LAMBDA = parseFloat(process.env.RETRIEVAL_MMR_LAMBDA || '0.7');  // Relevance vs diversity tradeoff
// Scale guards
export const FIRESTORE_FALLBACK_WARN_THRESHOLD = envInt('FIRESTORE_FALLBACK_WARN_THRESHOLD', 5000);  // Warn if using Firestore fallback above this chunk count
export const FIRESTORE_FALLBACK_MAX_SCAN = envInt('FIRESTORE_FALLBACK_MAX_SCAN', 2000);  // Max chunks for Firestore fallback scan

// ============================================
// Query Expansion (optional)
// ============================================
export const QUERY_EXPANSION_ENABLED = envBool('QUERY_EXPANSION_ENABLED', false);  // Multi-query expansion
export const QUERY_EXPANSION_REWRITES = envInt('QUERY_EXPANSION_REWRITES', 2);  // Number of query rewrites
export const QUERY_EXPANSION_TTL_MS = envInt('QUERY_EXPANSION_TTL_MS', 300000);  // Cache TTL (5 minutes)

// ============================================
// Citation Verification
// ============================================
export const CITATION_VERIFICATION_ENABLED = envBool('CITATION_VERIFICATION_ENABLED', true);  // Post-generation verification
export const CITATION_MIN_OVERLAP_SCORE = parseFloat(process.env.CITATION_MIN_OVERLAP_SCORE || '0.15');  // Min lexical overlap for validity
export const CITATION_ENTAILMENT_ENABLED = envBool('CITATION_ENTAILMENT_ENABLED', false);  // Use lightweight entailment check (optional)

// ============================================
// Retrieval Scoring Weights (tunable)
// ============================================
export const SCORE_WEIGHT_VECTOR = parseFloat(process.env.SCORE_WEIGHT_VECTOR || '0.45');
export const SCORE_WEIGHT_LEXICAL = parseFloat(process.env.SCORE_WEIGHT_LEXICAL || '0.35');
export const SCORE_WEIGHT_RECENCY = parseFloat(process.env.SCORE_WEIGHT_RECENCY || '0.12');
export const SCORE_WEIGHT_ID_BOOST = parseFloat(process.env.SCORE_WEIGHT_ID_BOOST || '0.08');

// ============================================
// Rate Limiting
// ============================================
export const RATE_LIMIT_ENABLED = envBool('RATE_LIMIT_ENABLED', false);
export const RATE_LIMIT_REQUESTS_PER_MIN = envInt('RATE_LIMIT_REQUESTS_PER_MIN', 60);
export const RATE_LIMIT_WINDOW_MS = envInt('RATE_LIMIT_WINDOW_MS', 60000);

// ============================================
// Background Queue / Cloud Tasks
// ============================================
export const QUEUE_MODE = process.env.QUEUE_MODE || 'in-process'; // 'in-process' | 'cloud-tasks'
export const BACKGROUND_QUEUE_MAX_SIZE = envInt('BACKGROUND_QUEUE_MAX_SIZE', 100);
export const BACKGROUND_QUEUE_MAX_CONCURRENT = envInt('BACKGROUND_QUEUE_MAX_CONCURRENT', 3);
export const CLOUD_TASKS_QUEUE_NAME = process.env.CLOUD_TASKS_QUEUE_NAME || 'note-processing';
export const CLOUD_TASKS_LOCATION = process.env.CLOUD_TASKS_LOCATION || 'us-central1';
export const CLOUD_TASKS_SERVICE_URL = process.env.CLOUD_TASKS_SERVICE_URL || '';

// ============================================
// Internal Endpoint Auth (OIDC)
// ============================================
// When enabled, /internal/* endpoints require valid OIDC JWT from Cloud Tasks
export const INTERNAL_AUTH_ENABLED = envBool('INTERNAL_AUTH_ENABLED', false);
// Expected audience for OIDC tokens (typically the service URL)
export const INTERNAL_AUTH_AUDIENCE = process.env.INTERNAL_AUTH_AUDIENCE || CLOUD_TASKS_SERVICE_URL;
// Expected issuer (Google OIDC)
export const INTERNAL_AUTH_ISSUER = process.env.INTERNAL_AUTH_ISSUER || 'https://accounts.google.com';
// Expected service account email (optional, for stricter validation)
export const INTERNAL_AUTH_SERVICE_ACCOUNT = process.env.INTERNAL_AUTH_SERVICE_ACCOUNT || '';

// ============================================
// Logging
// ============================================
export const LOG_LEVEL = process.env.LOG_LEVEL || 'info';
export const LOG_FULL_TEXT = envBool('LOG_FULL_TEXT', false); // Never log full note text in prod



================================================================================
FILE: src/embeddings.ts
LINES: 292
PATH: /Users/salscrudato/Projects/auroranotes-api/src/embeddings.ts
================================================================================

/**
 * AuroraNotes API - Embeddings Generation
 *
 * Uses Google's text-embedding models via the Generative AI SDK.
 * Includes LRU caching, retry logic, and query normalization.
 */

import {
  EMBEDDING_MODEL,
  EMBEDDING_DIMENSIONS,
  MAX_EMBEDDING_BATCH_SIZE,
  EMBEDDING_TIMEOUT_MS,
} from "./config";
import { logInfo, logError, logWarn, hashText } from "./utils";
import { getGenAIClient, isGenAIAvailable } from "./genaiClient";

// LRU Cache for embeddings by textHash (reduces API costs for repeated/identical content)
const EMBEDDING_CACHE_MAX_SIZE = parseInt(process.env.EMBEDDING_CACHE_MAX_SIZE || '') || 1000;
const embeddingCache = new Map<string, { embedding: number[]; timestamp: number }>();

// Track cache statistics
let cacheHits = 0;
let cacheMisses = 0;

/**
 * Normalize text for consistent embedding generation
 */
function normalizeText(text: string): string {
  return text
    .toLowerCase()
    .replace(/\s+/g, ' ')
    .trim()
    .slice(0, 8000); // Limit input length
}

/**
 * Get cache key for text
 */
function getCacheKey(text: string): string {
  return hashText(normalizeText(text));
}

/**
 * Evict oldest entries if cache is full
 */
function evictOldestCacheEntries(): void {
  if (embeddingCache.size < EMBEDDING_CACHE_MAX_SIZE) return;

  const entries = Array.from(embeddingCache.entries());
  entries.sort((a, b) => a[1].timestamp - b[1].timestamp);

  const toRemove = entries.slice(0, Math.floor(EMBEDDING_CACHE_MAX_SIZE * 0.2));
  for (const [key] of toRemove) {
    embeddingCache.delete(key);
  }
}

/**
 * Retry with exponential backoff
 */
async function withRetry<T>(
  fn: () => Promise<T>,
  maxRetries: number = 3,
  baseDelayMs: number = 1000
): Promise<T> {
  let lastError: Error | unknown;

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (err) {
      lastError = err;

      // Don't retry on certain errors
      const errMessage = err instanceof Error ? err.message : String(err);
      if (errMessage.includes('INVALID_ARGUMENT') ||
          errMessage.includes('PERMISSION_DENIED')) {
        throw err;
      }

      if (attempt < maxRetries) {
        const delay = baseDelayMs * Math.pow(2, attempt) * (0.5 + Math.random() * 0.5);
        logWarn('Embedding API retry', { attempt: attempt + 1, delayMs: delay });
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
  }

  throw lastError;
}

/**
 * Create a timeout promise that rejects after specified milliseconds
 */
function createTimeout<T>(ms: number, context: string): Promise<T> {
  return new Promise((_, reject) => {
    setTimeout(() => {
      reject(new Error(`Timeout after ${ms}ms: ${context}`));
    }, ms);
  });
}

/**
 * Generate embedding for a single text with timeout and retry
 */
async function generateSingleEmbedding(text: string): Promise<number[]> {
  const client = getGenAIClient();

  const result = await withRetry(async () => {
    // Race between embedding call and timeout
    const embeddingPromise = client.models.embedContent({
      model: EMBEDDING_MODEL,
      contents: text,
      config: {
        outputDimensionality: EMBEDDING_DIMENSIONS,
      },
    });

    return await Promise.race([
      embeddingPromise,
      createTimeout<typeof embeddingPromise>(EMBEDDING_TIMEOUT_MS, 'embedding generation'),
    ]) as Awaited<typeof embeddingPromise>;
  });

  if (result.embeddings && result.embeddings.length > 0 && result.embeddings[0].values) {
    return result.embeddings[0].values;
  }
  throw new Error('No embedding values in response');
}

/**
 * Custom error for embedding generation failures
 */
export class EmbeddingError extends Error {
  constructor(message: string, public readonly missingIndices: number[] = []) {
    super(message);
    this.name = 'EmbeddingError';
  }
}

/**
 * Generate embeddings for a batch of texts with caching and retry logic
 * Uses textHash for deduplication - identical text returns cached embedding
 *
 * IMPORTANT: Returns an array with EXACTLY the same length as input texts.
 * Throws EmbeddingError if any embedding fails to generate - this prevents
 * misaligned embeddings-to-chunks assignment which would corrupt the index.
 */
export async function generateEmbeddings(texts: string[]): Promise<number[][]> {
  if (texts.length === 0) return [];

  const startTime = Date.now();
  const results: (number[] | null)[] = new Array(texts.length).fill(null);
  const toGenerate: { index: number; text: string; cacheKey: string }[] = [];

  // Check cache first for all texts
  for (let i = 0; i < texts.length; i++) {
    const cacheKey = getCacheKey(texts[i]);
    const cached = embeddingCache.get(cacheKey);

    if (cached) {
      cached.timestamp = Date.now(); // Update LRU timestamp
      results[i] = cached.embedding;
      cacheHits++;
    } else {
      toGenerate.push({ index: i, text: texts[i], cacheKey });
      cacheMisses++;
    }
  }

  // Generate embeddings for cache misses in batches
  for (let i = 0; i < toGenerate.length; i += MAX_EMBEDDING_BATCH_SIZE) {
    const batch = toGenerate.slice(i, i + MAX_EMBEDDING_BATCH_SIZE);

    try {
      const batchEmbeddings = await Promise.all(
        batch.map(item => generateSingleEmbedding(item.text))
      );

      // Store results and cache
      for (let j = 0; j < batch.length; j++) {
        const item = batch[j];
        const embedding = batchEmbeddings[j];
        results[item.index] = embedding;

        // Cache the result
        evictOldestCacheEntries();
        embeddingCache.set(item.cacheKey, { embedding, timestamp: Date.now() });
      }
    } catch (err) {
      logError('Embedding batch failed', err, {
        batchStart: i,
        batchSize: batch.length
      });
      throw err;
    }
  }

  // CRITICAL: Verify all embeddings were generated successfully
  // If any are missing, throw an error to prevent misaligned embeddings
  const missingIndices: number[] = [];
  for (let i = 0; i < results.length; i++) {
    if (results[i] === null) {
      missingIndices.push(i);
    }
  }

  if (missingIndices.length > 0) {
    logError('Embedding generation incomplete - missing indices would cause misalignment', null, {
      inputCount: texts.length,
      missingCount: missingIndices.length,
      missingIndices: missingIndices.slice(0, 10), // Log first 10
    });
    throw new EmbeddingError(
      `Failed to generate ${missingIndices.length} of ${texts.length} embeddings`,
      missingIndices
    );
  }

  const elapsedMs = Date.now() - startTime;
  logInfo('Embeddings generated', {
    count: texts.length,
    fromCache: texts.length - toGenerate.length,
    generated: toGenerate.length,
    model: EMBEDDING_MODEL,
    dimensions: EMBEDDING_DIMENSIONS,
    elapsedMs,
  });

  // Safe to cast since we verified all entries are non-null above
  return results as number[][];
}

/**
 * Generate embedding for a query with caching
 */
export async function generateQueryEmbedding(query: string): Promise<number[]> {
  const cacheKey = getCacheKey(query);

  // Check cache first
  const cached = embeddingCache.get(cacheKey);
  if (cached) {
    cached.timestamp = Date.now(); // Update LRU timestamp
    logInfo('Query embedding cache hit', { queryLength: query.length });
    return cached.embedding;
  }

  // Generate new embedding
  const embedding = await generateSingleEmbedding(normalizeText(query));

  // Cache the result
  evictOldestCacheEntries();
  embeddingCache.set(cacheKey, { embedding, timestamp: Date.now() });

  return embedding;
}

/**
 * Check if embeddings service is available
 */
export function isEmbeddingsAvailable(): boolean {
  return isGenAIAvailable();
}

/**
 * Clear embedding cache (for testing/maintenance)
 */
export function clearEmbeddingCache(): void {
  embeddingCache.clear();
}

/**
 * Get cache stats (for monitoring)
 */
export function getEmbeddingCacheStats(): {
  size: number;
  maxSize: number;
  hits: number;
  misses: number;
  hitRate: number;
} {
  const total = cacheHits + cacheMisses;
  return {
    size: embeddingCache.size,
    maxSize: EMBEDDING_CACHE_MAX_SIZE,
    hits: cacheHits,
    misses: cacheMisses,
    hitRate: total > 0 ? Math.round((cacheHits / total) * 100) / 100 : 0,
  };
}



================================================================================
FILE: src/firestore.ts
LINES: 21
PATH: /Users/salscrudato/Projects/auroranotes-api/src/firestore.ts
================================================================================

import admin from "firebase-admin";

let app: admin.app.App | null = null;

export function getDb() {
  if (!app) {
    // On Cloud Run, default credentials are provided via the service account.
    // Use GOOGLE_CLOUD_PROJECT env var to ensure we connect to the correct project.
    const projectId = process.env.GOOGLE_CLOUD_PROJECT || process.env.GCLOUD_PROJECT;

    if (projectId) {
      app = admin.initializeApp({
        projectId: projectId,
      });
    } else {
      // Fallback to default (will use metadata server project)
      app = admin.initializeApp();
    }
  }
  return admin.firestore();
}

================================================================================
FILE: src/genaiClient.ts
LINES: 162
PATH: /Users/salscrudato/Projects/auroranotes-api/src/genaiClient.ts
================================================================================

/**
 * AuroraNotes API - GenAI Client Factory
 * 
 * Centralized client creation for all GenAI operations (chat, embeddings, reranking).
 * Supports two authentication modes:
 * 
 *   GENAI_MODE=apikey (default):
 *     Uses GOOGLE_API_KEY or GEMINI_API_KEY environment variable
 *     Suitable for development and simple deployments
 * 
 *   GENAI_MODE=vertex:
 *     Uses Application Default Credentials (ADC) via service account
 *     Required for production Cloud Run deployments
 *     Requires GOOGLE_CLOUD_PROJECT to be set
 */

import { GoogleGenAI } from "@google/genai";
import { logInfo, logError } from "./utils";

// Singleton instances for each mode
let apiKeyClient: GoogleGenAI | null = null;
let vertexClient: GoogleGenAI | null = null;

// Configuration
const GENAI_MODE = process.env.GENAI_MODE || 'apikey';
const PROJECT_ID = process.env.GOOGLE_CLOUD_PROJECT || process.env.GCLOUD_PROJECT;

/**
 * Supported GenAI modes
 */
export type GenAIMode = 'apikey' | 'vertex';

/**
 * Get the current GenAI mode
 */
export function getGenAIMode(): GenAIMode {
  if (GENAI_MODE === 'vertex') {
    return 'vertex';
  }
  return 'apikey';
}

/**
 * Check if GenAI client is available
 */
export function isGenAIAvailable(): boolean {
  const mode = getGenAIMode();
  
  if (mode === 'apikey') {
    const apiKey = process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY;
    return !!apiKey;
  }
  
  if (mode === 'vertex') {
    // Vertex requires project ID and ADC (which is automatically available on Cloud Run)
    return !!PROJECT_ID;
  }
  
  return false;
}

/**
 * Get the GenAI client with API key authentication
 */
function getApiKeyClient(): GoogleGenAI {
  if (!apiKeyClient) {
    const apiKey = process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY;
    if (!apiKey) {
      throw new Error(
        'GENAI_MODE=apikey requires GOOGLE_API_KEY or GEMINI_API_KEY environment variable'
      );
    }
    
    apiKeyClient = new GoogleGenAI({ apiKey });
    logInfo('GenAI client initialized', { mode: 'apikey' });
  }
  return apiKeyClient;
}

/**
 * Get the GenAI client with Vertex AI / ADC authentication
 * 
 * This uses Application Default Credentials which:
 * - On Cloud Run: automatically uses the service account
 * - Locally: uses gcloud auth application-default credentials
 */
function getVertexClient(): GoogleGenAI {
  if (!vertexClient) {
    if (!PROJECT_ID) {
      throw new Error(
        'GENAI_MODE=vertex requires GOOGLE_CLOUD_PROJECT environment variable'
      );
    }

    // The GoogleGenAI SDK supports Vertex AI through ADC when no apiKey is provided
    // and GOOGLE_APPLICATION_CREDENTIALS or Cloud Run service account is available
    try {
      // For Vertex AI, we need to use the Vertex AI endpoint
      // This is a simplified approach - full Vertex AI support would use @google-cloud/aiplatform
      vertexClient = new GoogleGenAI({
        vertexai: true,
        project: PROJECT_ID,
        location: process.env.VERTEX_AI_LOCATION || 'us-central1',
      } as any); // Type assertion needed as SDK types may not fully expose Vertex options
      
      logInfo('GenAI client initialized', { 
        mode: 'vertex',
        project: PROJECT_ID,
        location: process.env.VERTEX_AI_LOCATION || 'us-central1',
      });
    } catch (err) {
      logError('Failed to initialize Vertex AI client', err);
      throw new Error(
        `Failed to initialize Vertex AI: ${err instanceof Error ? err.message : String(err)}. ` +
        'Ensure GOOGLE_APPLICATION_CREDENTIALS is set or running on Cloud Run with appropriate IAM.'
      );
    }
  }
  return vertexClient;
}

/**
 * Get the GenAI client based on current mode
 */
export function getGenAIClient(): GoogleGenAI {
  const mode = getGenAIMode();
  
  switch (mode) {
    case 'vertex':
      return getVertexClient();
    case 'apikey':
    default:
      return getApiKeyClient();
  }
}

/**
 * Reset clients (for testing)
 */
export function resetGenAIClients(): void {
  apiKeyClient = null;
  vertexClient = null;
}

/**
 * Get configuration info for logging/debugging
 */
export function getGenAIConfig(): {
  mode: GenAIMode;
  available: boolean;
  project?: string;
  location?: string;
} {
  return {
    mode: getGenAIMode(),
    available: isGenAIAvailable(),
    project: PROJECT_ID,
    location: process.env.VERTEX_AI_LOCATION || 'us-central1',
  };
}



================================================================================
FILE: src/index.ts
LINES: 233
PATH: /Users/salscrudato/Projects/auroranotes-api/src/index.ts
================================================================================

/**
 * AuroraNotes API - Main Entry Point
 *
 * Express server with notes CRUD, pagination, and RAG-powered chat.
 */

import express from "express";
import cors from "cors";

import { PORT, PROJECT_ID, DEFAULT_TENANT_ID, NOTES_COLLECTION } from "./config";
import { createNote, listNotes } from "./notes";
import { generateChatResponse, ConfigurationError, RateLimitError } from "./chat";
import { logInfo, logError, logWarn, generateRequestId, withRequestContext } from "./utils";
import { ChatRequest, NoteDoc } from "./types";
import { rateLimitMiddleware } from "./rateLimit";
import { processNoteChunks } from "./chunking";
import { getDb } from "./firestore";
import { internalAuthMiddleware, isInternalAuthConfigured } from "./internalAuth";
import { getVertexConfigStatus, isVertexConfigured } from "./vectorIndex";

// Create Express application
const app = express();

// Middleware
app.use(cors());
app.use(express.json({ limit: "1mb" }));

// Request context middleware (for request ID correlation)
app.use((req, res, next) => {
  const requestId = req.headers['x-request-id'] as string || generateRequestId();
  res.set('X-Request-Id', requestId);

  withRequestContext(
    { requestId, startTime: Date.now(), path: req.path },
    () => next()
  );
});

app.use(rateLimitMiddleware);

// ============================================
// Health Endpoint
// ============================================
app.get("/health", (_req, res) => {
  res.status(200).json({
    status: "healthy",
    timestamp: new Date().toISOString(),
    service: "auroranotes-api",
    project: PROJECT_ID,
    version: "2.0.0",
  });
});

// ============================================
// Notes Endpoints
// ============================================

/**
 * POST /notes - Create a new note
 *
 * Request: { text: string, tenantId?: string }
 * Response: NoteResponse
 */
app.post("/notes", async (req, res) => {
  try {
    const text = (req.body?.text || "").toString();
    const tenantId = req.body?.tenantId || DEFAULT_TENANT_ID;

    const note = await createNote(text, tenantId);
    return res.status(201).json(note);
  } catch (err: unknown) {
    const message = err instanceof Error ? err.message : "internal error";

    if (message.includes("required") || message.includes("too long")) {
      return res.status(400).json({ error: message });
    }

    logError("POST /notes error", err);
    return res.status(500).json({ error: "internal error" });
  }
});

/**
 * GET /notes - List notes with pagination
 *
 * Query params:
 *   - limit: number (default 50, max 100)
 *   - cursor: string (pagination cursor)
 *   - tenantId: string (default 'public')
 *
 * Response: NotesListResponse
 */
app.get("/notes", async (req, res) => {
  try {
    const limit = parseInt(req.query.limit as string) || 50;
    const cursor = req.query.cursor as string | undefined;
    const tenantId = (req.query.tenantId as string) || DEFAULT_TENANT_ID;

    const result = await listNotes(tenantId, limit, cursor);
    return res.status(200).json(result);
  } catch (err: unknown) {
    logError("GET /notes error", err);
    return res.status(500).json({ error: "internal error" });
  }
});

// ============================================
// Chat Endpoint
// ============================================

/**
 * POST /chat - RAG-powered chat with inline citations
 *
 * Request: { message: string, tenantId?: string }
 * Response: ChatResponse with answer, citations[], and meta
 */
app.post("/chat", async (req, res) => {
  try {
    const request: ChatRequest = {
      message: (req.body?.message || "").toString(),
      tenantId: req.body?.tenantId || DEFAULT_TENANT_ID,
    };

    const response = await generateChatResponse(request);
    return res.status(200).json(response);
  } catch (err: unknown) {
    // Handle server configuration errors (503)
    if (err instanceof ConfigurationError) {
      logError("POST /chat configuration error", err);
      return res.status(503).json({
        error: "Chat service is not configured. Please contact support.",
        code: "SERVICE_UNAVAILABLE",
      });
    }

    // Handle rate limiting (429)
    if (err instanceof RateLimitError) {
      logError("POST /chat rate limit", err);
      return res.status(429).json({
        error: "Too many requests. Please try again later.",
        code: "RATE_LIMITED",
        retryAfterMs: 5000,
      });
    }

    const message = err instanceof Error ? err.message : "internal error";

    // Handle client validation errors (400)
    if (message === "message is required" || message.includes("too long")) {
      return res.status(400).json({ error: message });
    }

    logError("POST /chat error", err);
    return res.status(500).json({ error: "internal error" });
  }
});

// ============================================
// Internal Endpoints (Cloud Tasks callbacks)
// ============================================

// Validate internal auth configuration at startup
if (!isInternalAuthConfigured()) {
  logError('Internal auth misconfigured - see logs above', null);
}

/**
 * POST /internal/process-note - Cloud Tasks callback for processing notes
 *
 * This endpoint is called by Cloud Tasks to process note chunks/embeddings.
 * When INTERNAL_AUTH_ENABLED=true, validates OIDC token from Cloud Tasks.
 */
app.post("/internal/process-note", internalAuthMiddleware, async (req, res) => {
  try {
    const { noteId, tenantId } = req.body;

    if (!noteId) {
      return res.status(400).json({ error: "noteId is required" });
    }

    // Fetch the note from Firestore
    const db = getDb();
    const noteDoc = await db.collection(NOTES_COLLECTION).doc(noteId).get();

    if (!noteDoc.exists) {
      logWarn("Note not found for processing", { noteId });
      // Return 200 to prevent Cloud Tasks from retrying
      return res.status(200).json({ status: "not_found", noteId });
    }

    const note = noteDoc.data() as NoteDoc;

    // Verify tenant if provided
    if (tenantId && note.tenantId !== tenantId) {
      logWarn("Tenant mismatch for note processing", { noteId, expected: tenantId, actual: note.tenantId });
      return res.status(200).json({ status: "tenant_mismatch", noteId });
    }

    // Process the note chunks
    await processNoteChunks(note);

    logInfo("Note processed via Cloud Tasks", { noteId });
    return res.status(200).json({ status: "processed", noteId });
  } catch (err) {
    logError("POST /internal/process-note error", err);
    // Return 500 to trigger Cloud Tasks retry
    return res.status(500).json({ error: "processing failed" });
  }
});

// ============================================
// Start Server
// ============================================
app.listen(PORT, () => {
  logInfo("auroranotes-api started", { port: PORT, project: PROJECT_ID });
  console.log(`auroranotes-api listening on http://localhost:${PORT}`);

  // Log vector search configuration status at startup
  const vertexStatus = getVertexConfigStatus();
  if (vertexStatus.enabled && vertexStatus.configured) {
    logInfo("Vector search: Vertex AI enabled and configured", {});
  } else if (vertexStatus.enabled && !vertexStatus.configured) {
    logWarn("Vector search: Vertex AI enabled but MISCONFIGURED - using Firestore fallback", {
      errors: vertexStatus.errors,
      hint: "Set VERTEX_INDEX_ENDPOINT_RESOURCE and VERTEX_DEPLOYED_INDEX_ID",
    });
  } else {
    logInfo("Vector search: Using Firestore fallback (Vertex not enabled)", {
      hint: "Set VERTEX_VECTOR_SEARCH_ENABLED=true for better recall",
    });
  }
});


================================================================================
FILE: src/internalAuth.ts
LINES: 172
PATH: /Users/salscrudato/Projects/auroranotes-api/src/internalAuth.ts
================================================================================

/**
 * AuroraNotes API - Internal Endpoint Authentication
 *
 * Provides OIDC JWT validation for /internal/* endpoints.
 * When INTERNAL_AUTH_ENABLED=true, validates that requests come from
 * authorized Cloud Tasks with valid Google OIDC tokens.
 *
 * Configuration:
 *   INTERNAL_AUTH_ENABLED=true       - Enable OIDC validation
 *   INTERNAL_AUTH_AUDIENCE=<url>     - Expected audience (service URL)
 *   INTERNAL_AUTH_SERVICE_ACCOUNT=<email> - Optional: expected SA email
 */

import { Request, Response, NextFunction } from 'express';
import { OAuth2Client } from 'google-auth-library';
import {
  INTERNAL_AUTH_ENABLED,
  INTERNAL_AUTH_AUDIENCE,
  INTERNAL_AUTH_ISSUER,
  INTERNAL_AUTH_SERVICE_ACCOUNT,
} from './config';
import { logInfo, logWarn, logError } from './utils';

// Singleton OAuth2 client for token verification
let oauthClient: OAuth2Client | null = null;

function getOAuthClient(): OAuth2Client {
  if (!oauthClient) {
    oauthClient = new OAuth2Client();
  }
  return oauthClient;
}

/**
 * Extract bearer token from Authorization header
 */
function extractBearerToken(req: Request): string | null {
  const authHeader = req.headers.authorization;
  if (!authHeader) return null;

  const parts = authHeader.split(' ');
  if (parts.length !== 2 || parts[0].toLowerCase() !== 'bearer') {
    return null;
  }

  return parts[1];
}

/**
 * Verify OIDC token from Google
 */
async function verifyOidcToken(token: string): Promise<{
  valid: boolean;
  email?: string;
  audience?: string;
  error?: string;
}> {
  try {
    const client = getOAuthClient();
    const ticket = await client.verifyIdToken({
      idToken: token,
      audience: INTERNAL_AUTH_AUDIENCE || undefined,
    });

    const payload = ticket.getPayload();
    if (!payload) {
      return { valid: false, error: 'No payload in token' };
    }

    // Verify issuer
    if (payload.iss !== INTERNAL_AUTH_ISSUER && payload.iss !== 'accounts.google.com') {
      return { valid: false, error: `Invalid issuer: ${payload.iss}` };
    }

    // Verify audience if configured
    if (INTERNAL_AUTH_AUDIENCE && payload.aud !== INTERNAL_AUTH_AUDIENCE) {
      return { valid: false, error: `Invalid audience: ${payload.aud}` };
    }

    // Verify service account if configured
    if (INTERNAL_AUTH_SERVICE_ACCOUNT && payload.email !== INTERNAL_AUTH_SERVICE_ACCOUNT) {
      return { valid: false, error: `Invalid service account: ${payload.email}` };
    }

    return {
      valid: true,
      email: payload.email,
      audience: payload.aud as string,
    };
  } catch (err) {
    const message = err instanceof Error ? err.message : String(err);
    return { valid: false, error: message };
  }
}

/**
 * Express middleware for internal endpoint authentication
 *
 * When INTERNAL_AUTH_ENABLED=true:
 * - Requires valid OIDC bearer token
 * - Validates issuer, audience, and optionally service account
 * - Returns 401 for missing/invalid tokens
 *
 * When INTERNAL_AUTH_ENABLED=false:
 * - Passes through all requests (development mode)
 */
export async function internalAuthMiddleware(
  req: Request,
  res: Response,
  next: NextFunction
): Promise<void> {
  // Skip auth if not enabled (development mode)
  if (!INTERNAL_AUTH_ENABLED) {
    logInfo('Internal auth disabled, allowing request', {
      path: req.path,
      method: req.method,
    });
    return next();
  }

  // Extract bearer token
  const token = extractBearerToken(req);
  if (!token) {
    logWarn('Internal auth: missing bearer token', {
      path: req.path,
      method: req.method,
    });
    res.status(401).json({ error: 'Missing authorization token' });
    return;
  }

  // Verify OIDC token
  const result = await verifyOidcToken(token);

  if (!result.valid) {
    logWarn('Internal auth: invalid token', {
      path: req.path,
      method: req.method,
      error: result.error,
    });
    res.status(401).json({ error: 'Invalid authorization token' });
    return;
  }

  // Token is valid
  logInfo('Internal auth: token verified', {
    path: req.path,
    method: req.method,
    email: result.email,
  });

  return next();
}

/**
 * Check if internal auth is properly configured
 */
export function isInternalAuthConfigured(): boolean {
  if (!INTERNAL_AUTH_ENABLED) {
    return true; // Not enabled = no config needed
  }

  // Must have audience configured
  if (!INTERNAL_AUTH_AUDIENCE) {
    logError('INTERNAL_AUTH_ENABLED=true but INTERNAL_AUTH_AUDIENCE not set', null);
    return false;
  }

  return true;
}



================================================================================
FILE: src/notes.ts
LINES: 254
PATH: /Users/salscrudato/Projects/auroranotes-api/src/notes.ts
================================================================================

/**
 * AuroraNotes API - Notes Service
 * 
 * Handles note CRUD operations with pagination and tenant support.
 */

import { FieldValue, Timestamp } from "firebase-admin/firestore";
import { v4 as uuidv4 } from "uuid";
import { getDb } from "./firestore";
import { 
  NOTES_COLLECTION, 
  MAX_NOTE_LENGTH, 
  DEFAULT_TENANT_ID,
  NOTES_PAGE_LIMIT,
  MAX_NOTES_PAGE_LIMIT
} from "./config";
import { NoteDoc, NoteResponse, NotesListResponse } from "./types";
import { timestampToISO, parseCursor, encodeCursor, logInfo, logError, logWarn, sanitizeText, isValidTenantId } from "./utils";
import { enqueueNoteProcessing } from "./queue";

/**
 * Convert Firestore document to API response
 */
function docToResponse(doc: NoteDoc): NoteResponse {
  return {
    id: doc.id,
    text: doc.text,
    tenantId: doc.tenantId,
    createdAt: timestampToISO(doc.createdAt),
    updatedAt: timestampToISO(doc.updatedAt),
  };
}

/**
 * Create a new note with input validation and sanitization
 */
export async function createNote(
  text: string,
  tenantId: string = DEFAULT_TENANT_ID
): Promise<NoteResponse> {
  // Sanitize and validate input
  const sanitizedText = sanitizeText(text, MAX_NOTE_LENGTH + 100);
  const trimmedText = sanitizedText.trim();

  if (!trimmedText) {
    throw new Error('text is required');
  }

  if (trimmedText.length > MAX_NOTE_LENGTH) {
    throw new Error(`text too long (max ${MAX_NOTE_LENGTH})`);
  }

  // Validate tenant ID
  if (!isValidTenantId(tenantId)) {
    throw new Error('invalid tenantId format');
  }

  const id = uuidv4();
  const now = FieldValue.serverTimestamp();
  
  const doc: NoteDoc = {
    id,
    text: trimmedText,
    tenantId,
    createdAt: now,
    updatedAt: now,
  };

  const db = getDb();
  await db.collection(NOTES_COLLECTION).doc(id).set(doc);
  
  // Fetch the document to get actual server timestamp
  const savedDoc = await db.collection(NOTES_COLLECTION).doc(id).get();
  const savedData = savedDoc.data() as NoteDoc;
  
  // Enqueue for background chunk/embedding processing with backpressure
  // Note: enqueueNoteProcessing is async but we don't await it to avoid blocking the response
  enqueueNoteProcessing(savedData).then(enqueued => {
    if (!enqueued) {
      logError('Failed to enqueue note for processing - queue full', null, { noteId: id });
    }
  }).catch(err => {
    logError('Failed to enqueue note for processing', err, { noteId: id });
  });
  
  logInfo('Note created', { 
    noteId: id, 
    tenantId, 
    textLength: trimmedText.length 
  });
  
  return docToResponse(savedData);
}

/**
 * List notes with cursor-based pagination
 *
 * Uses stable ordering: createdAt DESC, id DESC to ensure deterministic pagination.
 * The cursor encodes both createdAt and id to handle timestamp collisions correctly.
 *
 * PREFERRED FIRESTORE INDEX (for best performance):
 *   Collection: notes
 *   Fields: tenantId ASC, createdAt DESC, __name__ DESC
 *
 * Falls back to client-side filtering if index doesn't exist yet.
 */
export async function listNotes(
  tenantId: string = DEFAULT_TENANT_ID,
  limit: number = NOTES_PAGE_LIMIT,
  cursor?: string
): Promise<NotesListResponse> {
  const db = getDb();
  const pageLimit = Math.min(Math.max(1, limit), MAX_NOTES_PAGE_LIMIT);
  const cursorData = parseCursor(cursor);

  // Try optimized query with index first, fall back to legacy if index missing
  try {
    return await listNotesOptimized(db, tenantId, pageLimit, cursorData);
  } catch (err: unknown) {
    const errorMessage = err instanceof Error ? err.message : String(err);
    if (errorMessage.includes('FAILED_PRECONDITION') || errorMessage.includes('requires an index')) {
      logWarn('Notes index not found, using legacy query', { tenantId });
      return await listNotesLegacy(db, tenantId, pageLimit, cursorData);
    }
    throw err;
  }
}

/**
 * Optimized query using composite index
 */
async function listNotesOptimized(
  db: FirebaseFirestore.Firestore,
  tenantId: string,
  pageLimit: number,
  cursorData: { createdAt: Date; id: string } | null
): Promise<NotesListResponse> {
  let query = db
    .collection(NOTES_COLLECTION)
    .where('tenantId', '==', tenantId)
    .orderBy('createdAt', 'desc')
    .orderBy('__name__', 'desc')
    .limit(pageLimit + 1);

  if (cursorData) {
    query = query.startAfter(
      Timestamp.fromDate(cursorData.createdAt),
      cursorData.id
    );
  }

  const snap = await query.get();

  // Map documents to NoteDoc (no client-side filtering needed anymore)
  const docs = snap.docs.map(d => {
    const data = d.data() as NoteDoc;
    // Ensure tenantId is set (should always be present after backfill)
    if (!data.tenantId) {
      data.tenantId = DEFAULT_TENANT_ID;
    }
    return data;
  });

  // Determine if there are more results
  const hasMore = docs.length > pageLimit;
  const resultDocs = hasMore ? docs.slice(0, pageLimit) : docs;

  // Build next cursor from last result
  let nextCursor: string | null = null;
  if (hasMore && resultDocs.length > 0) {
    const lastDoc = resultDocs[resultDocs.length - 1];
    const lastCreatedAt = lastDoc.createdAt as Timestamp;
    nextCursor = encodeCursor(lastCreatedAt, lastDoc.id);
  }

  return {
    notes: resultDocs.map(docToResponse),
    cursor: nextCursor,
    hasMore,
  };
}

/**
 * Legacy query fallback - uses client-side filtering when index doesn't exist
 */
async function listNotesLegacy(
  db: FirebaseFirestore.Firestore,
  tenantId: string,
  pageLimit: number,
  cursorData: { createdAt: Date; id: string } | null
): Promise<NotesListResponse> {
  // Fetch more than needed to account for client-side filtering
  const fetchLimit = pageLimit * 3;

  let query = db
    .collection(NOTES_COLLECTION)
    .orderBy('createdAt', 'desc')
    .limit(fetchLimit + 1);

  if (cursorData) {
    query = query.startAfter(Timestamp.fromDate(cursorData.createdAt));
  }

  const snap = await query.get();

  // Client-side filtering (legacy mode)
  const allDocs = snap.docs.map(d => {
    const data = d.data() as NoteDoc;
    if (!data.tenantId) {
      data.tenantId = DEFAULT_TENANT_ID;
    }
    return data;
  });

  const docs = allDocs.filter(d => d.tenantId === tenantId);

  const hasMore = docs.length > pageLimit;
  const resultDocs = hasMore ? docs.slice(0, pageLimit) : docs;

  let nextCursor: string | null = null;
  if (hasMore && resultDocs.length > 0) {
    const lastDoc = resultDocs[resultDocs.length - 1];
    const lastCreatedAt = lastDoc.createdAt as Timestamp;
    nextCursor = encodeCursor(lastCreatedAt, lastDoc.id);
  }

  return {
    notes: resultDocs.map(docToResponse),
    cursor: nextCursor,
    hasMore,
  };
}

/**
 * Get a single note by ID
 */
export async function getNote(
  noteId: string,
  tenantId: string = DEFAULT_TENANT_ID
): Promise<NoteResponse | null> {
  const db = getDb();
  const doc = await db.collection(NOTES_COLLECTION).doc(noteId).get();
  
  if (!doc.exists) return null;
  
  const data = doc.data() as NoteDoc;
  
  // Verify tenant access
  if (data.tenantId !== tenantId) return null;
  
  return docToResponse(data);
}



================================================================================
FILE: src/query.ts
LINES: 200
PATH: /Users/salscrudato/Projects/auroranotes-api/src/query.ts
================================================================================

/**
 * AuroraNotes API - Query Understanding Module
 * 
 * Analyzes user queries to extract intent, time hints, keywords, and entities.
 * Improves retrieval quality by understanding what the user is looking for.
 */

import { QueryAnalysis, QueryIntent } from "./types";
import { extractKeywords } from "./utils";

// Intent detection patterns - ordered by specificity (most specific first)
const INTENT_PATTERNS: { pattern: RegExp; intent: QueryIntent }[] = [
  // Summarize patterns
  { pattern: /\b(summarize|summary|overview|recap|brief|tldr|tl;dr)\b/i, intent: 'summarize' },
  { pattern: /\bwhat (are|were) (my|the|our) (key|main|important)\b/i, intent: 'summarize' },
  { pattern: /\bgive me (a|the) (summary|overview|recap)\b/i, intent: 'summarize' },
  { pattern: /\bhighlight(s)?\b/i, intent: 'summarize' },

  // Decision patterns - check before general question patterns
  { pattern: /\b(decision|decide|chose|chosen|selected|picked|went with)\b/i, intent: 'decision' },
  { pattern: /\bwhy did (I|we) (choose|pick|select|go with|decide)\b/i, intent: 'decision' },
  { pattern: /\bwhat did (I|we) decide\b/i, intent: 'decision' },
  { pattern: /\b(reasoning|rationale) (behind|for)\b/i, intent: 'decision' },

  // Action item patterns
  { pattern: /\b(todo|to-do|action item|task|next step|follow[- ]?up)\b/i, intent: 'action_item' },
  { pattern: /\bwhat (do I|should I|need to|must I) (do|complete|finish|work on)\b/i, intent: 'action_item' },
  { pattern: /\bpending (task|item|work)\b/i, intent: 'action_item' },
  { pattern: /\bremind(er)?\b/i, intent: 'action_item' },

  // List patterns
  { pattern: /\b(list|show me|give me|enumerate|all the)\b/i, intent: 'list' },
  { pattern: /\bhow many\b/i, intent: 'list' },
  { pattern: /\bwhat are (all|the)\b/i, intent: 'list' },

  // Question patterns (generic - lowest priority)
  { pattern: /^(what|who|when|where|why|how|which|is|are|was|were|do|does|did|can|could|will|would)\b/i, intent: 'question' },
];

// Time hint patterns with more granularity
const TIME_PATTERNS: { pattern: RegExp; days: number }[] = [
  { pattern: /\b(today|now|current|just now)\b/i, days: 1 },
  { pattern: /\byesterday\b/i, days: 2 },
  { pattern: /\b(this week|past week|current week)\b/i, days: 7 },
  { pattern: /\blast week\b/i, days: 14 },
  { pattern: /\bpast (few|couple) days\b/i, days: 5 },
  { pattern: /\b(this month|past month|current month)\b/i, days: 30 },
  { pattern: /\blast month\b/i, days: 60 },
  { pattern: /\b(this year|past year)\b/i, days: 365 },
  { pattern: /\b(recent(ly)?|latest|newest|new)\b/i, days: 14 },
  { pattern: /\b(last|past) (\d+) days?\b/i, days: -1 }, // Special: extract number
  { pattern: /\b(last|past) (\d+) weeks?\b/i, days: -2 }, // Special: extract weeks
  { pattern: /\b(last|past) (\d+) months?\b/i, days: -3 }, // Special: extract months
  { pattern: /\ball (time|notes|history|ever)\b/i, days: 365 },
  { pattern: /\b(older|old|earlier)\b/i, days: 180 },
];

// Entity extraction patterns (projects, names, etc.)
const ENTITY_PATTERNS: RegExp[] = [
  // Capitalized words that might be names/projects (2+ chars)
  /\b([A-Z][a-z]{2,}(?:\s+[A-Z][a-z]{2,})*)\b/g,
  // Quoted terms
  /"([^"]+)"/g,
  /'([^']+)'/g,
];

/**
 * Normalize query for consistent processing
 */
function normalizeQuery(query: string): string {
  return query
    .trim()
    .replace(/\s+/g, ' ')
    .replace(/[^\w\s?!.,'"()-]/g, '') // Remove unusual chars
    .slice(0, 2000); // Limit length
}

/**
 * Detect query intent
 */
function detectIntent(query: string): QueryIntent {
  const lowerQuery = query.toLowerCase();
  
  for (const { pattern, intent } of INTENT_PATTERNS) {
    if (pattern.test(lowerQuery)) {
      return intent;
    }
  }
  
  // Default to search if no specific intent detected
  return 'search';
}

/**
 * Extract time hints from query with support for various time units
 */
function extractTimeHint(query: string): QueryAnalysis['timeHint'] | undefined {
  const lowerQuery = query.toLowerCase();

  for (const { pattern, days } of TIME_PATTERNS) {
    const match = lowerQuery.match(pattern);
    if (match) {
      // Handle "last N days" pattern
      if (days === -1 && match[2]) {
        const numDays = parseInt(match[2], 10);
        if (!isNaN(numDays) && numDays > 0 && numDays <= 365) {
          return { days: numDays };
        }
      }
      // Handle "last N weeks" pattern
      if (days === -2 && match[2]) {
        const numWeeks = parseInt(match[2], 10);
        if (!isNaN(numWeeks) && numWeeks > 0 && numWeeks <= 52) {
          return { days: numWeeks * 7 };
        }
      }
      // Handle "last N months" pattern
      if (days === -3 && match[2]) {
        const numMonths = parseInt(match[2], 10);
        if (!isNaN(numMonths) && numMonths > 0 && numMonths <= 12) {
          return { days: numMonths * 30 };
        }
      }
      if (days > 0) {
        return { days };
      }
    }
  }

  return undefined;
}

/**
 * Extract named entities from query
 */
function extractEntities(query: string): string[] {
  const entities = new Set<string>();
  
  for (const pattern of ENTITY_PATTERNS) {
    let match;
    // Reset regex state
    pattern.lastIndex = 0;
    while ((match = pattern.exec(query)) !== null) {
      const entity = match[1].trim();
      // Filter out common words that might be capitalized
      const commonWords = ['I', 'My', 'The', 'What', 'When', 'Where', 'Why', 'How', 'Which', 'Who'];
      if (entity.length > 1 && !commonWords.includes(entity)) {
        entities.add(entity);
      }
    }
  }
  
  return Array.from(entities).slice(0, 5);
}

/**
 * Generate boost terms based on intent and keywords
 */
function generateBoostTerms(keywords: string[], intent: QueryIntent): string[] {
  const boostTerms = [...keywords];
  
  // Add intent-specific boost terms
  switch (intent) {
    case 'decision':
      boostTerms.push('decided', 'chose', 'selected', 'because', 'reason');
      break;
    case 'action_item':
      boostTerms.push('todo', 'need', 'must', 'should', 'action', 'next');
      break;
    case 'summarize':
      boostTerms.push('key', 'main', 'important', 'summary');
      break;
  }
  
  return [...new Set(boostTerms)].slice(0, 15);
}

/**
 * Main query analysis function
 */
export function analyzeQuery(query: string): QueryAnalysis {
  const normalizedQuery = normalizeQuery(query);
  const intent = detectIntent(normalizedQuery);
  const keywords = extractKeywords(normalizedQuery);
  const timeHint = extractTimeHint(normalizedQuery);
  const entities = extractEntities(query); // Use original for entity extraction
  const boostTerms = generateBoostTerms(keywords, intent);
  
  return {
    originalQuery: query,
    normalizedQuery,
    keywords,
    intent,
    timeHint,
    entities: entities.length > 0 ? entities : undefined,
    boostTerms: boostTerms.length > keywords.length ? boostTerms : undefined,
  };
}



================================================================================
FILE: src/queryExpansion.ts
LINES: 173
PATH: /Users/salscrudato/Projects/auroranotes-api/src/queryExpansion.ts
================================================================================

/**
 * AuroraNotes API - Query Expansion Module
 * 
 * Uses Gemini to generate multiple query variations for improved recall.
 * This is an optional feature behind the QUERY_EXPANSION_ENABLED flag.
 * 
 * Multi-query expansion helps with:
 * - Synonym coverage (e.g., "meeting" → "call", "discussion", "sync")
 * - Phrasing variations (e.g., "how to X" → "steps for X", "X tutorial")
 * - Entity normalization (e.g., "AWS" → "Amazon Web Services")
 */

import { getGenAIClient, isGenAIAvailable } from "./genaiClient";
import { logInfo, logError, logWarn } from "./utils";
import { QUERY_EXPANSION_ENABLED, QUERY_EXPANSION_REWRITES } from "./config";

// Cache for expanded queries to avoid repeated LLM calls
const expansionCache = new Map<string, { variants: string[]; timestamp: number }>();
const CACHE_TTL_MS = 5 * 60 * 1000; // 5 minutes
const MAX_CACHE_SIZE = 100;

// Expansion prompt template
const EXPANSION_PROMPT = `You are a query expansion assistant for a personal notes search system.

Given a user's search query, generate ${QUERY_EXPANSION_REWRITES} alternative phrasings that would help find relevant notes.

Rules:
1. Keep the same semantic meaning
2. Use synonyms and related terms
3. Try different phrasings (questions, statements, keywords)
4. Include any acronym expansions or abbreviations
5. Keep each variant concise (under 50 words)
6. Return ONLY the variants, one per line, no numbering or bullets

User query: "{query}"

Alternative phrasings:`;

/**
 * Check if query expansion is available
 */
export function isQueryExpansionAvailable(): boolean {
  return QUERY_EXPANSION_ENABLED && isGenAIAvailable();
}

/**
 * Get cache key for a query
 */
function getCacheKey(query: string): string {
  return query.toLowerCase().trim();
}

/**
 * Evict old cache entries
 */
function evictOldCacheEntries(): void {
  const now = Date.now();
  for (const [key, value] of expansionCache.entries()) {
    if (now - value.timestamp > CACHE_TTL_MS) {
      expansionCache.delete(key);
    }
  }
  
  // Also evict if cache is too large
  if (expansionCache.size > MAX_CACHE_SIZE) {
    const entries = Array.from(expansionCache.entries());
    entries.sort((a, b) => a[1].timestamp - b[1].timestamp);
    const toDelete = entries.slice(0, entries.length - MAX_CACHE_SIZE);
    for (const [key] of toDelete) {
      expansionCache.delete(key);
    }
  }
}

/**
 * Expand a query into multiple variants using Gemini
 * 
 * @param query - Original user query
 * @returns Array of query variants (including original)
 */
export async function expandQuery(query: string): Promise<string[]> {
  if (!isQueryExpansionAvailable()) {
    return [query];
  }

  const cacheKey = getCacheKey(query);
  
  // Check cache
  const cached = expansionCache.get(cacheKey);
  if (cached && Date.now() - cached.timestamp < CACHE_TTL_MS) {
    logInfo('Query expansion cache hit', { query: query.slice(0, 50) });
    return cached.variants;
  }

  const startTime = Date.now();

  try {
    const client = getGenAIClient();
    const prompt = EXPANSION_PROMPT.replace('{query}', query);

    const response = await client.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: prompt,
      config: {
        temperature: 0.7, // Some creativity for variations
        maxOutputTokens: 200,
      },
    });

    const text = response.text?.trim() || '';
    
    // Parse variants from response
    const variants = text
      .split('\n')
      .map(line => line.trim())
      .filter(line => line.length > 0 && line.length < 200)
      .slice(0, QUERY_EXPANSION_REWRITES);

    // Always include original query first
    const allVariants = [query, ...variants.filter(v => v.toLowerCase() !== query.toLowerCase())];

    // Cache the result
    evictOldCacheEntries();
    expansionCache.set(cacheKey, { variants: allVariants, timestamp: Date.now() });

    logInfo('Query expansion complete', {
      originalQuery: query.slice(0, 50),
      variantCount: allVariants.length,
      elapsedMs: Date.now() - startTime,
    });

    return allVariants;
  } catch (err) {
    logError('Query expansion failed', err);
    return [query]; // Fallback to original query
  }
}

/**
 * Generate embeddings for all query variants
 * Returns a combined embedding (average of all variants)
 */
export async function generateExpandedEmbedding(
  query: string,
  generateEmbedding: (text: string) => Promise<number[]>
): Promise<{ embedding: number[]; variants: string[] }> {
  const variants = await expandQuery(query);
  
  if (variants.length === 1) {
    // No expansion, just use original
    const embedding = await generateEmbedding(query);
    return { embedding, variants };
  }

  // Generate embeddings for all variants in parallel
  const embeddings = await Promise.all(
    variants.map(v => generateEmbedding(v))
  );

  // Average the embeddings
  const dim = embeddings[0].length;
  const avgEmbedding = new Array(dim).fill(0);
  
  for (const emb of embeddings) {
    for (let i = 0; i < dim; i++) {
      avgEmbedding[i] += emb[i] / embeddings.length;
    }
  }

  return { embedding: avgEmbedding, variants };
}



================================================================================
FILE: src/queue.ts
LINES: 327
PATH: /Users/salscrudato/Projects/auroranotes-api/src/queue.ts
================================================================================

/**
 * AuroraNotes API - Background Job Queue
 *
 * In-process async queue with backpressure for chunk/embedding processing.
 * Provides graceful degradation when queue is full and retry logic.
 *
 * QUEUE MODES:
 *   - in-process (default): Jobs processed in-memory with backpressure
 *   - cloud-tasks (env QUEUE_MODE=cloud-tasks): Optional Cloud Tasks for durability
 */

import { NoteDoc } from "./types";
import { processNoteChunks } from "./chunking";
import { logInfo, logError, logWarn } from "./utils";
import {
  QUEUE_MODE,
  CLOUD_TASKS_QUEUE_NAME,
  CLOUD_TASKS_LOCATION,
  CLOUD_TASKS_SERVICE_URL,
  PROJECT_ID,
} from "./config";

// Queue configuration
const DEFAULT_MAX_QUEUE_SIZE = 100;
const DEFAULT_MAX_CONCURRENT = 3;
const DEFAULT_MAX_RETRIES = 3;
const DEFAULT_RETRY_DELAY_MS = 5000;
const STATS_LOG_INTERVAL_MS = 60000; // Log stats every minute

interface QueueJob {
  id: string;
  note: NoteDoc;
  retries: number;
  createdAt: Date;
}

interface QueueConfig {
  maxSize: number;
  maxConcurrent: number;
  maxRetries: number;
  retryDelayMs: number;
}

class BackgroundQueue {
  private queue: QueueJob[] = [];
  private processing: Set<string> = new Set();
  private config: QueueConfig;
  private isProcessing = false;
  private totalProcessed = 0;
  private totalFailed = 0;
  private totalDropped = 0;
  private totalRetries = 0;
  private lastStatsLog = 0;

  constructor(config?: Partial<QueueConfig>) {
    const envMaxSize = parseInt(process.env.BACKGROUND_QUEUE_MAX_SIZE || '');
    this.config = {
      maxSize: config?.maxSize ?? (isNaN(envMaxSize) ? DEFAULT_MAX_QUEUE_SIZE : envMaxSize),
      maxConcurrent: config?.maxConcurrent ?? DEFAULT_MAX_CONCURRENT,
      maxRetries: config?.maxRetries ?? DEFAULT_MAX_RETRIES,
      retryDelayMs: config?.retryDelayMs ?? DEFAULT_RETRY_DELAY_MS,
    };

    logInfo('Background queue initialized', {
      maxSize: this.config.maxSize,
      maxConcurrent: this.config.maxConcurrent,
      mode: process.env.QUEUE_MODE || 'in-process',
    });

    // Start periodic stats logging if there's activity
    this.startStatsLogger();
  }

  /**
   * Periodically log queue statistics for monitoring
   */
  private startStatsLogger(): void {
    setInterval(() => {
      // Only log if there's been activity since last log
      if (this.totalProcessed > 0 || this.totalFailed > 0 ||
          this.totalDropped > 0 || this.queue.length > 0) {
        this.logQueueStats();
      }
    }, STATS_LOG_INTERVAL_MS);
  }

  /**
   * Log comprehensive queue statistics
   */
  private logQueueStats(): void {
    const stats = this.getStats();
    const utilization = Math.round((stats.queueSize / this.config.maxSize) * 100);

    logInfo('Queue stats', {
      ...stats,
      maxSize: this.config.maxSize,
      utilization: `${utilization}%`,
      healthy: this.isHealthy(),
      mode: process.env.QUEUE_MODE || 'in-process',
    });
  }

  /**
   * Enqueue a note for background processing
   * Returns true if enqueued, false if queue is full
   */
  enqueue(note: NoteDoc): boolean {
    // Check if already in queue or processing
    if (this.queue.some(j => j.id === note.id) || this.processing.has(note.id)) {
      logInfo('Note already in queue', { noteId: note.id });
      return true;
    }

    // Check queue capacity
    if (this.queue.length >= this.config.maxSize) {
      this.totalDropped++;
      logWarn('Queue full, dropping job', { 
        noteId: note.id, 
        queueSize: this.queue.length,
        totalDropped: this.totalDropped,
      });
      return false;
    }

    this.queue.push({
      id: note.id,
      note,
      retries: 0,
      createdAt: new Date(),
    });

    logInfo('Job enqueued', { 
      noteId: note.id, 
      queueSize: this.queue.length,
    });

    // Start processing if not already running
    this.processQueue();
    return true;
  }

  /**
   * Process jobs from the queue
   */
  private async processQueue(): Promise<void> {
    if (this.isProcessing) return;
    this.isProcessing = true;

    while (this.queue.length > 0 && this.processing.size < this.config.maxConcurrent) {
      const job = this.queue.shift();
      if (!job) break;

      this.processing.add(job.id);
      this.processJob(job).finally(() => {
        this.processing.delete(job.id);
        // Continue processing if more jobs
        if (this.queue.length > 0) {
          setImmediate(() => this.processQueue());
        }
      });
    }

    this.isProcessing = false;
  }

  /**
   * Process a single job with retry logic
   */
  private async processJob(job: QueueJob): Promise<void> {
    const startTime = Date.now();
    try {
      await processNoteChunks(job.note);
      this.totalProcessed++;
      logInfo('Background job completed', {
        noteId: job.id,
        elapsedMs: Date.now() - startTime,
        totalProcessed: this.totalProcessed,
      });
    } catch (err) {
      if (job.retries < this.config.maxRetries) {
        job.retries++;
        this.totalRetries++;

        const delay = this.config.retryDelayMs * job.retries;
        logWarn('Background job failed, retrying', {
          noteId: job.id,
          attempt: job.retries,
          maxRetries: this.config.maxRetries,
          nextRetryMs: delay,
          errorMessage: err instanceof Error ? err.message : String(err),
        });

        // Re-queue with exponential backoff delay
        setTimeout(() => {
          this.queue.push(job);
          this.processQueue();
        }, delay);
      } else {
        this.totalFailed++;
        logError('Background job failed permanently', err, {
          noteId: job.id,
          attempts: job.retries + 1,
          totalFailed: this.totalFailed,
          queueStats: this.getStats(),
        });
      }
    }
  }

  /**
   * Get queue statistics for monitoring
   */
  getStats(): {
    queueSize: number;
    processing: number;
    totalProcessed: number;
    totalFailed: number;
    totalDropped: number;
    totalRetries: number;
  } {
    return {
      queueSize: this.queue.length,
      processing: this.processing.size,
      totalProcessed: this.totalProcessed,
      totalFailed: this.totalFailed,
      totalDropped: this.totalDropped,
      totalRetries: this.totalRetries,
    };
  }

  /**
   * Check if queue is healthy
   */
  isHealthy(): boolean {
    return this.queue.length < this.config.maxSize * 0.9;
  }
}

// Singleton instance
let queueInstance: BackgroundQueue | null = null;

export function getBackgroundQueue(): BackgroundQueue {
  if (!queueInstance) {
    queueInstance = new BackgroundQueue();
  }
  return queueInstance;
}

/**
 * Enqueue note for processing - uses Cloud Tasks or in-process queue based on QUEUE_MODE
 */
export async function enqueueNoteProcessing(note: NoteDoc): Promise<boolean> {
  if (QUEUE_MODE === 'cloud-tasks') {
    return enqueueToCloudTasks(note);
  }
  return getBackgroundQueue().enqueue(note);
}

/**
 * Enqueue note processing to Google Cloud Tasks for durable processing
 */
async function enqueueToCloudTasks(note: NoteDoc): Promise<boolean> {
  if (!CLOUD_TASKS_SERVICE_URL) {
    logError('Cloud Tasks service URL not configured', null);
    // Fall back to in-process queue
    return getBackgroundQueue().enqueue(note);
  }

  try {
    // Dynamic require to handle optional @google-cloud/tasks dependency
    // eslint-disable-next-line @typescript-eslint/no-var-requires
    let CloudTasksClient: any;
    try {
      // eslint-disable-next-line @typescript-eslint/no-require-imports
      CloudTasksClient = require('@google-cloud/tasks').CloudTasksClient;
    } catch {
      logError('@google-cloud/tasks not installed, falling back to in-process queue', null);
      return getBackgroundQueue().enqueue(note);
    }
    const client = new CloudTasksClient();

    const queuePath = client.queuePath(
      PROJECT_ID,
      CLOUD_TASKS_LOCATION,
      CLOUD_TASKS_QUEUE_NAME
    );

    // Create the task payload
    const taskPayload = {
      noteId: note.id,
      tenantId: note.tenantId,
    };

    const task = {
      httpRequest: {
        httpMethod: 'POST' as const,
        url: `${CLOUD_TASKS_SERVICE_URL}/internal/process-note`,
        headers: {
          'Content-Type': 'application/json',
        },
        body: Buffer.from(JSON.stringify(taskPayload)).toString('base64'),
      },
    };

    const [response] = await client.createTask({ parent: queuePath, task });

    logInfo('Task enqueued to Cloud Tasks', {
      noteId: note.id,
      taskName: response.name,
      queuePath,
    });

    return true;
  } catch (err) {
    logError('Failed to enqueue to Cloud Tasks, falling back to in-process', err, {
      noteId: note.id,
    });
    // Fall back to in-process queue on failure
    return getBackgroundQueue().enqueue(note);
  }
}

export function getQueueStats() {
  return getBackgroundQueue().getStats();
}



================================================================================
FILE: src/rateLimit.ts
LINES: 146
PATH: /Users/salscrudato/Projects/auroranotes-api/src/rateLimit.ts
================================================================================

/**
 * AuroraNotes API - Simple Rate Limiter
 * 
 * In-memory rate limiter for API protection.
 * Designed for single-instance deployment (Cloud Run with concurrency).
 */

import { Request, Response, NextFunction } from 'express';
import { logWarn } from './utils';
import { 
  RATE_LIMIT_ENABLED,
  RATE_LIMIT_REQUESTS_PER_MIN,
  RATE_LIMIT_WINDOW_MS,
} from './config';

interface RateLimitEntry {
  count: number;
  resetAt: number;
}

// In-memory store for rate limits (per IP)
const rateLimits = new Map<string, RateLimitEntry>();

// Cleanup interval to prevent memory leaks
const CLEANUP_INTERVAL_MS = 60000;

/**
 * Get client identifier for rate limiting
 * Uses X-Forwarded-For for Cloud Run, falls back to remote IP
 */
function getClientId(req: Request): string {
  const forwarded = req.headers['x-forwarded-for'];
  if (forwarded) {
    // X-Forwarded-For can contain multiple IPs, take the first
    const ip = Array.isArray(forwarded) ? forwarded[0] : forwarded.split(',')[0];
    return ip.trim();
  }
  return req.ip || req.socket.remoteAddress || 'unknown';
}

/**
 * Check rate limit for a client
 */
function checkRateLimit(clientId: string): { allowed: boolean; remaining: number; resetIn: number } {
  const now = Date.now();
  const entry = rateLimits.get(clientId);
  
  // No existing entry or expired window - create new
  if (!entry || now > entry.resetAt) {
    rateLimits.set(clientId, {
      count: 1,
      resetAt: now + RATE_LIMIT_WINDOW_MS,
    });
    return { 
      allowed: true, 
      remaining: RATE_LIMIT_REQUESTS_PER_MIN - 1,
      resetIn: RATE_LIMIT_WINDOW_MS,
    };
  }
  
  // Within window - check count
  if (entry.count >= RATE_LIMIT_REQUESTS_PER_MIN) {
    return { 
      allowed: false, 
      remaining: 0,
      resetIn: entry.resetAt - now,
    };
  }
  
  // Increment and allow
  entry.count++;
  return { 
    allowed: true, 
    remaining: RATE_LIMIT_REQUESTS_PER_MIN - entry.count,
    resetIn: entry.resetAt - now,
  };
}

/**
 * Rate limiting middleware
 * Only applies if RATE_LIMIT_ENABLED is true
 */
export function rateLimitMiddleware(req: Request, res: Response, next: NextFunction): void {
  // Skip rate limiting if disabled
  if (!RATE_LIMIT_ENABLED) {
    next();
    return;
  }
  
  // Skip health checks
  if (req.path === '/health') {
    next();
    return;
  }
  
  const clientId = getClientId(req);
  const { allowed, remaining, resetIn } = checkRateLimit(clientId);
  
  // Set rate limit headers
  res.set('X-RateLimit-Limit', String(RATE_LIMIT_REQUESTS_PER_MIN));
  res.set('X-RateLimit-Remaining', String(remaining));
  res.set('X-RateLimit-Reset', String(Math.ceil(resetIn / 1000)));
  
  if (!allowed) {
    logWarn('Rate limit exceeded', { clientId, path: req.path });
    res.status(429).json({
      error: 'Too many requests',
      retryAfter: Math.ceil(resetIn / 1000),
    });
    return;
  }
  
  next();
}

/**
 * Cleanup expired entries periodically
 */
function cleanupExpiredEntries(): void {
  const now = Date.now();
  let cleaned = 0;
  
  for (const [key, entry] of rateLimits.entries()) {
    if (now > entry.resetAt) {
      rateLimits.delete(key);
      cleaned++;
    }
  }
  
  // Only log if we cleaned something significant
  if (cleaned > 10) {
    logWarn('Rate limit cleanup', { cleaned, remaining: rateLimits.size });
  }
}

// Start cleanup interval
setInterval(cleanupExpiredEntries, CLEANUP_INTERVAL_MS);

/**
 * Get current rate limit stats (for debugging/monitoring)
 */
export function getRateLimitStats(): { activeClients: number } {
  return { activeClients: rateLimits.size };
}



================================================================================
FILE: src/reranker.ts
LINES: 151
PATH: /Users/salscrudato/Projects/auroranotes-api/src/reranker.ts
================================================================================

/**
 * AuroraNotes API - LLM Reranker Module
 *
 * Optional LLM-based reranking for improved retrieval quality.
 * Controlled by LLM_RERANK_ENABLED feature flag.
 * Uses minimal tokens and caches results for cost control.
 */

import { ScoredChunk } from "./types";
import { logInfo, logError, logWarn } from "./utils";
import { getGenAIClient, isGenAIAvailable } from "./genaiClient";

// Reranker configuration
const RERANK_MODEL = process.env.RERANK_MODEL || 'gemini-2.0-flash';
const RERANK_MAX_CHUNKS = 20;        // Max chunks to consider for reranking
const RERANK_MAX_OUTPUT_TOKENS = 200; // Limit output tokens for cost
const RERANK_TIMEOUT_MS = 5000;       // Timeout for rerank call

/**
 * Build reranking prompt
 */
function buildRerankPrompt(query: string, chunks: ScoredChunk[]): string {
  const chunkList = chunks
    .slice(0, RERANK_MAX_CHUNKS)
    .map((chunk, i) => `[${i + 1}] ${chunk.text.slice(0, 150)}`)
    .join('\n');

  return `Given this query: "${query}"

Rate these passages by relevance (most to least relevant).
Return ONLY comma-separated numbers like: 3,1,5,2,4

Passages:
${chunkList}

Ranking:`;
}

/**
 * Parse reranking response
 */
function parseRerankResponse(response: string, chunkCount: number): number[] {
  // Extract numbers from response
  const numbers = response.match(/\d+/g);
  if (!numbers) return [];

  const indices: number[] = [];
  const seen = new Set<number>();

  for (const numStr of numbers) {
    const num = parseInt(numStr, 10);
    // Validate: 1-indexed, within range, not duplicate
    if (num >= 1 && num <= chunkCount && !seen.has(num)) {
      indices.push(num - 1); // Convert to 0-indexed
      seen.add(num);
    }
  }

  return indices;
}

/**
 * LLM-based reranking of chunks
 * Returns chunks reordered by LLM relevance assessment
 */
export async function llmRerank(
  query: string,
  chunks: ScoredChunk[],
  maxResults: number
): Promise<ScoredChunk[]> {
  if (chunks.length <= 1) return chunks;

  const client = getGenAIClient();
  if (!client) {
    logWarn('LLM reranker: no API key, skipping');
    return chunks.slice(0, maxResults);
  }

  const startTime = Date.now();
  const chunksToRerank = chunks.slice(0, RERANK_MAX_CHUNKS);

  try {
    const prompt = buildRerankPrompt(query, chunksToRerank);
    
    // Create timeout promise
    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => reject(new Error('Rerank timeout')), RERANK_TIMEOUT_MS);
    });

    // Race between LLM call and timeout
    const result = await Promise.race([
      client.models.generateContent({
        model: RERANK_MODEL,
        contents: prompt,
        config: {
          temperature: 0.1,
          maxOutputTokens: RERANK_MAX_OUTPUT_TOKENS,
        },
      }),
      timeoutPromise,
    ]);

    const response = result.text || '';
    const reorderedIndices = parseRerankResponse(response, chunksToRerank.length);

    if (reorderedIndices.length === 0) {
      logWarn('LLM reranker: failed to parse response', { response: response.slice(0, 100) });
      return chunks.slice(0, maxResults);
    }

    // Build reranked array
    const reranked: ScoredChunk[] = [];
    const usedIndices = new Set<number>();

    // Add chunks in LLM-specified order
    for (const idx of reorderedIndices) {
      if (reranked.length >= maxResults) break;
      reranked.push(chunksToRerank[idx]);
      usedIndices.add(idx);
    }

    // Add any remaining chunks by original score
    for (let i = 0; i < chunksToRerank.length && reranked.length < maxResults; i++) {
      if (!usedIndices.has(i)) {
        reranked.push(chunksToRerank[i]);
      }
    }

    const elapsedMs = Date.now() - startTime;
    logInfo('LLM rerank complete', {
      inputCount: chunksToRerank.length,
      outputCount: reranked.length,
      elapsedMs,
    });

    return reranked;
  } catch (err) {
    logError('LLM rerank failed', err);
    // Graceful degradation: return original order
    return chunks.slice(0, maxResults);
  }
}

/**
 * Check if LLM reranker is available
 */
export function isLLMRerankerAvailable(): boolean {
  return isGenAIAvailable();
}



================================================================================
FILE: src/retrieval.ts
LINES: 1199
PATH: /Users/salscrudato/Projects/auroranotes-api/src/retrieval.ts
================================================================================

/**
 * AuroraNotes API - Retrieval Module
 *
 * Implements best-in-class multi-stage hybrid retrieval with:
 * - Multi-stage candidate generation (vector → lexical → recency)
 * - Recall-first architecture for near-perfect recall at scale
 * - BM25-like keyword matching (lexical precision)
 * - MMR diversity reranking (multi-note coverage)
 * - Entity/unique-ID detection (expands search for specific queries)
 * - Position bonuses (intro/summary detection)
 * - Coverage-aware reranking (ensure query keywords are represented)
 *
 * Scale targets: 100k+ notes, millions of chunks with sub-second retrieval
 */

import { Timestamp } from "firebase-admin/firestore";
import { getDb } from "./firestore";
import {
  CHUNKS_COLLECTION,
  RETRIEVAL_DEFAULT_DAYS,
  RETRIEVAL_MAX_CONTEXT_CHARS,
  VECTOR_SEARCH_ENABLED,
  RERANKING_ENABLED,
  LLM_RERANK_ENABLED,
  RETRIEVAL_VECTOR_TOP_K,
  RETRIEVAL_LEXICAL_TOP_K,
  RETRIEVAL_LEXICAL_MAX_TERMS,
  RETRIEVAL_RECENCY_TOP_K,
  RETRIEVAL_MERGED_TOP_K,
  RETRIEVAL_MMR_ENABLED,
  RETRIEVAL_MMR_LAMBDA,
  SCORE_WEIGHT_VECTOR,
  SCORE_WEIGHT_LEXICAL,
  SCORE_WEIGHT_RECENCY,
  SCORE_WEIGHT_ID_BOOST,
} from "./config";
import { ChunkDoc, ScoredChunk, RetrievalOptions, QueryAnalysis, CandidateCounts, RetrievalTimingsStage } from "./types";
import { generateQueryEmbedding, isEmbeddingsAvailable } from "./embeddings";
import { cosineSimilarity, logInfo, logError, logWarn, extractTermsForIndexing } from "./utils";
import { analyzeQuery } from "./query";
import { llmRerank, isLLMRerankerAvailable } from "./reranker";
import { getVectorIndex, VectorSearchResult } from "./vectorIndex";
import { expandQuery, isQueryExpansionAvailable } from "./queryExpansion";

// Quality thresholds (tuned for better recall while maintaining precision)
const MIN_VECTOR_SCORE = 0.15;     // Lower threshold for recall-first (was 0.20)
const MIN_COMBINED_SCORE = 0.05;   // Lower for better recall (was 0.08)
const DIVERSITY_PENALTY = 0.10;    // Penalty for over-represented notes
const MAX_CHUNKS_PER_NOTE = 4;     // Max chunks from single note before diversity penalty

// Batch hydration configuration
const BATCH_HYDRATION_MAX = 500;   // Max chunks to hydrate from vector results (configurable cap)

// Entity/unique-ID query detection settings
const ENTITY_EXPANDED_DAYS = 365;     // Expand to 1 year for entity queries
const ENTITY_EXPANDED_LIMIT = 500;    // Fetch more candidates for entity queries
const ALL_TIME_PATTERNS = [
  /\b(all|ever|always|any time|anytime|history|historical)\b/i,
  /\b(first|original|oldest|earliest|initial)\b/i,
];

// BM25 parameters (tuned for note-style documents)
const BM25_K1 = 1.2;  // Slightly lower for shorter documents
const BM25_B = 0.75;  // Document length normalization

// Position bonus for chunks earlier in a note (more likely to be introduction/summary)
const POSITION_BONUS_MAX = 0.05;   // Reduced to not over-weight position

// Re-export analyzeQuery for backward compatibility
export { analyzeQuery } from "./query";

/**
 * Batch hydrate chunk documents from Firestore using getAll().
 * Preserves ordering from vectorResults by score.
 *
 * Uses Firestore Admin SDK batch getAll for efficient multi-document fetch.
 * Caps to BATCH_HYDRATION_MAX to prevent excessive memory usage.
 *
 * @param vectorResults - Results from vector search with chunkId and score
 * @returns ChunkDoc array ordered by original score ranking
 */
async function batchHydrateChunks(
  vectorResults: VectorSearchResult[]
): Promise<{ chunks: ChunkDoc[]; hydratedCount: number; cappedAt: number | null }> {
  if (vectorResults.length === 0) {
    return { chunks: [], hydratedCount: 0, cappedAt: null };
  }

  const db = getDb();
  const cappedAt = vectorResults.length > BATCH_HYDRATION_MAX ? BATCH_HYDRATION_MAX : null;
  const resultsToFetch = vectorResults.slice(0, BATCH_HYDRATION_MAX);

  // Build document references for batch fetch
  const docRefs = resultsToFetch.map(r =>
    db.collection(CHUNKS_COLLECTION).doc(r.chunkId)
  );

  // Use getAll for efficient batch fetch (single round-trip to Firestore)
  const startTime = Date.now();
  const snapshots = await db.getAll(...docRefs);

  // Build a map of chunkId -> ChunkDoc for reordering
  const chunkMap = new Map<string, ChunkDoc>();
  for (const snap of snapshots) {
    if (snap.exists) {
      const data = snap.data() as ChunkDoc;
      chunkMap.set(snap.id, data);
    }
  }

  // Preserve ordering by vector score (resultsToFetch order)
  const orderedChunks: ChunkDoc[] = [];
  for (const r of resultsToFetch) {
    const chunk = chunkMap.get(r.chunkId);
    if (chunk) {
      orderedChunks.push(chunk);
    }
  }

  if (cappedAt) {
    logWarn('Batch hydration capped due to size limit', {
      requestedCount: vectorResults.length,
      cappedAt: BATCH_HYDRATION_MAX,
      hydratedCount: orderedChunks.length,
      elapsedMs: Date.now() - startTime,
    });
  } else {
    logInfo('Batch hydration complete', {
      requestedCount: vectorResults.length,
      hydratedCount: orderedChunks.length,
      elapsedMs: Date.now() - startTime,
    });
  }

  return {
    chunks: orderedChunks,
    hydratedCount: orderedChunks.length,
    cappedAt,
  };
}

/**
 * Fetch candidate chunks from Firestore with optimized queries
 *
 * Uses server-side filtering when possible for better performance.
 * Falls back to client-side filtering for backward compatibility.
 */
async function fetchCandidates(
  tenantId: string,
  maxAgeDays: number,
  limit: number
): Promise<ChunkDoc[]> {
  const db = getDb();
  const cutoffDate = new Date();
  cutoffDate.setDate(cutoffDate.getDate() - maxAgeDays);
  const cutoffTimestamp = Timestamp.fromDate(cutoffDate);

  try {
    // Try optimized query with composite index (tenantId + createdAt)
    // Requires Firestore index: noteChunks(tenantId ASC, createdAt DESC)
    const snap = await db
      .collection(CHUNKS_COLLECTION)
      .where('tenantId', '==', tenantId)
      .where('createdAt', '>=', cutoffTimestamp)
      .orderBy('createdAt', 'desc')
      .limit(limit)
      .get();

    if (!snap.empty) {
      return snap.docs.map(d => d.data() as ChunkDoc);
    }
  } catch (err) {
    // Index may not exist yet, fall back to client-side filtering
    logError('Optimized chunk query failed, using fallback', err);
  }

  // Fallback: fetch all and filter client-side (for backward compatibility)
  const snap = await db
    .collection(CHUNKS_COLLECTION)
    .orderBy('createdAt', 'desc')
    .limit(limit * 2) // Fetch more to account for filtering
    .get();

  const chunks = snap.docs
    .map(d => {
      const data = d.data() as ChunkDoc;
      if (!data.tenantId) {
        data.tenantId = 'public';
      }
      return data;
    })
    .filter(c => {
      const createdAt = c.createdAt instanceof Timestamp
        ? c.createdAt.toDate()
        : new Date();
      return c.tenantId === tenantId && createdAt >= cutoffDate;
    })
    .slice(0, limit);

  return chunks;
}

/**
 * Fetch chunks via lexical search using terms[] field
 *
 * Uses Firestore array-contains-any for indexed term matching.
 * This provides exact-match recall for identifiers, codes, and specific terms.
 * Requires Firestore index: noteChunks(tenantId, terms array-contains-any)
 */
async function fetchLexicalCandidates(
  tenantId: string,
  queryTerms: string[],
  limit: number
): Promise<ChunkDoc[]> {
  if (queryTerms.length === 0) {
    return [];
  }

  const db = getDb();
  const startTime = Date.now();

  // Firestore array-contains-any supports max 10 values
  const searchTerms = queryTerms.slice(0, RETRIEVAL_LEXICAL_MAX_TERMS);

  try {
    // Query chunks where terms[] contains any of the search terms
    const snap = await db
      .collection(CHUNKS_COLLECTION)
      .where('tenantId', '==', tenantId)
      .where('terms', 'array-contains-any', searchTerms)
      .limit(limit)
      .get();

    const chunks = snap.docs.map(d => d.data() as ChunkDoc);

    logInfo('Lexical search complete', {
      tenantId,
      searchTerms: searchTerms.length,
      resultsReturned: chunks.length,
      elapsedMs: Date.now() - startTime,
    });

    return chunks;
  } catch (err) {
    // Index may not exist yet - this is expected for new deployments
    logWarn('Lexical search failed (index may not exist)', { error: String(err) });
    return [];
  }
}

/**
 * Fetch recent chunks for recency signal
 * Returns the most recent chunks regardless of relevance
 */
async function fetchRecentCandidates(
  tenantId: string,
  limit: number
): Promise<ChunkDoc[]> {
  const db = getDb();

  const snap = await db
    .collection(CHUNKS_COLLECTION)
    .where('tenantId', '==', tenantId)
    .orderBy('createdAt', 'desc')
    .limit(limit)
    .get();

  return snap.docs.map(d => d.data() as ChunkDoc);
}

/**
 * Merge candidates from multiple stages, deduplicating by chunkId
 * Returns merged list with source tracking
 */
function mergeCandidates(
  vectorChunks: ChunkDoc[],
  lexicalChunks: ChunkDoc[],
  recencyChunks: ChunkDoc[]
): { chunks: ChunkDoc[]; sources: Map<string, Set<'vector' | 'lexical' | 'recency'>> } {
  const chunkMap = new Map<string, ChunkDoc>();
  const sources = new Map<string, Set<'vector' | 'lexical' | 'recency'>>();

  // Add vector candidates
  for (const chunk of vectorChunks) {
    chunkMap.set(chunk.chunkId, chunk);
    sources.set(chunk.chunkId, new Set(['vector']));
  }

  // Add lexical candidates
  for (const chunk of lexicalChunks) {
    if (chunkMap.has(chunk.chunkId)) {
      sources.get(chunk.chunkId)!.add('lexical');
    } else {
      chunkMap.set(chunk.chunkId, chunk);
      sources.set(chunk.chunkId, new Set(['lexical']));
    }
  }

  // Add recency candidates
  for (const chunk of recencyChunks) {
    if (chunkMap.has(chunk.chunkId)) {
      sources.get(chunk.chunkId)!.add('recency');
    } else {
      chunkMap.set(chunk.chunkId, chunk);
      sources.set(chunk.chunkId, new Set(['recency']));
    }
  }

  return {
    chunks: Array.from(chunkMap.values()),
    sources,
  };
}

/**
 * Score chunks based on vector similarity with normalization
 */
function scoreByVector(
  chunks: ChunkDoc[],
  queryEmbedding: number[]
): Map<string, number> {
  const rawScores = new Map<string, number>();

  for (const chunk of chunks) {
    if (chunk.embedding) {
      const similarity = cosineSimilarity(queryEmbedding, chunk.embedding);
      // Apply minimum threshold
      if (similarity >= MIN_VECTOR_SCORE) {
        rawScores.set(chunk.chunkId, similarity);
      } else {
        rawScores.set(chunk.chunkId, similarity * 0.5); // Penalize low scores
      }
    }
  }

  return normalizeScores(rawScores);
}

/**
 * Check if a keyword looks like a unique identifier (uppercase with numbers/underscores)
 */
function isUniqueIdentifier(keyword: string): boolean {
  // Match patterns like CITE_TEST_002, PROJECT_ALPHA, TEST123
  return /^[a-z][a-z0-9_]*[0-9_][a-z0-9_]*$/i.test(keyword) ||
         /^[a-z]+_[a-z0-9_]+$/i.test(keyword);
}

/**
 * Score chunks based on keyword overlap with BM25-like weighting
 * BM25 provides better relevance ranking than simple TF-IDF
 * Unique identifiers get significantly boosted scoring
 */
function scoreByKeywords(
  chunks: ChunkDoc[],
  keywords: string[]
): Map<string, number> {
  const scores = new Map<string, number>();

  if (keywords.length === 0) return scores;

  // Separate unique identifiers from regular keywords
  const uniqueIds = keywords.filter(isUniqueIdentifier);
  const regularKeywords = keywords.filter(k => !isUniqueIdentifier(k));

  // Calculate document frequency for each keyword
  const docFreq = new Map<string, number>();
  for (const keyword of keywords) {
    let count = 0;
    for (const chunk of chunks) {
      if (chunk.text.toLowerCase().includes(keyword)) {
        count++;
      }
    }
    docFreq.set(keyword, count || 1);
  }

  const totalDocs = chunks.length || 1;

  // Calculate average document length for BM25
  const avgDocLength = chunks.reduce((sum, c) => sum + c.text.length, 0) / totalDocs;

  for (const chunk of chunks) {
    const chunkLower = chunk.text.toLowerCase();
    const docLength = chunk.text.length;
    let weightedScore = 0;
    let uniqueIdMatchCount = 0;

    // First pass: check unique identifier matches (these are critical)
    for (const uniqueId of uniqueIds) {
      if (chunkLower.includes(uniqueId.toLowerCase())) {
        uniqueIdMatchCount++;
        // Unique IDs get massive boost - they're the most specific signals
        weightedScore += 3.0; // High fixed score for unique ID match
      }
    }

    // Second pass: regular keywords with BM25
    for (const keyword of regularKeywords) {
      const keywordLower = keyword.toLowerCase();
      const matches = chunkLower.match(new RegExp(escapeRegex(keywordLower), 'g')) || [];
      const tf = matches.length;

      if (tf > 0) {
        const df = docFreq.get(keyword) || 1;
        const idf = Math.log((totalDocs - df + 0.5) / (df + 0.5) + 1);

        const tfNormalized = (tf * (BM25_K1 + 1)) /
          (tf + BM25_K1 * (1 - BM25_B + BM25_B * (docLength / avgDocLength)));

        weightedScore += idf * tfNormalized;

        // Position boost for early matches (intro/summary detection)
        if (chunkLower.startsWith(keywordLower) || chunkLower.indexOf(keywordLower) < 50) {
          weightedScore += idf * 0.3;
        }

        // Exact word boundary match bonus (not just substring)
        const wordBoundaryPattern = new RegExp(`\\b${escapeRegex(keywordLower)}\\b`, 'g');
        const exactMatches = chunkLower.match(wordBoundaryPattern) || [];
        if (exactMatches.length > 0) {
          weightedScore += idf * 0.4 * exactMatches.length; // Bonus for exact word matches
        }
      }
    }

    // Penalize chunks that don't match unique IDs when unique IDs are present in query
    if (uniqueIds.length > 0 && uniqueIdMatchCount === 0) {
      weightedScore *= 0.2; // Strong penalty for missing required unique ID
    }

    scores.set(chunk.chunkId, weightedScore / Math.max(keywords.length, 1));
  }

  return normalizeScores(scores);
}

/**
 * Escape special regex characters in keyword
 */
function escapeRegex(str: string): string {
  return str.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
}

/**
 * Score chunks based on recency with exponential decay
 */
function scoreByRecency(
  chunks: ChunkDoc[],
  maxAgeDays: number
): Map<string, number> {
  const scores = new Map<string, number>();
  const now = Date.now();
  const halfLifeMs = (maxAgeDays / 3) * 24 * 60 * 60 * 1000; // Decay half-life

  for (const chunk of chunks) {
    const createdAt = chunk.createdAt instanceof Timestamp
      ? chunk.createdAt.toDate()
      : new Date();
    const ageMs = now - createdAt.getTime();

    // Exponential decay for more natural recency scoring
    const recencyScore = Math.exp(-ageMs / halfLifeMs);
    scores.set(chunk.chunkId, recencyScore);
  }

  return scores;
}

/**
 * Normalize scores to [0, 1] range using min-max normalization
 */
function normalizeScores(scores: Map<string, number>): Map<string, number> {
  if (scores.size === 0) return scores;

  let min = Infinity;
  let max = -Infinity;

  for (const score of scores.values()) {
    min = Math.min(min, score);
    max = Math.max(max, score);
  }

  if (max === min) return scores; // All scores are equal

  const normalized = new Map<string, number>();
  for (const [key, value] of scores.entries()) {
    normalized.set(key, (value - min) / (max - min));
  }

  return normalized;
}

/**
 * Calculate position bonus - earlier chunks in a note often contain key info
 */
function getPositionBonus(position: number): number {
  // Position 0 (first chunk) gets max bonus, decays quickly
  return POSITION_BONUS_MAX * Math.exp(-position * 0.5);
}

/**
 * Combine scores with dynamic weights based on available signals
 * Includes position bonus for chunks at the start of notes
 */
function combineScores(
  chunks: ChunkDoc[],
  vectorScores: Map<string, number>,
  keywordScores: Map<string, number>,
  recencyScores: Map<string, number>,
  hasVectorSearch: boolean
): ScoredChunk[] {
  // Dynamic weights based on available signals
  let vectorWeight: number;
  let keywordWeight: number;
  let recencyWeight: number;

  if (hasVectorSearch && vectorScores.size > 0) {
    // Full hybrid mode - slightly increased keyword weight for better recall
    vectorWeight = 0.45;  // Semantic understanding (was 0.50)
    keywordWeight = 0.40; // Keyword matching (BM25) - increased for better recall (was 0.35)
    recencyWeight = 0.15; // Freshness
  } else {
    // Keyword-only fallback
    vectorWeight = 0;
    keywordWeight = 0.75; // Increased for better keyword matching (was 0.70)
    recencyWeight = 0.25; // Reduced slightly (was 0.30)
  }

  return chunks.map(chunk => {
    const vectorScore = vectorScores.get(chunk.chunkId) || 0;
    const keywordScore = keywordScores.get(chunk.chunkId) || 0;
    const recencyScore = recencyScores.get(chunk.chunkId) || 0;
    const positionBonus = getPositionBonus(chunk.position);

    // Combine weighted scores plus position bonus
    const combinedScore =
      vectorWeight * vectorScore +
      keywordWeight * keywordScore +
      recencyWeight * recencyScore +
      positionBonus;

    const createdAt = chunk.createdAt instanceof Timestamp
      ? chunk.createdAt.toDate()
      : new Date();

    return {
      chunkId: chunk.chunkId,
      noteId: chunk.noteId,
      tenantId: chunk.tenantId,
      text: chunk.text,
      position: chunk.position,
      createdAt,
      score: combinedScore,
      vectorScore,
      keywordScore,
      recencyScore,
    };
  });
}

/**
 * MMR (Maximal Marginal Relevance) reranking for diversity
 *
 * Balances relevance with diversity to avoid returning 8 chunks from one note
 * unless the query really requires it.
 *
 * MMR score = λ * relevance - (1-λ) * max_similarity_to_selected
 *
 * @param chunks - Scored chunks sorted by relevance
 * @param lambda - Trade-off parameter (0.7 = 70% relevance, 30% diversity)
 * @param targetK - Number of chunks to select
 */
export function applyMMRReranking(
  chunks: ScoredChunk[],
  lambda: number = RETRIEVAL_MMR_LAMBDA,
  targetK: number
): ScoredChunk[] {
  if (chunks.length <= targetK) {
    return chunks;
  }

  const selected: ScoredChunk[] = [];
  const remaining = new Set(chunks.map((_, i) => i));

  // Normalize scores to [0, 1] for MMR calculation
  const maxScore = Math.max(...chunks.map(c => c.score), 0.001);
  const normalizedScores = chunks.map(c => c.score / maxScore);

  while (selected.length < targetK && remaining.size > 0) {
    let bestIdx = -1;
    let bestMMR = -Infinity;

    for (const idx of remaining) {
      const relevance = normalizedScores[idx];

      // Calculate max similarity to already selected chunks
      let maxSimilarity = 0;
      for (const selectedChunk of selected) {
        // Use note-based similarity: same note = high similarity
        // This encourages diversity across notes
        const sameNote = chunks[idx].noteId === selectedChunk.noteId;
        const textSim = sameNote ? 0.8 : 0; // Penalize same note

        // Could also compute actual text/embedding similarity here
        // but note-based is efficient and effective
        maxSimilarity = Math.max(maxSimilarity, textSim);
      }

      // MMR score
      const mmrScore = lambda * relevance - (1 - lambda) * maxSimilarity;

      if (mmrScore > bestMMR) {
        bestMMR = mmrScore;
        bestIdx = idx;
      }
    }

    if (bestIdx >= 0) {
      selected.push(chunks[bestIdx]);
      remaining.delete(bestIdx);
    } else {
      break;
    }
  }

  return selected;
}

/**
 * Apply diversity reranking to avoid too many chunks from the same note
 * while still allowing sufficient context from relevant notes
 * (Fallback when MMR is disabled)
 */
function applyDiversityReranking(chunks: ScoredChunk[], maxPerNote: number = MAX_CHUNKS_PER_NOTE): ScoredChunk[] {
  const noteCount = new Map<string, number>();
  const result: ScoredChunk[] = [];

  for (const chunk of chunks) {
    const count = noteCount.get(chunk.noteId) || 0;

    if (count < maxPerNote) {
      result.push(chunk);
      noteCount.set(chunk.noteId, count + 1);
    } else {
      // Apply penalty for over-represented notes
      const penalizedChunk = {
        ...chunk,
        score: chunk.score * (1 - DIVERSITY_PENALTY * (count - maxPerNote + 1)),
      };
      result.push(penalizedChunk);
    }
  }

  // Re-sort after applying penalties
  result.sort((a, b) => b.score - a.score);
  return result;
}

/**
 * Check if query contains unique identifiers that warrant expanded search
 */
function hasUniqueIdentifiers(keywords: string[]): boolean {
  return keywords.some(isUniqueIdentifier);
}

/**
 * Check if query suggests searching all time (not just recent)
 */
function suggestsAllTimeSearch(query: string): boolean {
  return ALL_TIME_PATTERNS.some(pattern => pattern.test(query));
}

/**
 * Apply unique-ID precision boost
 * Ensures chunks containing unique identifiers from the query are prioritized
 */
function applyUniqueIdPrecisionBoost(
  chunks: ScoredChunk[],
  keywords: string[]
): ScoredChunk[] {
  const uniqueIds = keywords.filter(isUniqueIdentifier);
  if (uniqueIds.length === 0) {
    return chunks;
  }

  // Boost chunks that contain unique IDs
  return chunks.map(chunk => {
    const chunkLower = chunk.text.toLowerCase();
    let matchCount = 0;

    for (const uid of uniqueIds) {
      if (chunkLower.includes(uid.toLowerCase())) {
        matchCount++;
      }
    }

    if (matchCount > 0) {
      // Significant boost for unique ID matches
      return {
        ...chunk,
        score: chunk.score * (1 + 0.5 * matchCount),
      };
    }
    return chunk;
  }).sort((a, b) => b.score - a.score);
}

/**
 * Apply coverage-aware reranking
 * Ensures keywords from the query are represented in the final results
 */
function applyCoverageReranking(
  chunks: ScoredChunk[],
  keywords: string[],
  targetCount: number
): ScoredChunk[] {
  if (chunks.length <= targetCount || keywords.length === 0) {
    return chunks.slice(0, targetCount);
  }

  const selected: ScoredChunk[] = [];
  const coveredKeywords = new Set<string>();
  const remaining = [...chunks];

  // First pass: ensure keyword coverage
  for (const keyword of keywords) {
    if (selected.length >= targetCount) break;
    if (coveredKeywords.has(keyword)) continue;

    // Find best chunk that covers this keyword
    const matchingIdx = remaining.findIndex(c =>
      c.text.toLowerCase().includes(keyword.toLowerCase())
    );

    if (matchingIdx >= 0) {
      const chunk = remaining[matchingIdx];
      selected.push(chunk);
      remaining.splice(matchingIdx, 1);

      // Mark all keywords covered by this chunk
      for (const kw of keywords) {
        if (chunk.text.toLowerCase().includes(kw.toLowerCase())) {
          coveredKeywords.add(kw);
        }
      }
    }
  }

  // Second pass: fill with highest scoring remaining
  for (const chunk of remaining) {
    if (selected.length >= targetCount) break;
    selected.push(chunk);
  }

  // Re-sort by score for final ordering
  selected.sort((a, b) => b.score - a.score);
  return selected;
}

/**
 * Detailed retrieval result with candidate counts for observability
 */
export interface RetrievalResult {
  chunks: ScoredChunk[];
  strategy: string;
  candidateCount: number;
  candidateCounts: CandidateCounts;
  timings?: RetrievalTimingsStage;
  scoreDistribution?: {
    topScore: number;
    scoreGap: number;
    uniqueNoteCount: number;
  };
  elapsedMs: number;
}

/**
 * Main retrieval function with multi-stage candidate generation
 *
 * Pipeline stages:
 * 1. Vector candidate generation (PRIMARY at scale via Vertex)
 * 2. Lexical candidate generation (exact-match recall via terms[])
 * 3. Recency candidates (soft support for "recent" intents)
 * 4. Merge and deduplicate
 * 5. Score with normalized features
 * 6. MMR/diversity reranking
 * 7. Final context assembly
 */
export async function retrieveRelevantChunks(
  query: string,
  options: RetrievalOptions
): Promise<RetrievalResult> {
  const startTime = Date.now();
  const timings: RetrievalTimingsStage = {
    queryParseMs: 0,
    embeddingMs: 0,
    vectorSearchMs: 0,
    lexicalSearchMs: 0,
    firestoreFetchMs: 0,
    scoringMs: 0,
    rerankMs: 0,
    totalMs: 0,
  };

  // Initialize candidate counts for observability
  const candidateCounts: CandidateCounts = {
    vectorK: 0,
    lexicalK: 0,
    recencyK: 0,
    mergedK: 0,
    rerankedK: 0,
    finalK: 0,
  };

  // Stage 1: Query analysis
  const parseStart = Date.now();
  const analysis = analyzeQuery(query);
  const keywords = options.keywords ?? analysis.boostTerms ?? analysis.keywords;

  // Extract terms for lexical search (same normalization as indexing)
  const queryTerms = extractTermsForIndexing(query);

  timings.queryParseMs = Date.now() - parseStart;

  // Determine time window - expand for entity queries or all-time hints
  const hasEntities = hasUniqueIdentifiers(keywords);
  const wantsAllTime = suggestsAllTimeSearch(query);
  const expandTimeWindow = hasEntities || wantsAllTime;

  const maxAgeDays = options.maxAgeDays ??
    (expandTimeWindow ? ENTITY_EXPANDED_DAYS :
      (analysis.timeHint?.days ?? RETRIEVAL_DEFAULT_DAYS));

  let strategy = 'multistage';
  if (expandTimeWindow) strategy += '_expanded';

  // Stage 2: Parallel candidate generation
  let vectorChunks: ChunkDoc[] = [];
  let lexicalChunks: ChunkDoc[] = [];
  let recencyChunks: ChunkDoc[] = [];
  let queryEmbedding: number[] | null = null;

  const useVector = VECTOR_SEARCH_ENABLED && isEmbeddingsAvailable();

  // Generate embedding first (needed for both vector search and scoring)
  if (useVector) {
    const embeddingStart = Date.now();
    try {
      queryEmbedding = await generateQueryEmbedding(query);
      timings.embeddingMs = Date.now() - embeddingStart;
    } catch (err) {
      logError('Embedding generation failed', err);
      timings.embeddingMs = Date.now() - embeddingStart;
    }
  }

  // Run vector, lexical, and recency searches in parallel
  // Track each stage independently for accurate timing
  const parallelSearchStart = Date.now();
  const searchPromises: Promise<void>[] = [];

  // Track individual stage timings (set inside async closures)
  let vectorSearchMs = 0;
  let vectorHydrationMs = 0;
  let recencySearchMs = 0;

  // Vector search (primary at scale)
  if (queryEmbedding) {
    searchPromises.push((async () => {
      const vectorStart = Date.now();
      try {
        const vectorIndex = getVectorIndex();
        const vectorTopK = expandTimeWindow ? ENTITY_EXPANDED_LIMIT : RETRIEVAL_VECTOR_TOP_K;
        const vectorResults = await vectorIndex.search(queryEmbedding!, options.tenantId, vectorTopK);

        // Track vector search time (before hydration)
        vectorSearchMs = Date.now() - vectorStart;

        // Batch hydrate chunk docs from Firestore using efficient getAll()
        // Preserves ordering by vector score and caps at BATCH_HYDRATION_MAX
        if (vectorResults.length > 0) {
          const hydrationStart = Date.now();
          const { chunks, cappedAt } = await batchHydrateChunks(vectorResults);
          vectorChunks = chunks;
          vectorHydrationMs = Date.now() - hydrationStart;

          if (cappedAt) {
            strategy += `_hydration_capped(${cappedAt})`;
          }
        }

        candidateCounts.vectorK = vectorChunks.length;
        strategy += `_vector(${vectorIndex.getName()})`;
      } catch (err) {
        vectorSearchMs = Date.now() - vectorStart;
        logError('Vector search failed', err);
      }
    })());
  }

  // Lexical search (for exact-match recall)
  // Optionally expand query for better synonym coverage
  if (queryTerms.length > 0) {
    searchPromises.push((async () => {
      const lexStart = Date.now();

      // Use query expansion if enabled
      let allTerms = queryTerms;
      if (isQueryExpansionAvailable()) {
        try {
          const expandedQueries = await expandQuery(query);
          // Extract terms from all expanded queries
          const expandedTerms = new Set(queryTerms);
          for (const eq of expandedQueries.slice(1)) { // Skip original
            const terms = extractTermsForIndexing(eq);
            terms.forEach(t => expandedTerms.add(t));
          }
          allTerms = Array.from(expandedTerms).slice(0, RETRIEVAL_LEXICAL_MAX_TERMS);
          if (allTerms.length > queryTerms.length) {
            strategy += '_expanded';
          }
        } catch (err) {
          logWarn('Query expansion failed, using original terms', { error: String(err) });
        }
      }

      lexicalChunks = await fetchLexicalCandidates(
        options.tenantId,
        allTerms,
        RETRIEVAL_LEXICAL_TOP_K
      );
      candidateCounts.lexicalK = lexicalChunks.length;
      timings.lexicalSearchMs = Date.now() - lexStart;
      if (lexicalChunks.length > 0) {
        strategy += '_lexical';
      }
    })());
  }

  // Recency search (soft support)
  searchPromises.push((async () => {
    const recencyStart = Date.now();
    recencyChunks = await fetchRecentCandidates(
      options.tenantId,
      RETRIEVAL_RECENCY_TOP_K
    );
    candidateCounts.recencyK = recencyChunks.length;
    recencySearchMs = Date.now() - recencyStart;
  })());

  await Promise.all(searchPromises);

  // Record individual stage timings (parallel execution)
  // Note: These ran in parallel, so total wall time is max(all stages)
  // Record individual stage timings (parallel execution)
  // Note: These ran in parallel, so total wall time is max(all stages)
  timings.vectorSearchMs = vectorSearchMs;
  timings.firestoreFetchMs = vectorHydrationMs;

  // Fallback: if no vector results AND no lexical results, use traditional Firestore fetch
  // This indicates either:
  // 1. Vertex Vector Search is not configured/enabled
  // 2. Vector search returned no results (empty index or query issue)
  // 3. Embeddings are not available
  // 4. Lexical terms didn't match indexed terms
  const isInFallbackMode = vectorChunks.length === 0 && lexicalChunks.length === 0;

  if (isInFallbackMode) {
    const fetchStart = Date.now();
    const candidateLimit = expandTimeWindow ? ENTITY_EXPANDED_LIMIT : Math.max(options.topK * 4, 150);

    // Determine specific fallback reason for debugging
    let fallbackReason: string;
    if (!isEmbeddingsAvailable()) {
      fallbackReason = 'embeddings_unavailable';
    } else if (!queryEmbedding) {
      fallbackReason = 'embedding_generation_failed';
    } else if (!VECTOR_SEARCH_ENABLED) {
      fallbackReason = 'vector_search_disabled';
    } else {
      fallbackReason = 'no_matching_results';
    }

    // Log fallback with detailed diagnostics
    logWarn('Retrieval using Firestore fallback mode', {
      reason: fallbackReason,
      tenantId: options.tenantId,
      query: query.slice(0, 50),
      keywordCount: keywords.length,
      candidateLimit,
      maxAgeDays,
      embeddingsAvailable: isEmbeddingsAvailable(),
      vectorSearchEnabled: VECTOR_SEARCH_ENABLED,
      hint: 'Consider enabling Vertex Vector Search for better recall at scale',
    });

    const fallbackChunks = await fetchCandidates(options.tenantId, maxAgeDays, candidateLimit);
    vectorChunks = fallbackChunks;
    candidateCounts.vectorK = fallbackChunks.length;
    // Fallback fetch replaces hydration timing
    timings.firestoreFetchMs = Date.now() - fetchStart;
    strategy += '_fallback';

    // In fallback mode, apply keyword boosting to improve precision
    // since we don't have vector similarity scores
    if (keywords.length > 0) {
      strategy += '_keyword_boost';
    }
  }

  // Stage 3: Merge candidates
  const { chunks: mergedChunks, sources } = mergeCandidates(vectorChunks, lexicalChunks, recencyChunks);
  candidateCounts.mergedK = mergedChunks.length;

  if (mergedChunks.length === 0) {
    return {
      chunks: [],
      strategy: strategy + '_no_candidates',
      candidateCount: 0,
      candidateCounts,
      timings,
      elapsedMs: Date.now() - startTime,
    };
  }

  // Stage 4: Score all candidates
  const scoringStart = Date.now();

  // Compute vector scores for merged candidates
  let vectorScores = new Map<string, number>();
  if (queryEmbedding) {
    vectorScores = scoreByVector(mergedChunks, queryEmbedding);
  }

  // Compute keyword and recency scores
  const keywordScores = scoreByKeywords(mergedChunks, keywords);
  const recencyScores = scoreByRecency(mergedChunks, maxAgeDays);

  // Combine scores with configurable weights
  const hasVectorSearch = vectorScores.size > 0;
  let scored = combineScoresWeighted(
    mergedChunks,
    vectorScores,
    keywordScores,
    recencyScores,
    sources,
    hasVectorSearch
  );

  timings.scoringMs = Date.now() - scoringStart;

  // Filter out very low quality results
  scored = scored.filter(chunk => chunk.score >= MIN_COMBINED_SCORE);

  // Sort by combined score
  scored.sort((a, b) => b.score - a.score);

  // Stage 5: Reranking
  const rerankStart = Date.now();

  // Apply MMR diversity reranking if enabled
  if (RETRIEVAL_MMR_ENABLED && scored.length > 1) {
    scored = applyMMRReranking(scored, RETRIEVAL_MMR_LAMBDA, options.topK);
    strategy += '_mmr';
  } else if (RERANKING_ENABLED && scored.length > 1) {
    // Fallback to simpler diversity reranking
    scored = applyDiversityReranking(scored, MAX_CHUNKS_PER_NOTE);
    strategy += '_diverse';
  }

  // Apply unique-ID precision boost for queries with identifiers
  if (hasUniqueIdentifiers(keywords) && scored.length > 1) {
    scored = applyUniqueIdPrecisionBoost(scored, keywords);
    strategy += '_uidboost';
  }

  // Apply coverage-aware reranking to ensure keywords are represented
  if (keywords.length > 1 && scored.length > options.rerankTo) {
    scored = applyCoverageReranking(scored, keywords, options.rerankTo);
    strategy += '_coverage';
  }

  // Apply LLM reranking if enabled (optional, behind feature flag)
  if (LLM_RERANK_ENABLED && isLLMRerankerAvailable() && scored.length > 1) {
    try {
      scored = await llmRerank(query, scored, options.rerankTo);
      strategy += '_llm';
    } catch (err) {
      logError('LLM rerank failed, using heuristic order', err);
    }
  }

  // Trim to final count
  if (scored.length > options.rerankTo) {
    scored = scored.slice(0, options.rerankTo);
  }
  candidateCounts.rerankedK = scored.length;
  timings.rerankMs = Date.now() - rerankStart;

  // Stage 6: Context assembly (limit total size)
  let totalChars = 0;
  const limitedChunks: ScoredChunk[] = [];

  for (const chunk of scored) {
    if (totalChars + chunk.text.length > RETRIEVAL_MAX_CONTEXT_CHARS) break;
    limitedChunks.push(chunk);
    totalChars += chunk.text.length;
  }
  candidateCounts.finalK = limitedChunks.length;

  // Compute score distribution for observability
  const scoreDistribution = limitedChunks.length > 0 ? {
    topScore: limitedChunks[0].score,
    scoreGap: limitedChunks.length > 1
      ? limitedChunks[0].score - limitedChunks[1].score
      : 0,
    uniqueNoteCount: new Set(limitedChunks.map(c => c.noteId)).size,
  } : undefined;

  timings.totalMs = Date.now() - startTime;

  logInfo('Multi-stage retrieval complete', {
    query: query.slice(0, 50),
    intent: analysis.intent,
    candidateCounts,
    scoreDistribution,
    strategy,
    hasVectorSearch,
    expandedTimeWindow: expandTimeWindow,
    maxAgeDays,
    timings,
  });

  return {
    chunks: limitedChunks,
    strategy,
    candidateCount: candidateCounts.mergedK,
    candidateCounts,
    timings,
    scoreDistribution,
    elapsedMs: timings.totalMs,
  };
}

/**
 * Combine scores with configurable weights and source boost
 */
function combineScoresWeighted(
  chunks: ChunkDoc[],
  vectorScores: Map<string, number>,
  keywordScores: Map<string, number>,
  recencyScores: Map<string, number>,
  sources: Map<string, Set<'vector' | 'lexical' | 'recency'>>,
  hasVectorSearch: boolean
): ScoredChunk[] {
  // Use configurable weights
  const vectorWeight = hasVectorSearch ? SCORE_WEIGHT_VECTOR : 0;
  const keywordWeight = hasVectorSearch ? SCORE_WEIGHT_LEXICAL : 0.75;
  const recencyWeight = hasVectorSearch ? SCORE_WEIGHT_RECENCY : 0.25;
  const idBoostWeight = SCORE_WEIGHT_ID_BOOST;

  return chunks.map(chunk => {
    const vectorScore = vectorScores.get(chunk.chunkId) || 0;
    const keywordScore = keywordScores.get(chunk.chunkId) || 0;
    const recencyScore = recencyScores.get(chunk.chunkId) || 0;
    const positionBonus = getPositionBonus(chunk.position);

    // Boost chunks found by multiple retrieval stages
    const chunkSources = sources.get(chunk.chunkId) || new Set();
    const multiSourceBoost = chunkSources.size > 1 ? 0.1 * (chunkSources.size - 1) : 0;

    // Combine weighted scores
    const combinedScore =
      vectorWeight * vectorScore +
      keywordWeight * keywordScore +
      recencyWeight * recencyScore +
      positionBonus +
      multiSourceBoost;

    const createdAt = chunk.createdAt instanceof Timestamp
      ? chunk.createdAt.toDate()
      : new Date();

    return {
      chunkId: chunk.chunkId,
      noteId: chunk.noteId,
      tenantId: chunk.tenantId,
      text: chunk.text,
      position: chunk.position,
      createdAt,
      score: combinedScore,
      vectorScore,
      keywordScore,
      recencyScore,
    };
  });
}



================================================================================
FILE: src/retrievalLogger.ts
LINES: 248
PATH: /Users/salscrudato/Projects/auroranotes-api/src/retrievalLogger.ts
================================================================================

/**
 * AuroraNotes API - Retrieval and Citation Logger
 *
 * Provides structured observability for the retrieval pipeline:
 * - Request/response tracing with latency breakdown
 * - Multi-stage candidate counts (vector, lexical, recency)
 * - Per-citation metadata (score, noteId, chunkId)
 * - Quality flags (citation coverage, validation results)
 * - Score distribution summaries for debugging
 * - BigQuery-compatible structured logging
 */

import { v4 as uuid } from 'uuid';
import { logInfo, logWarn } from './utils';
import { CandidateCounts, RetrievalTimingsStage, ScoredChunk } from './types';

export interface RetrievalTimings {
  queryParseMs?: number;
  embeddingMs?: number;
  vectorSearchMs?: number;
  lexicalSearchMs?: number;  // Renamed from keywordSearchMs
  firestoreFetchMs?: number; // Fallback fetch time
  rerankMs?: number;
  contextAssemblyMs?: number;
  generationMs?: number;
  validationMs?: number;
  repairMs?: number;      // Time spent on citation repair
  retrievalMs?: number;   // Total retrieval time
  totalMs: number;
}

export interface CitationLogEntry {
  cid: string;
  noteId: string;
  chunkId: string;
  score: number;
  vectorScore?: number;
  keywordScore?: number;
  recencyScore?: number;
  overlapScore?: number;  // Citation verification overlap score
  snippetLength: number;
}

export interface QualityFlags {
  citationCoveragePct: number;
  invalidCitationsRemoved: number;
  fallbackUsed: boolean;
  insufficientEvidence: boolean;
  regenerationAttempted: boolean;
  diversityScore?: number;
  queryExpanded?: boolean;  // Whether query expansion was used
  mmrApplied?: boolean;     // Whether MMR reranking was applied
}

/**
 * Score distribution summary for debugging retrieval quality
 */
export interface ScoreDistribution {
  topScore: number;
  medianScore: number;
  minScore: number;
  scoreGap: number;        // Gap between top and second score
  uniqueNoteCount: number;
  scoreStdDev: number;
}

export interface RetrievalLogEntry {
  requestId: string;
  traceId: string;
  tenantId: string;
  query: string;
  queryLength: number;
  intent: string;
  retrievalMode: 'vector' | 'hybrid' | 'keyword_only' | 'fallback';
  candidateCounts: {
    vectorK: number;
    keywordK: number;  // Kept for backward compatibility (maps to lexicalK)
    mergedK: number;
    afterRerank: number;
    finalChunks: number;
  };
  stageDetails?: {
    vectorK: number;
    lexicalK: number;
    recencyK: number;
    mergedK: number;
    rerankedK: number;
    finalK: number;
  };
  scoreDistribution?: ScoreDistribution;
  rerankMethod: string;
  citations: CitationLogEntry[];
  timings: RetrievalTimings;
  quality: QualityFlags;
  answerLength: number;
  timestamp: string;
}

/**
 * Creates a new retrieval log entry with a fresh request ID
 */
export function createRetrievalLog(
  tenantId: string,
  query: string,
  requestId?: string
): Partial<RetrievalLogEntry> {
  return {
    requestId: requestId || `req_${uuid().slice(0, 8)}`,
    traceId: uuid(),
    tenantId,
    query: query.slice(0, 500), // Truncate for logging
    queryLength: query.length,
    timestamp: new Date().toISOString(),
    candidateCounts: {
      vectorK: 0,
      keywordK: 0,
      mergedK: 0,
      afterRerank: 0,
      finalChunks: 0,
    },
    citations: [],
    quality: {
      citationCoveragePct: 0,
      invalidCitationsRemoved: 0,
      fallbackUsed: false,
      insufficientEvidence: false,
      regenerationAttempted: false,
    },
    timings: {
      totalMs: 0,
    },
  };
}

/**
 * Logs a complete retrieval/citation entry
 */
export function logRetrieval(entry: RetrievalLogEntry): void {
  // Structured log for Cloud Logging / BigQuery export
  logInfo('Retrieval/citation trace', {
    requestId: entry.requestId,
    traceId: entry.traceId,
    tenantId: entry.tenantId,
    queryLength: entry.queryLength,
    intent: entry.intent,
    retrievalMode: entry.retrievalMode,
    candidateCounts: entry.candidateCounts,
    stageDetails: entry.stageDetails,
    scoreDistribution: entry.scoreDistribution,
    rerankMethod: entry.rerankMethod,
    citationCount: entry.citations.length,
    // Per-citation summary (not full snippets)
    citationSummary: entry.citations.map(c => ({
      cid: c.cid,
      noteId: c.noteId.slice(0, 8),
      score: Math.round(c.score * 1000) / 1000,
      vectorScore: c.vectorScore ? Math.round(c.vectorScore * 1000) / 1000 : undefined,
    })),
    timings: entry.timings,
    quality: entry.quality,
    answerLength: entry.answerLength,
  });

  // Warn on potential quality issues
  if (entry.quality.citationCoveragePct < 50 && entry.citations.length > 0) {
    logWarn('Low citation coverage in response', {
      requestId: entry.requestId,
      coverage: entry.quality.citationCoveragePct,
      citationCount: entry.citations.length,
    });
  }

  if (entry.scoreDistribution && entry.scoreDistribution.scoreGap > 0.3) {
    logWarn('Large score gap detected (potential single-source dominance)', {
      requestId: entry.requestId,
      topScore: entry.scoreDistribution.topScore,
      scoreGap: entry.scoreDistribution.scoreGap,
    });
  }
}

/**
 * Calculate citation coverage percentage
 * Counts sentences with at least one citation vs total sentences
 */
export function calculateCitationCoverage(answer: string): number {
  const sentences = answer
    .split(/[.!?]+/)
    .map(s => s.trim())
    .filter(s => s.length > 10); // Only count substantial sentences

  if (sentences.length === 0) return 100;

  const citedSentences = sentences.filter(s => /\[N\d+\]/.test(s));
  return Math.round((citedSentences.length / sentences.length) * 100);
}

/**
 * Parse citation IDs from answer text
 */
export function parseCitationIds(answer: string): string[] {
  const matches = answer.match(/\[N\d+\]/g) || [];
  return [...new Set(matches.map(m => m.slice(1, -1)))]; // Remove brackets, dedupe
}

/**
 * Compute score distribution summary from scored chunks
 */
export function computeScoreDistribution(chunks: ScoredChunk[]): ScoreDistribution | undefined {
  if (chunks.length === 0) return undefined;

  const scores = chunks.map(c => c.score).sort((a, b) => b - a);
  const topScore = scores[0];
  const minScore = scores[scores.length - 1];
  const medianScore = scores[Math.floor(scores.length / 2)];
  const scoreGap = scores.length > 1 ? scores[0] - scores[1] : 0;
  const uniqueNoteCount = new Set(chunks.map(c => c.noteId)).size;

  // Calculate standard deviation
  const mean = scores.reduce((a, b) => a + b, 0) / scores.length;
  const variance = scores.reduce((sum, s) => sum + Math.pow(s - mean, 2), 0) / scores.length;
  const scoreStdDev = Math.sqrt(variance);

  return {
    topScore: Math.round(topScore * 1000) / 1000,
    medianScore: Math.round(medianScore * 1000) / 1000,
    minScore: Math.round(minScore * 1000) / 1000,
    scoreGap: Math.round(scoreGap * 1000) / 1000,
    uniqueNoteCount,
    scoreStdDev: Math.round(scoreStdDev * 1000) / 1000,
  };
}

/**
 * Convert CandidateCounts to stage details for logging
 */
export function candidateCountsToStageDetails(counts: CandidateCounts): RetrievalLogEntry['stageDetails'] {
  return {
    vectorK: counts.vectorK,
    lexicalK: counts.lexicalK,
    recencyK: counts.recencyK,
    mergedK: counts.mergedK,
    rerankedK: counts.rerankedK,
    finalK: counts.finalK,
  };
}



================================================================================
FILE: src/types.ts
LINES: 228
PATH: /Users/salscrudato/Projects/auroranotes-api/src/types.ts
================================================================================

/**
 * AuroraNotes API - Shared Types
 */

import { FieldValue, Timestamp } from "firebase-admin/firestore";

// ============================================
// Note Types
// ============================================

/** Note document in Firestore */
export interface NoteDoc {
  id: string;
  text: string;
  tenantId: string;
  createdAt: Timestamp | FieldValue;
  updatedAt: Timestamp | FieldValue;
}

/** Note as returned from API (ISO strings) */
export interface NoteResponse {
  id: string;
  text: string;
  tenantId: string;
  createdAt: string;
  updatedAt: string;
}

/** Paginated notes response */
export interface NotesListResponse {
  notes: NoteResponse[];
  cursor: string | null;
  hasMore: boolean;
}

// ============================================
// Chunk Types
// ============================================

/** Chunk document in Firestore */
export interface ChunkDoc {
  chunkId: string;
  noteId: string;
  tenantId: string;
  text: string;
  textHash: string;
  position: number;
  tokenEstimate: number;
  createdAt: Timestamp | FieldValue;
  embedding?: number[];
  embeddingModel?: string;
  // Lexical indexing fields (for exact-match recall)
  terms?: string[];          // Normalized tokens for array-contains-any queries
  termsVersion?: number;     // Version of term extraction algorithm (for backfill)
}

/** Chunk with score for retrieval */
export interface ScoredChunk {
  chunkId: string;
  noteId: string;
  tenantId: string;
  text: string;
  position: number;
  createdAt: Date;
  score: number;
  vectorScore?: number;
  keywordScore?: number;
  recencyScore?: number;
}

// ============================================
// Query Intent Types (defined early for use in Chat types)
// ============================================

/** Query intent types */
export type QueryIntent =
  | 'summarize'      // User wants a summary of their notes
  | 'list'           // User wants a list of items
  | 'decision'       // User is asking about decisions made
  | 'action_item'    // User is looking for action items/todos
  | 'search'         // General search/lookup
  | 'question';      // Direct question

// ============================================
// Chat Types
// ============================================

/** Citation in chat response */
export interface Citation {
  cid: string;          // e.g., "N12"
  noteId: string;
  chunkId: string;
  createdAt: string;    // ISO string
  snippet: string;
  score: number;
}

/** Chat request body */
export interface ChatRequest {
  message: string;
  tenantId?: string;
}

/** Chat response */
export interface ChatResponse {
  answer: string;
  citations: Citation[];
  meta: {
    model: string;
    retrieval: {
      k: number;
      strategy: string;
      candidateCount?: number;
      rerankCount?: number;
      intent?: QueryIntent;
      timeMs?: number;
    };
  };
}

// ============================================
// Retrieval Types
// ============================================

/** Query analysis result */
export interface QueryAnalysis {
  originalQuery: string;
  normalizedQuery: string;
  keywords: string[];
  intent: QueryIntent;
  timeHint?: {
    days?: number;
    after?: Date;
    before?: Date;
  };
  entities?: string[];        // Extracted named entities (names, projects, etc.)
  boostTerms?: string[];      // Terms to boost in scoring
}

/** Retrieval options */
export interface RetrievalOptions {
  tenantId: string;
  topK: number;
  rerankTo: number;
  maxAgeDays?: number;
  keywords?: string[];
  useVectorSearch?: boolean;
  // Multi-stage retrieval options
  useQueryExpansion?: boolean;  // Enable multi-query expansion
  requestId?: string;           // For logging correlation
}

// ============================================
// Multi-Stage Retrieval Types
// ============================================

/** Candidate counts by retrieval stage for observability */
export interface CandidateCounts {
  vectorK: number;
  lexicalK: number;
  recencyK: number;
  mergedK: number;
  rerankedK: number;
  finalK: number;
}

/** Timings by retrieval stage for observability */
export interface RetrievalTimingsStage {
  queryParseMs: number;
  embeddingMs: number;
  vectorSearchMs: number;
  lexicalSearchMs: number;
  firestoreFetchMs: number;
  scoringMs: number;
  rerankMs: number;
  totalMs: number;
}

/** Distribution summaries for quality analysis */
export interface ScoreDistribution {
  topScore: number;
  scoreGap: number;        // Gap between top and second score
  uniqueNoteCount: number;
  avgScore: number;
  minScore: number;
}

/** Citation metrics for observability */
export interface CitationMetrics {
  citationCoveragePct: number;
  invalidCitationCount: number;
  droppedCitationCount: number;
  repaired: boolean;
  entailmentCheckRun: boolean;
}

// ============================================
// Sources Pack - Single source of truth for sources and citations
// ============================================

/**
 * SourcesPack represents the exact set of sources used in a chat response.
 * This object flows through the entire pipeline ensuring:
 * - sources: The exact chunks used as sources in the LLM prompt
 * - citationsMap: 1:1 mapping of cid (N1, N2, ...) to Citation
 * - The prompt source count matches citationsMap.size EXACTLY
 */
export interface SourcesPack {
  /** The exact chunks used as sources - post-filtering, post-reranking */
  sources: ScoredChunk[];
  /** 1:1 mapping from cid (e.g., "N1") to Citation object */
  citationsMap: Map<string, Citation>;
  /** Number of sources = citationsMap.size = prompt source count */
  sourceCount: number;
}

// ============================================
// Health Types
// ============================================

export interface HealthResponse {
  ok: boolean;
  service: string;
  project: string;
  version?: string;
}



================================================================================
FILE: src/utils.ts
LINES: 328
PATH: /Users/salscrudato/Projects/auroranotes-api/src/utils.ts
================================================================================

/**
 * AuroraNotes API - Utility Functions
 *
 * Common utilities for logging, validation, text processing, and security.
 */

import { Timestamp } from "firebase-admin/firestore";
import * as crypto from "crypto";

// ============================================
// Input Sanitization
// ============================================

/**
 * Sanitize user input text (remove control characters, limit length)
 */
export function sanitizeText(text: string, maxLength: number = 10000): string {
  if (!text || typeof text !== 'string') return '';

  return text
    // Remove null bytes and other control characters (except newlines/tabs)
    .replace(/[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]/g, '')
    // Normalize unicode
    .normalize('NFC')
    // Trim whitespace
    .trim()
    // Limit length
    .slice(0, maxLength);
}

/**
 * Sanitize query string for safe logging
 */
export function sanitizeForLogging(text: string, maxLength: number = 100): string {
  return sanitizeText(text, maxLength)
    .replace(/[\n\r]/g, ' ')
    .replace(/\s+/g, ' ');
}

/**
 * Validate tenant ID format
 */
export function isValidTenantId(tenantId: string): boolean {
  if (!tenantId || typeof tenantId !== 'string') return false;
  // Allow alphanumeric, hyphens, underscores, max 64 chars
  return /^[a-zA-Z0-9_-]{1,64}$/.test(tenantId);
}

/**
 * Convert Firestore Timestamp to ISO string
 */
export function timestampToISO(ts: Timestamp | Date | unknown): string {
  if (ts instanceof Timestamp) {
    return ts.toDate().toISOString();
  }
  if (ts instanceof Date) {
    return ts.toISOString();
  }
  // Handle serialized timestamp
  if (ts && typeof ts === 'object' && '_seconds' in ts) {
    const obj = ts as { _seconds: number; _nanoseconds?: number };
    return new Date(obj._seconds * 1000).toISOString();
  }
  return new Date().toISOString();
}

/**
 * Create a hash of text for deduplication
 */
export function hashText(text: string): string {
  return crypto.createHash('sha256').update(text).digest('hex').slice(0, 16);
}

/**
 * Estimate token count (rough approximation: ~4 chars per token)
 */
export function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4);
}

/**
 * Parse cursor for pagination (base64 encoded)
 */
export function parseCursor(cursor: string | undefined): { createdAt: Date; id: string } | null {
  if (!cursor) return null;
  try {
    const decoded = Buffer.from(cursor, 'base64').toString('utf8');
    const [timestamp, id] = decoded.split('|');
    const createdAt = new Date(timestamp);
    if (isNaN(createdAt.getTime()) || !id) return null;
    return { createdAt, id };
  } catch {
    return null;
  }
}

/**
 * Encode cursor for pagination
 */
export function encodeCursor(createdAt: Date | Timestamp, id: string): string {
  const date = createdAt instanceof Timestamp ? createdAt.toDate() : createdAt;
  return Buffer.from(`${date.toISOString()}|${id}`).toString('base64');
}

// ============================================
// Request Context (for request ID correlation)
// ============================================

// Using AsyncLocalStorage for request-scoped context
import { AsyncLocalStorage } from 'async_hooks';

interface RequestContext {
  requestId: string;
  startTime: number;
  path?: string;
}

const requestContextStorage = new AsyncLocalStorage<RequestContext>();

/**
 * Generate a unique request ID
 */
export function generateRequestId(): string {
  return `req_${Date.now().toString(36)}_${crypto.randomBytes(4).toString('hex')}`;
}

/**
 * Run a function with request context
 */
export function withRequestContext<T>(context: RequestContext, fn: () => T): T {
  return requestContextStorage.run(context, fn);
}

/**
 * Get current request context
 */
export function getRequestContext(): RequestContext | undefined {
  return requestContextStorage.getStore();
}

/**
 * Structured log helper (for Cloud Logging)
 */
export function logInfo(message: string, data?: Record<string, unknown>): void {
  const ctx = getRequestContext();
  console.log(JSON.stringify({
    severity: 'INFO',
    message,
    requestId: ctx?.requestId,
    ...data,
    timestamp: new Date().toISOString(),
  }));
}

export function logWarn(message: string, data?: Record<string, unknown>): void {
  const ctx = getRequestContext();
  console.log(JSON.stringify({
    severity: 'WARNING',
    message,
    requestId: ctx?.requestId,
    ...data,
    timestamp: new Date().toISOString(),
  }));
}

export function logError(message: string, error?: unknown, data?: Record<string, unknown>): void {
  const ctx = getRequestContext();
  const errorInfo = error instanceof Error
    ? { errorMessage: error.message, errorStack: error.stack }
    : { errorMessage: String(error) };

  console.error(JSON.stringify({
    severity: 'ERROR',
    message,
    requestId: ctx?.requestId,
    ...errorInfo,
    ...data,
    timestamp: new Date().toISOString(),
  }));
}

/**
 * Extract keywords from query (simple implementation)
 */
export function extractKeywords(query: string): string[] {
  const stopWords = new Set([
    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
    'may', 'might', 'must', 'shall', 'can', 'to', 'of', 'in', 'for', 'on', 'with',
    'at', 'by', 'from', 'as', 'into', 'through', 'during', 'before', 'after',
    'above', 'below', 'between', 'under', 'again', 'further', 'then', 'once',
    'here', 'there', 'when', 'where', 'why', 'how', 'all', 'each', 'few', 'more',
    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',
    'so', 'than', 'too', 'very', 'just', 'and', 'but', 'if', 'or', 'because',
    'until', 'while', 'about', 'what', 'which', 'who', 'whom', 'this', 'that',
    'these', 'those', 'am', 'it', 'its', 'my', 'your', 'his', 'her', 'their', 'our',
    'me', 'you', 'him', 'us', 'them', 'i', 'we', 'they', 'he', 'she',
    'include', 'including', 'tell', 'everything', 'complete', 'give', 'show'
  ]);

  // First, extract unique identifiers (uppercase with underscores/numbers) - these get priority
  const uniqueIdPattern = /\b([A-Z][A-Z0-9_]{2,})\b/g;
  const uniqueIds: string[] = [];
  let match;
  while ((match = uniqueIdPattern.exec(query)) !== null) {
    uniqueIds.push(match[1].toLowerCase());
  }

  // Extract regular keywords
  const regularKeywords = query
    .toLowerCase()
    .replace(/[^\w\s]/g, ' ')
    .split(/\s+/)
    .filter(word => word.length > 2 && !stopWords.has(word));

  // Combine: unique IDs first (they're more specific), then regular keywords
  const combined = [...new Set([...uniqueIds, ...regularKeywords])];
  return combined.slice(0, 15); // Allow more keywords for better recall
}

/**
 * Cosine similarity between two vectors
 */
export function cosineSimilarity(a: number[], b: number[]): number {
  if (a.length !== b.length) return 0;

  let dotProduct = 0;
  let normA = 0;
  let normB = 0;

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }

  const denominator = Math.sqrt(normA) * Math.sqrt(normB);
  return denominator === 0 ? 0 : dotProduct / denominator;
}

// ============================================
// Term Extraction for Lexical Indexing
// ============================================

// Current version of term extraction algorithm (increment when algorithm changes for backfill)
export const TERMS_VERSION = 1;

// Stop words for term extraction (more comprehensive for indexing)
const TERM_STOP_WORDS = new Set([
  'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
  'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
  'may', 'might', 'must', 'shall', 'can', 'to', 'of', 'in', 'for', 'on', 'with',
  'at', 'by', 'from', 'as', 'into', 'through', 'during', 'before', 'after',
  'above', 'below', 'between', 'under', 'again', 'further', 'then', 'once',
  'here', 'there', 'when', 'where', 'why', 'how', 'all', 'each', 'few', 'more',
  'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',
  'so', 'than', 'too', 'very', 'just', 'and', 'but', 'if', 'or', 'because',
  'until', 'while', 'about', 'what', 'which', 'who', 'whom', 'this', 'that',
  'these', 'those', 'am', 'it', 'its', 'my', 'your', 'his', 'her', 'their', 'our',
  'me', 'you', 'him', 'us', 'them', 'i', 'we', 'they', 'he', 'she',
]);

/**
 * Extract normalized terms from text for lexical indexing.
 * Returns unique, lowercase tokens suitable for Firestore array-contains-any queries.
 *
 * Includes:
 * - Regular words (normalized, lowercased, stemmed minimally)
 * - Unique identifiers (preserved with underscores/numbers)
 * - Numbers (preserved for ID matching)
 *
 * Max 50 terms per chunk to stay within Firestore limits.
 */
export function extractTermsForIndexing(text: string): string[] {
  const terms = new Set<string>();

  // Extract unique identifiers first (e.g., CITE_TEST_002, PROJECT_ALPHA)
  // These are high-value for exact matching
  const uniqueIdPattern = /\b([A-Z][A-Z0-9_]{2,})\b/g;
  let match;
  while ((match = uniqueIdPattern.exec(text)) !== null) {
    terms.add(match[1].toLowerCase());
  }

  // Extract regular terms
  const normalizedText = text
    .toLowerCase()
    .replace(/[^\w\s-]/g, ' ')  // Keep hyphens for compound words
    .replace(/\s+/g, ' ');

  const tokens = normalizedText.split(/\s+/);

  for (const token of tokens) {
    // Skip short terms and stop words
    if (token.length < 2) continue;
    if (TERM_STOP_WORDS.has(token)) continue;

    // Add the term
    terms.add(token);

    // For hyphenated terms, also add components
    if (token.includes('-')) {
      const parts = token.split('-');
      for (const part of parts) {
        if (part.length >= 2 && !TERM_STOP_WORDS.has(part)) {
          terms.add(part);
        }
      }
    }
  }

  // Convert to array and limit to 50 terms (Firestore array limit considerations)
  const termArray = Array.from(terms).slice(0, 50);

  return termArray;
}

/**
 * Check if a term looks like a unique identifier
 */
export function isUniqueIdentifier(term: string): boolean {
  // Match patterns like CITE_TEST_002, PROJECT_ALPHA, TEST123
  return /^[a-z][a-z0-9_]*[0-9_][a-z0-9_]*$/i.test(term) ||
         /^[a-z]+_[a-z0-9_]+$/i.test(term) ||
         /^[A-Z][A-Z0-9_]{2,}$/.test(term);
}



================================================================================
FILE: src/vectorIndex.ts
LINES: 717
PATH: /Users/salscrudato/Projects/auroranotes-api/src/vectorIndex.ts
================================================================================

/**
 * AuroraNotes API - Vector Index Abstraction
 *
 * Provides a unified interface for vector search operations.
 * Supports multiple implementations:
 *   - FirestoreApproxVectorIndex: In-memory cosine similarity over Firestore docs
 *   - VertexVectorSearchIndex: Optional Vertex AI Vector Search (behind VERTEX_VECTOR_SEARCH env)
 *
 * Includes scale guards to warn when Firestore fallback is used with large datasets.
 */

import { Timestamp } from "firebase-admin/firestore";
import { getDb } from "./firestore";
import { ChunkDoc } from "./types";
import { cosineSimilarity, logInfo, logError, logWarn } from "./utils";
import {
  CHUNKS_COLLECTION,
  FIRESTORE_FALLBACK_WARN_THRESHOLD,
  FIRESTORE_FALLBACK_MAX_SCAN,
  PROJECT_ID,
  VERTEX_VECTOR_SEARCH_REGION,
  VERTEX_INDEX_ENDPOINT_RESOURCE,
  VERTEX_INDEX_ENDPOINT_ID,
  VERTEX_VECTOR_SEARCH_ENDPOINT,
  VERTEX_VECTOR_SEARCH_INDEX_ID,
  VERTEX_DEPLOYED_INDEX_ID,
  VERTEX_DISTANCE_METRIC,
} from "./config";

/**
 * Result from vector search
 */
export interface VectorSearchResult {
  chunkId: string;
  noteId: string;
  score: number;
}

/**
 * Vector index interface
 */
export interface VectorIndex {
  /**
   * Search for similar chunks by query embedding
   * @param queryEmbedding The query embedding vector
   * @param tenantId Tenant to search within
   * @param topK Number of results to return
   * @returns Array of chunk IDs with scores
   */
  search(
    queryEmbedding: number[],
    tenantId: string,
    topK: number
  ): Promise<VectorSearchResult[]>;

  /**
   * Get the implementation name for logging
   */
  getName(): string;
}

/**
 * Firestore-based approximate vector search
 *
 * Fetches chunks with embeddings and computes cosine similarity in-memory.
 * Suitable for small-medium datasets (<5k chunks per tenant).
 *
 * IMPORTANT: This is a FALLBACK for development/small datasets.
 * For production at scale (100k+ notes), use Vertex Vector Search.
 *
 * Scale guards:
 * - Warns if corpus size exceeds FIRESTORE_FALLBACK_WARN_THRESHOLD
 * - Expands scan to FIRESTORE_FALLBACK_MAX_SCAN to avoid silently missing older notes
 */
export class FirestoreApproxVectorIndex implements VectorIndex {
  private maxCandidates: number;
  private warnedForTenant: Set<string> = new Set();

  constructor(maxCandidates: number = 500) {
    // Use the higher of provided limit or config-based max scan
    this.maxCandidates = Math.max(maxCandidates, FIRESTORE_FALLBACK_MAX_SCAN);
  }

  getName(): string {
    return 'firestore_approx';
  }

  /**
   * Get the total chunk count for a tenant (for scale guard warnings)
   */
  private async getTenantChunkCount(tenantId: string): Promise<number> {
    const db = getDb();
    try {
      // Use a count aggregation if available, otherwise estimate from limit
      const countSnap = await db
        .collection(CHUNKS_COLLECTION)
        .where('tenantId', '==', tenantId)
        .count()
        .get();
      return countSnap.data().count;
    } catch {
      // Count aggregation not available, return -1 to indicate unknown
      return -1;
    }
  }

  async search(
    queryEmbedding: number[],
    tenantId: string,
    topK: number
  ): Promise<VectorSearchResult[]> {
    const db = getDb();
    const startTime = Date.now();

    // Scale guard: Check corpus size and warn if large
    if (!this.warnedForTenant.has(tenantId)) {
      const corpusSize = await this.getTenantChunkCount(tenantId);
      if (corpusSize > FIRESTORE_FALLBACK_WARN_THRESHOLD) {
        logWarn('Firestore vector search fallback used with large corpus', {
          tenantId,
          corpusSize,
          threshold: FIRESTORE_FALLBACK_WARN_THRESHOLD,
          recommendation: 'Enable Vertex Vector Search (VERTEX_VECTOR_SEARCH_ENABLED=true) for production scale',
        });
        this.warnedForTenant.add(tenantId);
      }
    }

    // Fetch chunks that have embeddings - scan up to maxCandidates
    // This ensures we don't silently miss older relevant notes
    const snap = await db
      .collection(CHUNKS_COLLECTION)
      .where('tenantId', '==', tenantId)
      .orderBy('createdAt', 'desc')
      .limit(this.maxCandidates)
      .get();

    const results: VectorSearchResult[] = [];

    for (const doc of snap.docs) {
      const chunk = doc.data() as ChunkDoc;
      if (chunk.embedding && chunk.embedding.length > 0) {
        const score = cosineSimilarity(queryEmbedding, chunk.embedding);
        results.push({
          chunkId: chunk.chunkId,
          noteId: chunk.noteId,
          score,
        });
      }
    }

    // Sort by score descending
    results.sort((a, b) => b.score - a.score);

    logInfo('Firestore vector search complete', {
      tenantId,
      candidatesScanned: snap.docs.length,
      chunksWithEmbeddings: results.length,
      maxCandidatesConfig: this.maxCandidates,
      topK,
      elapsedMs: Date.now() - startTime,
    });

    return results.slice(0, topK);
  }
}

// Static flag to track if misconfiguration warning has been logged
let vertexMisconfigWarningLogged = false;

/**
 * Parsed Vertex configuration with validated fields
 */
interface VertexConfig {
  projectId: string;
  region: string;
  indexEndpointResource: string;  // Full resource name: projects/X/locations/Y/indexEndpoints/Z
  deployedIndexId: string;
  indexId: string;                // For upsert/remove operations
  distanceMetric: 'COSINE' | 'DOT_PRODUCT' | 'SQUARED_L2';
  findNeighborsUrl: string;       // Precomputed URL for search
  upsertUrl: string;              // Precomputed URL for upsert
  removeUrl: string;              // Precomputed URL for remove
  isValid: boolean;
  validationErrors: string[];
}

/**
 * Parse and validate Vertex Vector Search configuration from environment.
 * Produces the correct findNeighbors URL using the standard Vertex AI API format.
 *
 * ENV CONTRACT:
 * - GOOGLE_CLOUD_PROJECT or GCLOUD_PROJECT: GCP project ID (required)
 * - VERTEX_VECTOR_SEARCH_REGION: Region (default: us-central1)
 * - VERTEX_INDEX_ENDPOINT_RESOURCE: Full resource name (preferred)
 *   OR VERTEX_INDEX_ENDPOINT_ID: Just the endpoint ID (will be combined with project/region)
 * - VERTEX_DEPLOYED_INDEX_ID: ID of the deployed index (required)
 * - VERTEX_VECTOR_SEARCH_INDEX_ID: Index ID for upsert/remove (optional for search)
 * - VERTEX_DISTANCE_METRIC: COSINE | DOT_PRODUCT | SQUARED_L2 (default: COSINE)
 */
function parseVertexConfig(): VertexConfig {
  const errors: string[] = [];

  // Project ID: use config's PROJECT_ID which already handles GOOGLE_CLOUD_PROJECT/GCLOUD_PROJECT
  const projectId = PROJECT_ID;
  if (!projectId || projectId === 'local') {
    errors.push('GOOGLE_CLOUD_PROJECT or GCLOUD_PROJECT must be set');
  }

  const region = VERTEX_VECTOR_SEARCH_REGION;
  const deployedIndexId = VERTEX_DEPLOYED_INDEX_ID;
  if (!deployedIndexId) {
    errors.push('VERTEX_DEPLOYED_INDEX_ID is required');
  }

  // Parse index endpoint - prefer full resource name
  let indexEndpointResource = '';

  if (VERTEX_INDEX_ENDPOINT_RESOURCE) {
    // Preferred: full resource name provided directly
    // Expected format: projects/{project}/locations/{region}/indexEndpoints/{endpoint_id}
    indexEndpointResource = VERTEX_INDEX_ENDPOINT_RESOURCE;

    // Validate format
    const resourcePattern = /^projects\/[^/]+\/locations\/[^/]+\/indexEndpoints\/[^/]+$/;
    if (!resourcePattern.test(indexEndpointResource)) {
      errors.push(`VERTEX_INDEX_ENDPOINT_RESOURCE has invalid format. Expected: projects/{project}/locations/{region}/indexEndpoints/{endpoint_id}. Got: ${indexEndpointResource}`);
    }
  } else if (VERTEX_INDEX_ENDPOINT_ID) {
    // Fallback: construct from endpoint ID + project + region
    if (projectId && projectId !== 'local') {
      indexEndpointResource = `projects/${projectId}/locations/${region}/indexEndpoints/${VERTEX_INDEX_ENDPOINT_ID}`;
    } else {
      errors.push('Cannot construct endpoint resource: project ID not available');
    }
  } else if (VERTEX_VECTOR_SEARCH_ENDPOINT) {
    // Legacy support: try to parse the old VERTEX_VECTOR_SEARCH_ENDPOINT
    // This could be either a public domain or a resource name
    const legacyEndpoint = VERTEX_VECTOR_SEARCH_ENDPOINT;

    if (legacyEndpoint.includes('projects/')) {
      // It looks like a resource name
      indexEndpointResource = legacyEndpoint;
    } else if (legacyEndpoint.includes('.')) {
      // It looks like a domain - extract endpoint ID and construct resource
      // Format: {endpoint_id}.{region}-aiplatform.googleapis.com
      const match = legacyEndpoint.match(/^(\d+)\./);
      if (match && projectId && projectId !== 'local') {
        indexEndpointResource = `projects/${projectId}/locations/${region}/indexEndpoints/${match[1]}`;
      } else {
        errors.push(`Cannot parse legacy VERTEX_VECTOR_SEARCH_ENDPOINT: ${legacyEndpoint}. Use VERTEX_INDEX_ENDPOINT_RESOURCE instead.`);
      }
    } else {
      // Assume it's just an endpoint ID
      if (projectId && projectId !== 'local') {
        indexEndpointResource = `projects/${projectId}/locations/${region}/indexEndpoints/${legacyEndpoint}`;
      } else {
        errors.push('Cannot construct endpoint resource from legacy endpoint: project ID not available');
      }
    }
  } else {
    errors.push('One of VERTEX_INDEX_ENDPOINT_RESOURCE, VERTEX_INDEX_ENDPOINT_ID, or VERTEX_VECTOR_SEARCH_ENDPOINT is required');
  }

  // Index ID for upsert/remove (optional for search-only)
  const indexId = VERTEX_VECTOR_SEARCH_INDEX_ID;

  // Distance metric
  const distanceMetric = VERTEX_DISTANCE_METRIC;

  // Construct URLs
  // findNeighbors URL: https://{region}-aiplatform.googleapis.com/v1/{indexEndpointResource}:findNeighbors
  const findNeighborsUrl = indexEndpointResource
    ? `https://${region}-aiplatform.googleapis.com/v1/${indexEndpointResource}:findNeighbors`
    : '';

  // upsert/remove URLs use the index resource (different from endpoint)
  const indexResource = indexId && projectId && projectId !== 'local'
    ? `projects/${projectId}/locations/${region}/indexes/${indexId}`
    : '';
  const upsertUrl = indexResource
    ? `https://${region}-aiplatform.googleapis.com/v1/${indexResource}:upsertDatapoints`
    : '';
  const removeUrl = indexResource
    ? `https://${region}-aiplatform.googleapis.com/v1/${indexResource}:removeDatapoints`
    : '';

  return {
    projectId,
    region,
    indexEndpointResource,
    deployedIndexId,
    indexId,
    distanceMetric,
    findNeighborsUrl,
    upsertUrl,
    removeUrl,
    isValid: errors.length === 0,
    validationErrors: errors,
  };
}

/**
 * Vertex AI Vector Search implementation (optional)
 *
 * Uses Google Cloud Vertex AI Vector Search for scalable nearest neighbor search.
 *
 * ENV CONTRACT:
 * - GOOGLE_CLOUD_PROJECT: GCP project (consistent with rest of codebase)
 * - VERTEX_VECTOR_SEARCH_REGION: Region (default: us-central1)
 * - VERTEX_INDEX_ENDPOINT_RESOURCE: Full resource name (preferred)
 *   OR VERTEX_INDEX_ENDPOINT_ID: Just endpoint ID
 * - VERTEX_DEPLOYED_INDEX_ID: Deployed index ID (required)
 * - VERTEX_VECTOR_SEARCH_INDEX_ID: Index ID for upsert/remove
 *
 * Distance metric handling:
 * - COSINE_DISTANCE: score = 1 - distance
 * - DOT_PRODUCT_DISTANCE: score = 1 - distance
 * - SQUARED_L2_DISTANCE: score = 1 / (1 + distance)
 */
export class VertexVectorSearchIndex implements VectorIndex {
  private config: VertexConfig;
  private configChecked: boolean = false;

  constructor() {
    this.config = parseVertexConfig();
  }

  /**
   * Convert Vertex distance to similarity score [0, 1]
   */
  private distanceToSimilarity(distance: number): number {
    switch (this.config.distanceMetric) {
      case 'COSINE':
      case 'DOT_PRODUCT':
        return Math.max(0, Math.min(1, 1 - distance));
      case 'SQUARED_L2':
        return 1 / (1 + distance);
      default:
        return Math.max(0, Math.min(1, 1 - distance));
    }
  }

  getName(): string {
    return 'vertex_vector_search';
  }

  isConfigured(): boolean {
    return this.config.isValid && !!this.config.deployedIndexId;
  }

  /**
   * Get detailed configuration status for debugging
   */
  getConfigStatus(): { configured: boolean; errors: string[]; urls: { findNeighbors: string; upsert: string } } {
    return {
      configured: this.isConfigured(),
      errors: this.config.validationErrors,
      urls: {
        findNeighbors: this.config.findNeighborsUrl,
        upsert: this.config.upsertUrl,
      },
    };
  }

  /**
   * Log misconfiguration error once per process
   */
  private logMisconfigurationOnce(): void {
    if (!this.configChecked) {
      this.configChecked = true;

      if (!this.config.isValid && !vertexMisconfigWarningLogged) {
        vertexMisconfigWarningLogged = true;
        logError('Vertex Vector Search misconfigured - falling back to Firestore. Fix configuration for production scale.', {
          errors: this.config.validationErrors,
          recommendation: 'Set VERTEX_INDEX_ENDPOINT_RESOURCE (full resource name) and VERTEX_DEPLOYED_INDEX_ID',
          example: 'VERTEX_INDEX_ENDPOINT_RESOURCE=projects/my-project/locations/us-central1/indexEndpoints/123456789',
        });
      }
    }
  }

  /**
   * Search for similar vectors using Vertex AI Vector Search REST API
   *
   * Uses the findNeighbors endpoint:
   * POST https://{region}-aiplatform.googleapis.com/v1/{indexEndpointResource}:findNeighbors
   */
  async search(
    queryEmbedding: number[],
    tenantId: string,
    topK: number
  ): Promise<VectorSearchResult[]> {
    this.logMisconfigurationOnce();

    if (!this.isConfigured()) {
      return [];
    }

    const startTime = Date.now();

    try {
      // Get access token for authentication
      const { GoogleAuth } = await import('google-auth-library');
      const auth = new GoogleAuth({
        scopes: ['https://www.googleapis.com/auth/cloud-platform'],
      });
      const client = await auth.getClient();
      const accessToken = await client.getAccessToken();

      if (!accessToken.token) {
        throw new Error('Failed to get access token');
      }

      // Build the findNeighbors request
      const requestBody = {
        deployedIndexId: this.config.deployedIndexId,
        queries: [
          {
            datapoint: {
              datapointId: 'query',
              featureVector: queryEmbedding,
              restricts: [
                {
                  namespace: 'tenantId',
                  allowList: [tenantId],
                },
              ],
            },
            neighborCount: topK,
          },
        ],
      };

      // Use precomputed URL
      const response = await fetch(this.config.findNeighborsUrl, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${accessToken.token}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Vertex API error: ${response.status} ${errorText}`);
      }

      const data = await response.json() as VertexFindNeighborsResponse;

      // Parse the response
      const results: VectorSearchResult[] = [];
      if (data.nearestNeighbors && data.nearestNeighbors.length > 0) {
        const neighbors = data.nearestNeighbors[0].neighbors || [];
        for (const neighbor of neighbors) {
          const [chunkId, noteId] = neighbor.datapoint.datapointId.split(':');
          const similarity = neighbor.distance !== undefined
            ? this.distanceToSimilarity(neighbor.distance)
            : 0;
          results.push({
            chunkId,
            noteId: noteId || '',
            score: similarity,
          });
        }
      }

      logInfo('Vertex Vector Search complete', {
        tenantId,
        topK,
        resultsReturned: results.length,
        elapsedMs: Date.now() - startTime,
      });

      return results;
    } catch (err) {
      logError('Vertex Vector Search failed', err);
      return [];
    }
  }

  /**
   * Upsert vectors to the Vertex AI index
   *
   * Uses the upsertDatapoints endpoint for streaming updates.
   * For batch updates, use the backfill script with batch import.
   */
  async upsert(
    datapoints: VertexDatapoint[]
  ): Promise<boolean> {
    if (!this.config.upsertUrl) {
      logError('Vertex Vector Search index ID not configured for upsert', {
        recommendation: 'Set VERTEX_VECTOR_SEARCH_INDEX_ID',
      });
      return false;
    }

    const startTime = Date.now();

    try {
      const { GoogleAuth } = await import('google-auth-library');
      const auth = new GoogleAuth({
        scopes: ['https://www.googleapis.com/auth/cloud-platform'],
      });
      const client = await auth.getClient();
      const accessToken = await client.getAccessToken();

      if (!accessToken.token) {
        throw new Error('Failed to get access token');
      }

      const requestBody = {
        datapoints: datapoints.map(dp => ({
          datapointId: dp.datapointId,
          featureVector: dp.featureVector,
          restricts: dp.restricts,
        })),
      };

      const response = await fetch(this.config.upsertUrl, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${accessToken.token}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Vertex upsert error: ${response.status} ${errorText}`);
      }

      logInfo('Vertex Vector Search upsert complete', {
        datapointsUpserted: datapoints.length,
        elapsedMs: Date.now() - startTime,
      });

      return true;
    } catch (err) {
      logError('Vertex Vector Search upsert failed', err);
      return false;
    }
  }

  /**
   * Remove vectors from the Vertex AI index
   */
  async remove(datapointIds: string[]): Promise<boolean> {
    if (!this.config.removeUrl) {
      logError('Vertex Vector Search index ID not configured for remove', {
        recommendation: 'Set VERTEX_VECTOR_SEARCH_INDEX_ID',
      });
      return false;
    }

    try {
      const { GoogleAuth } = await import('google-auth-library');
      const auth = new GoogleAuth({
        scopes: ['https://www.googleapis.com/auth/cloud-platform'],
      });
      const client = await auth.getClient();
      const accessToken = await client.getAccessToken();

      if (!accessToken.token) {
        throw new Error('Failed to get access token');
      }

      const requestBody = {
        datapointIds,
      };

      const response = await fetch(this.config.removeUrl, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${accessToken.token}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Vertex remove error: ${response.status} ${errorText}`);
      }

      logInfo('Vertex Vector Search remove complete', {
        datapointsRemoved: datapointIds.length,
      });

      return true;
    } catch (err) {
      logError('Vertex Vector Search remove failed', err);
      return false;
    }
  }
}

/**
 * Vertex AI Vector Search response types
 */
interface VertexFindNeighborsResponse {
  nearestNeighbors?: Array<{
    id?: string;
    neighbors?: Array<{
      datapoint: {
        datapointId: string;
      };
      distance?: number;
    }>;
  }>;
}

/**
 * Datapoint for Vertex AI Vector Search upsert
 */
export interface VertexDatapoint {
  datapointId: string;  // Format: {chunkId}:{noteId}
  featureVector: number[];
  restricts?: Array<{
    namespace: string;
    allowList?: string[];
    denyList?: string[];
  }>;
}

// Cached vector index instance (avoid re-parsing config on every call)
let cachedVectorIndex: VectorIndex | null = null;
let cachedVertexIndex: VertexVectorSearchIndex | null = null;

/**
 * Get the active vector index based on configuration.
 * Returns Vertex if enabled and configured, otherwise Firestore fallback.
 */
export function getVectorIndex(): VectorIndex {
  if (cachedVectorIndex) {
    return cachedVectorIndex;
  }

  const { VERTEX_VECTOR_SEARCH_ENABLED } = require('./config');

  if (VERTEX_VECTOR_SEARCH_ENABLED) {
    const vertexIndex = new VertexVectorSearchIndex();
    if (vertexIndex.isConfigured()) {
      cachedVectorIndex = vertexIndex;
      logInfo('Vector search using Vertex AI', { index: vertexIndex.getName() });
      return vertexIndex;
    }
    // Misconfiguration is logged inside VertexVectorSearchIndex.logMisconfigurationOnce()
  }

  const firestoreIndex = new FirestoreApproxVectorIndex();
  cachedVectorIndex = firestoreIndex;
  logInfo('Vector search using Firestore fallback', { index: firestoreIndex.getName() });
  return firestoreIndex;
}

/**
 * Get the Vertex index for upsert/remove operations.
 * Returns null if Vertex is not configured.
 */
export function getVertexIndex(): VertexVectorSearchIndex | null {
  if (cachedVertexIndex !== null) {
    return cachedVertexIndex;
  }

  const { VERTEX_VECTOR_SEARCH_ENABLED } = require('./config');

  if (VERTEX_VECTOR_SEARCH_ENABLED) {
    const vertexIndex = new VertexVectorSearchIndex();
    if (vertexIndex.isConfigured()) {
      cachedVertexIndex = vertexIndex;
      return vertexIndex;
    }
  }

  return null;
}

/**
 * Check if Vertex Vector Search is properly configured.
 * Useful for health checks and diagnostics.
 */
export function isVertexConfigured(): boolean {
  const { VERTEX_VECTOR_SEARCH_ENABLED } = require('./config');

  if (!VERTEX_VECTOR_SEARCH_ENABLED) {
    return false;
  }

  const vertexIndex = new VertexVectorSearchIndex();
  return vertexIndex.isConfigured();
}

/**
 * Get Vertex configuration status for diagnostics.
 * Returns configuration details without sensitive data.
 */
export function getVertexConfigStatus(): { enabled: boolean; configured: boolean; errors: string[] } {
  const { VERTEX_VECTOR_SEARCH_ENABLED } = require('./config');

  if (!VERTEX_VECTOR_SEARCH_ENABLED) {
    return { enabled: false, configured: false, errors: [] };
  }

  const vertexIndex = new VertexVectorSearchIndex();
  const status = vertexIndex.getConfigStatus();
  return {
    enabled: true,
    configured: status.configured,
    errors: status.errors,
  };
}



================================================================================
FILE: src/vectorRetriever.ts
LINES: 173
PATH: /Users/salscrudato/Projects/auroranotes-api/src/vectorRetriever.ts
================================================================================

/**
 * AuroraNotes API - Vector Retriever Interface
 *
 * High-level abstraction for vector search with enriched results.
 * Uses VectorIndex from vectorIndex.ts for the underlying search,
 * then enriches results with chunk text and metadata.
 *
 * This module provides:
 * - VectorSearchResult with full chunk data (text, createdAt)
 * - VectorSearchOptions for filtering (maxAgeDays, excludeNoteIds, minScore)
 * - Convenience function vectorSearch() that handles embedding generation
 */

import { getDb } from './firestore';
import { generateQueryEmbedding } from './embeddings';
import { logInfo, logWarn } from './utils';
import { RETRIEVAL_TOP_K, CHUNKS_COLLECTION } from './config';
import { getVectorIndex, VectorSearchResult as VectorIndexResult } from './vectorIndex';
import { ChunkDoc } from './types';

/**
 * Enriched vector search result with full chunk data
 */
export interface VectorSearchResult {
  chunkId: string;
  noteId: string;
  text: string;
  score: number;
  createdAt: Date;
}

export interface VectorSearchOptions {
  maxAgeDays?: number;
  excludeNoteIds?: string[];
  minScore?: number;
}

/**
 * Vector retriever that uses VectorIndex and enriches results
 *
 * Uses the configured VectorIndex (Firestore or Vertex) for search,
 * then fetches full chunk data from Firestore for enrichment.
 */
export class EnrichedVectorRetriever {
  private readonly overFetchMultiplier: number;

  constructor(overFetchMultiplier: number = 2) {
    this.overFetchMultiplier = overFetchMultiplier;
  }

  async search(
    queryEmbedding: number[],
    tenantId: string,
    k: number,
    options: VectorSearchOptions = {}
  ): Promise<VectorSearchResult[]> {
    const minScore = options.minScore ?? 0.3;
    const excludeSet = new Set(options.excludeNoteIds ?? []);
    const maxAgeDays = options.maxAgeDays ?? 90;
    const cutoffDate = new Date();
    cutoffDate.setDate(cutoffDate.getDate() - maxAgeDays);

    // Get the configured vector index (Firestore or Vertex)
    const vectorIndex = getVectorIndex();

    // Over-fetch to account for filtering
    const overFetchK = k * this.overFetchMultiplier;
    const indexResults = await vectorIndex.search(queryEmbedding, tenantId, overFetchK);

    // Filter by score and exclusions
    const filteredResults = indexResults.filter(r =>
      r.score >= minScore && !excludeSet.has(r.noteId)
    );

    // Take top k after filtering
    const topResults = filteredResults.slice(0, k);

    // Enrich with chunk data from Firestore using batch getAll()
    // This is O(1) network round-trips instead of O(n)
    const db = getDb();
    const enrichedResults: VectorSearchResult[] = [];

    if (topResults.length === 0) {
      return enrichedResults;
    }

    try {
      // Build document references for batch fetch
      const chunkRefs = topResults.map(r =>
        db.collection(CHUNKS_COLLECTION).doc(r.chunkId)
      );

      // Batch fetch all chunks in a single call
      const chunkDocs = await db.getAll(...chunkRefs);

      // Build map for quick lookup
      const chunkDataMap = new Map<string, ChunkDoc>();
      for (const doc of chunkDocs) {
        if (doc.exists) {
          chunkDataMap.set(doc.id, doc.data() as ChunkDoc);
        }
      }

      // Process results maintaining order from vector search
      for (const result of topResults) {
        const data = chunkDataMap.get(result.chunkId);
        if (data) {
          const createdAt = data.createdAt && 'toDate' in data.createdAt
            ? data.createdAt.toDate()
            : new Date();

          // Apply time filter
          if (createdAt >= cutoffDate) {
            enrichedResults.push({
              chunkId: result.chunkId,
              noteId: result.noteId,
              text: data.text || '',
              score: result.score,
              createdAt,
            });
          }
        }
      }
    } catch (err) {
      logWarn('Batch chunk fetch failed', { chunkCount: topResults.length, error: err });
    }

    return enrichedResults;
  }
}

// Singleton instance
let retrieverInstance: EnrichedVectorRetriever | null = null;

/**
 * Get configured vector retriever
 */
export function getVectorRetriever(): EnrichedVectorRetriever {
  if (!retrieverInstance) {
    retrieverInstance = new EnrichedVectorRetriever();
  }
  return retrieverInstance;
}

/**
 * Convenience function to search with embedding generation
 */
export async function vectorSearch(
  query: string,
  tenantId: string,
  k: number,
  options: VectorSearchOptions = {},
  requestId?: string
): Promise<{ results: VectorSearchResult[]; embeddingMs: number }> {
  const startTime = Date.now();

  // Generate query embedding (returns single embedding array, not array of arrays)
  const embedding = await generateQueryEmbedding(query);
  const embeddingMs = Date.now() - startTime;

  if (!embedding || embedding.length === 0) {
    logWarn('Failed to generate query embedding', { query: query.slice(0, 50), requestId });
    return { results: [], embeddingMs };
  }

  // Search with configured retriever
  const retriever = getVectorRetriever();
  const results = await retriever.search(embedding, tenantId, k, options);

  return { results, embeddingMs };
}



